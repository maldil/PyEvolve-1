<link rel="stylesheet" href="../../../../..//default.css">
<script src="../../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/salu133445/musegan/blob/main/v2/musegan/musegan/models.py#L100">GitHubLink</a>


<a href="https://github.com/maldil/musegan/blob/main/v2/musegan/musegan/models.py#L100">GitMyHubLink</a>

Class that defines the GAN model.

import os.path
import time
import numpy as np
import tensorflow as tf
from musegan.model import Model
from musegan.musegan.components import Discriminator, Generator
from musegan.utils.metrics import Metrics

class GAN(Model):
    Class that defines the first-stage (without refiner) model.
    def __init__(self, sess, config, name=&quotGAN&quot, reuse=None):
        super().__init__(sess, config, name)

        print(&quot[*] Building GAN...&quot)
        with tf.variable_scope(name, reuse=reuse) as scope:
            self.scope = scope
            self.build()

    def build(self):
        Build the model.
        self.global_step = tf.Variable(0, trainable=False, name=&quotglobal_step&quot)

        &#47&#47 Create placeholders
        self.z = {}
        if self.config[&quotnet_g&quot][&quotz_dim_shared&quot] &gt; 0:
            self.z[&quotshared&quot] = tf.placeholder(
                tf.float32, (self.config[&quotbatch_size&quot],
                             self.config[&quotnet_g&quot][&quotz_dim_shared&quot]), &quotz_shared&quot
            )
        if self.config[&quotnet_g&quot][&quotz_dim_private&quot] &gt; 0:
            self.z[&quotprivate&quot] = tf.placeholder(
                tf.float32, (self.config[&quotbatch_size&quot],
                             self.config[&quotnet_g&quot][&quotz_dim_private&quot],
                             self.config[&quotnum_track&quot]), &quotz_private&quot
            )
        if self.config[&quotnet_g&quot][&quotz_dim_temporal_shared&quot] &gt; 0:
            self.z[&quottemporal_shared&quot] = tf.placeholder(
                tf.float32, (self.config[&quotbatch_size&quot],
                             self.config[&quotnet_g&quot][&quotz_dim_temporal_shared&quot]),
                &quotz_temporal_shared&quot
            )
        if self.config[&quotnet_g&quot][&quotz_dim_temporal_private&quot] &gt; 0:
            self.z[&quottemporal_private&quot] = tf.placeholder(
                tf.float32, (self.config[&quotbatch_size&quot],
                             self.config[&quotnet_g&quot][&quotz_dim_temporal_private&quot],
                             self.config[&quotnum_track&quot]), &quotz_temporal_private&quot
            )

        data_shape = (self.config[&quotbatch_size&quot], self.config[&quotnum_bar&quot],
                      self.config[&quotnum_timestep&quot], self.config[&quotnum_pitch&quot],
                      self.config[&quotnum_track&quot])
        self.x = tf.placeholder(tf.bool, data_shape, &quotx&quot)
        self.x_ = tf.cast(self.x, tf.float32, &quotx_&quot)

        &#47&#47 Components
        self.G = Generator(self.z, self.config, name=&quotG&quot)
        self.test_round = self.G.tensor_out &gt; 0.5
        self.test_bernoulli = self.G.tensor_out &gt; tf.random_uniform(data_shape)

        self.D_fake = Discriminator(self.G.tensor_out, self.config, name=&quotD&quot)
        self.D_real = Discriminator(self.x_, self.config, name=&quotD&quot, reuse=True)
        self.components = (self.G, self.D_fake)

        &#47&#47 Losses
        self.g_loss, self.d_loss = self.get_adversarial_loss(Discriminator)

        &#47&#47 Optimizers
        with tf.variable_scope(&quotOptimizer&quot):
            self.g_optimizer = self.get_optimizer()
            self.g_step = self.g_optimizer.minimize(
                self.g_loss, self.global_step, self.G.vars)

            self.d_optimizer = self.get_optimizer()
            self.d_step = self.d_optimizer.minimize(
                self.d_loss, self.global_step, self.D_fake.vars)

            &#47&#47 Apply weight clipping
            if self.config[&quotgan&quot][&quottype&quot] == &quotwgan&quot:
                with tf.control_dependencies([self.d_step]):
                    self.d_step = tf.group(
                        *(tf.assign(var, tf.clip_by_value(
                            var, -self.config[&quotgan&quot][&quotclip_value&quot],
                            self.config[&quotgan&quot][&quotclip_value&quot]))
                          for var in self.D_fake.vars))

        &#47&#47 Metrics
        self.metrics = Metrics(self.config)

        &#47&#47 Saver
        self.saver = tf.train.Saver()

        &#47&#47 Print and save model information
        self.print_statistics()
        self.save_statistics()
        self.print_summary()
        self.save_summary()

    def train(<a id="change">self</a>, <a id="change">x_train</a>, <a id="change">train_config</a>):
        Train the model.
        &#47&#47 Initialize sampler
        <a id="change">self.x_sample = x_train[np.random.choice(
            len(x_train), self.config[&quotbatch_size&quot], False)]</a>
        <a id="change">feed_dict_sample = {self.x: self.x_sample}</a>

        <a id="change">self.z_sample = {}</a>
        for key in self.z:
            <a id="change">self.z_sample[key] = np.random.normal(size=self.z[key].get_shape())</a>
            <a id="change">feed_dict_sample[self.z[key]] = self.z_sample[key]</a>

        &#47&#47 Save samples
        self.save_samples(&quotx_train&quot, x_train, save_midi=True)
        self.save_samples(&quotx_sample&quot, self.x_sample, save_midi=True)

        &#47&#47 Open log files and write headers
        <a id="change">log_step = open(os.path.join(self.config[&quotlog_dir&quot], &quotstep.log&quot), &quotw&quot)</a>
        <a id="change">log_batch = open(os.path.join(self.config[&quotlog_dir&quot], &quotbatch.log&quot), &quotw&quot)</a>
        <a id="change">log_epoch = open(os.path.join(self.config[&quotlog_dir&quot], &quotepoch.log&quot), &quotw&quot)</a>
        log_step.write(&quot&#47&#47 epoch, step, negative_critic_loss\n&quot)
        log_batch.write(&quot&#47&#47 epoch, batch, time, negative_critic_loss, g_loss\n&quot)
        log_epoch.write(&quot&#47&#47 epoch, time, negative_critic_loss, g_loss\n&quot)

        &#47&#47 Initialize counter
        <a id="change">counter = 0</a>
        <a id="change">num_batch = len(x_train) // self.config[&quotbatch_size&quot]</a>

        &#47&#47 Start epoch iteration
        print(&quot{:=^80}&quot.format(&quot Training Start &quot))
        for epoch in range(train_config[&quotnum_epoch&quot]):

            print(&quot{:-^80}&quot.format(&quot Epoch {} Start &quot.format(epoch)))
            <a id="change">epoch_start_time = time.time()</a>

            &#47&#47 Prepare batched training data
            <a id="change">z_random_batch = {}</a>
            for key in self.z:
                <a id="change">z_random_batch[key] = np.random.normal(
                    size=([num_batch] + self.z[key].get_shape().as_list()))</a>
            <a id="change">x_random_batch = np.random.choice(
                len(x_train), (num_batch, self.config[&quotbatch_size&quot]), False)</a>

            &#47&#47 Start batch iteration
            for batch in range(num_batch):

                <a id="change">feed_dict_batch = {self.x: x_train[x_random_batch[batch]]}</a>
                for key in self.z:
                    <a id="change">feed_dict_batch[self.z[key]] = z_random_batch[key][batch]</a>

                if (counter &lt; 25) or (counter % 500 == 0):
                    <a id="change">num_critics = 100</a>
                else:
                    <a id="change">num_critics = 5</a>

                <a id="change">batch_start_time = time.time()</a>

                &#47&#47 Update networks
                for _ in range(num_critics):
                    <a id="change">_, d_loss = self.sess.run([self.d_step, self.d_loss],
                                              feed_dict_batch)</a>
                    log_step.write("{}, {:14.6f}\n".format(
                        self.get_global_step_str(), -d_loss
                    ))

                <a id="change">_, d_loss, g_loss = self.sess.run(
                    [self.g_step, self.d_loss, self.g_loss], feed_dict_batch
                )</a>
                log_step.write("{}, {:14.6f}\n".format(
                    self.get_global_step_str(), -d_loss
                ))

                <a id="change">time_batch = time.time() - batch_start_time</a>

                &#47&#47 Print iteration summary
                if train_config[&quotverbose&quot]:
                    if batch &lt; 1:
                        print("epoch |   batch   |  time  |    - D_loss    |"
                              "     G_loss")
                    print("  {:2d}  | {:4d}/{:4d} | {:6.2f} | {:14.6f} | "
                          "{:14.6f}".format(epoch, batch, num_batch, time_batch,
                                            -d_loss, g_loss))

                log_batch.write("{:d}, {:d}, {:f}, {:f}, {:f}\n".format(
                    epoch, batch, time_batch, -d_loss, g_loss
                ))

                &#47&#47 run sampler
                if train_config[&quotsample_along_training&quot]:
                    if counter%100 == 0 or (counter &lt; 300 and counter%20 == 0):
                        self.run_sampler(self.G.tensor_out, feed_dict_sample,
                                         False)
                        self.run_sampler(self.test_round, feed_dict_sample,
                                         (counter &gt; 500), postfix=&quottest_round&quot)
                        self.run_sampler(self.test_bernoulli, feed_dict_sample,
                                         (counter &gt; 500),
                                         postfix=&quottest_bernoulli&quot)

                &#47&#47 run evaluation
                if train_config[&quotevaluate_along_training&quot]:
                    if counter%10 == 0:
                        self.run_eval(self.test_round, feed_dict_sample,
                                      postfix=&quottest_round&quot)
                        self.run_eval(self.test_bernoulli, feed_dict_sample,
                                      postfix=&quottest_bernoulli&quot)

                counter += 1

            &#47&#47 print epoch info
            <a id="change">time_epoch = time.time() - epoch_start_time</a>

            if not train_config[&quotverbose&quot]:
                if epoch &lt; 1:
                    print("epoch |   time   |    - D_loss    |     G_loss")
                print("  {:2d}  | {:8.2f} | {:14.6f} | {:14.6f}".format(
                    epoch, time_epoch, -d_loss, g_loss))

            log_epoch.write("{:d}, {:f}, {:f}, {:f}\n".format(
                epoch, time_epoch, -d_loss, g_loss
            ))

            &#47&#47 save checkpoints
            self.save()

        print(&quot{:=^80}&quot.format(&quot Training End &quot))
        <a id="change">log_step</a><a id="change">.close()</a>
        <a id="change">log_batch</a><a id="change">.close()</a>
        <a id="change">log_epoch</a><a id="change">.close()</a>
</code></pre>