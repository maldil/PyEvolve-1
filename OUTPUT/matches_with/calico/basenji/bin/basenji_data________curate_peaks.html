<link rel="stylesheet" href="../../..//default.css">
<script src="../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/calico/basenji/blob/master/bin/basenji_data.py#L627">GitHubLink</a>


<a href="https://github.com/maldil/basenji/blob/master/bin/basenji_data.py#L627">GitMyHubLink</a>

&#47&#47!/usr/bin/env python
&#47&#47 Copyright 2017 Calico LLC

&#47&#47 Licensed under the Apache License, Version 2.0 (the "License");
&#47&#47 you may not use this file except in compliance with the License.
&#47&#47 You may obtain a copy of the License at

&#47&#47     https://www.apache.org/licenses/LICENSE-2.0

&#47&#47 Unless required by applicable law or agreed to in writing, software
&#47&#47 distributed under the License is distributed on an "AS IS" BASIS,
&#47&#47 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
&#47&#47 See the License for the specific language governing permissions and
&#47&#47 limitations under the License.
&#47&#47 =========================================================================
from __future__ import print_function

from optparse import OptionParser
import collections
import gzip
import heapq
import json
import math
import pdb
import os
import random
import shutil
import subprocess
import sys
import tempfile
import time

import h5py
import numpy as np
import pandas as pd

from basenji import genome
from basenji import util

try:
  import slurm
except ModuleNotFoundError:
  pass

&quot&quot&quot
basenji_data.py

Compute model sequences from the genome, extracting DNA coverage values.
&quot&quot&quot

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def main():
  usage = &quotusage: %prog [options] &lt;fasta_file&gt; &lt;targets_file&gt;&quot
  parser = OptionParser(usage)
  parser.add_option(&quot-b&quot, dest=&quotblacklist_bed&quot,
      help=&quotSet blacklist nucleotides to a baseline value.&quot)
  parser.add_option(&quot--break&quot, dest=&quotbreak_t&quot,
      default=786432, type=&quotint&quot,
      help=&quotBreak in half contigs above length [Default: %default]&quot)
  parser.add_option(&quot-c&quot,&quot--crop&quot, dest=&quotcrop_bp&quot,
      default=0, type=&quotint&quot,
      help=&quotCrop bp off each end [Default: %default]&quot)
  parser.add_option(&quot-d&quot, dest=&quotsample_pct&quot,
      default=1.0, type=&quotfloat&quot,
      help=&quotDown-sample the segments&quot)
  parser.add_option(&quot-f&quot, dest=&quotfolds&quot,
      default=None, type=&quotint&quot,
      help=&quotGenerate cross fold split [Default: %default]&quot)
  parser.add_option(&quot-g&quot, dest=&quotgaps_file&quot,
      help=&quotGenome assembly gaps BED [Default: %default]&quot)
  parser.add_option(&quot-i&quot, dest=&quotinterp_nan&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotInterpolate NaNs [Default: %default]&quot) 
  parser.add_option(&quot-l&quot, dest=&quotseq_length&quot,
      default=131072, type=&quotint&quot,
      help=&quotSequence length [Default: %default]&quot)
  parser.add_option(&quot--limit&quot, dest=&quotlimit_bed&quot,
      help=&quotLimit to segments that overlap regions in a BED file&quot)
  parser.add_option(&quot--local&quot, dest=&quotrun_local&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotRun jobs locally as opposed to on SLURM [Default: %default]&quot)
  parser.add_option(&quot-o&quot, dest=&quotout_dir&quot,
      default=&quotdata_out&quot,
      help=&quotOutput directory [Default: %default]&quot)
  parser.add_option(&quot-p&quot, dest=&quotprocesses&quot,
      default=None, type=&quotint&quot,
      help=&quotNumber parallel processes [Default: %default]&quot)
  parser.add_option(&quot--peaks&quot, dest=&quotpeaks_only&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotCreate contigs only from peaks [Default: %default]&quot) 
  parser.add_option(&quot-r&quot, dest=&quotseqs_per_tfr&quot,
      default=256, type=&quotint&quot,
      help=&quotSequences per TFRecord file [Default: %default]&quot)
  parser.add_option(&quot--restart&quot, dest=&quotrestart&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotContinue progress from midpoint. [Default: %default]&quot)
  parser.add_option(&quot--seed&quot, dest=&quotseed&quot,
      default=44, type=&quotint&quot,
      help=&quotRandom seed [Default: %default]&quot)
  parser.add_option(&quot--snap&quot, dest=&quotsnap&quot,
      default=1, type=&quotint&quot,
      help=&quotSnap sequences to multiple of the given value [Default: %default]&quot)
  parser.add_option(&quot--st&quot, &quot--split_test&quot, dest=&quotsplit_test&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotExit after split. [Default: %default]&quot)
  parser.add_option(&quot--stride&quot, &quot--stride_train&quot, dest=&quotstride_train&quot,
      default=1., type=&quotfloat&quot,
      help=&quotStride to advance train sequences [Default: seq_length]&quot)
  parser.add_option(&quot--stride_test&quot, dest=&quotstride_test&quot,
      default=1., type=&quotfloat&quot,
      help=&quotStride to advance valid and test sequences [Default: seq_length]&quot)
  parser.add_option(&quot-t&quot, dest=&quottest_pct_or_chr&quot,
      default=0.05, type=&quotstr&quot,
      help=&quotProportion of the data for testing [Default: %default]&quot)
  parser.add_option(&quot-u&quot, dest=&quotumap_bed&quot,
      help=&quotUnmappable regions in BED format&quot)
  parser.add_option(&quot--umap_t&quot, dest=&quotumap_t&quot,
      default=0.5, type=&quotfloat&quot,
      help=&quotRemove sequences with more than this unmappable bin % [Default: %default]&quot)
  parser.add_option(&quot--umap_clip&quot, dest=&quotumap_clip&quot,
      default=1, type=&quotfloat&quot,
      help=&quotClip values at unmappable positions to distribution quantiles, eg 0.25. [Default: %default]&quot)
  parser.add_option(&quot--umap_tfr&quot, dest=&quotumap_tfr&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotSave umap array into TFRecords [Default: %default]&quot)
  parser.add_option(&quot-w&quot, dest=&quotpool_width&quot,
      default=128, type=&quotint&quot,
      help=&quotSum pool width [Default: %default]&quot)
  parser.add_option(&quot-v&quot, dest=&quotvalid_pct_or_chr&quot,
      default=0.05, type=&quotstr&quot,
      help=&quotProportion of the data for validation [Default: %default]&quot)
  (options, args) = parser.parse_args()

  if len(args) != 2:
    parser.error(&quotMust provide FASTA and sample coverage labels and paths.&quot)
  else:
    fasta_file = args[0]
    targets_file = args[1]

  random.seed(options.seed)
  np.random.seed(options.seed)

  if options.break_t is not None and options.break_t &lt; options.seq_length:
    print(&quotMaximum contig length --break cannot be less than sequence length.&quot, file=sys.stderr)
    exit(1)

  &#47&#47 transform proportion strides to base pairs
  if options.stride_train &lt;= 1:
    print(&quotstride_train %.f&quot%options.stride_train, end=&quot&quot)
    options.stride_train = options.stride_train*options.seq_length
    print(&quot converted to %f&quot % options.stride_train)
  options.stride_train = int(np.round(options.stride_train))
  if options.stride_test &lt;= 1:
    if options.folds is None:
      print(&quotstride_test %.f&quot%options.stride_test, end=&quot&quot)
      options.stride_test = options.stride_test*options.seq_length
      print(&quot converted to %f&quot % options.stride_test)
  options.stride_test = int(np.round(options.stride_test))

  &#47&#47 check snap
  if options.snap is not None:
    if np.mod(options.seq_length, options.snap) != 0: 
      raise ValueError(&quotseq_length must be a multiple of snap&quot)
    if np.mod(options.stride_train, options.snap) != 0: 
      raise ValueError(&quotstride_train must be a multiple of snap&quot)
    if np.mod(options.stride_test, options.snap) != 0:
      raise ValueError(&quotstride_test must be a multiple of snap&quot)

  &#47&#47 setup output directory
  if os.path.isdir(options.out_dir) and not options.restart:
    print(&quotRemove output directory %s or use --restart option.&quot % options.out_dir)
    exit(1)
  elif not os.path.isdir(options.out_dir):
    os.mkdir(options.out_dir)

  &#47&#47 read target datasets
  targets_df = pd.read_csv(targets_file, index_col=0, sep=&quot\t&quot)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 define genomic contigs
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  if not options.restart:
    chrom_contigs = genome.load_chromosomes(fasta_file)

    &#47&#47 remove gaps
    if options.gaps_file:
      chrom_contigs = genome.split_contigs(chrom_contigs,
                                           options.gaps_file)

    &#47&#47 ditch the chromosomes for contigs
    contigs = []
    for chrom in chrom_contigs:
      contigs += [Contig(chrom, ctg_start, ctg_end)
                   for ctg_start, ctg_end in chrom_contigs[chrom]]

    &#47&#47 limit to a BED file
    if options.limit_bed is not None:
      contigs = limit_contigs(contigs, options.limit_bed)

    &#47&#47 limit to peaks
    if options.peaks_only:
      peaks_bed = curate_peaks(targets_df, options.out_dir, options.pool_width, options.crop_bp)
      contigs = limit_contigs(contigs, peaks_bed)

    &#47&#47 filter for large enough
    seq_tlength = options.seq_length - 2*options.crop_bp
    contigs = [ctg for ctg in contigs if ctg.end - ctg.start &gt;= seq_tlength]

    &#47&#47 break up large contigs
    if options.break_t is not None:
      contigs = break_large_contigs(contigs, options.break_t)

    &#47&#47 print contigs to BED file
    &#47&#47 ctg_bed_file = &quot%s/contigs.bed&quot % options.out_dir
    &#47&#47 write_seqs_bed(ctg_bed_file, contigs)


  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 divide between train/valid/test
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 label folds
  if options.folds is not None:
    fold_labels = [&quotfold%d&quot % fi for fi in range(options.folds)]
    num_folds = options.folds
  else:
    fold_labels = [&quottrain&quot, &quotvalid&quot, &quottest&quot]
    num_folds = 3

  if not options.restart:
    if options.folds is not None:
      &#47&#47 divide by fold pct
      fold_contigs = divide_contigs_folds(contigs, options.folds)

    else:
      try:
        &#47&#47 convert to float pct
        valid_pct = float(options.valid_pct_or_chr)
        test_pct = float(options.test_pct_or_chr)
        assert(0 &lt;= valid_pct &lt;= 1)
        assert(0 &lt;= test_pct &lt;= 1)

        &#47&#47 divide by pct
        fold_contigs = divide_contigs_pct(contigs, test_pct, valid_pct)

      except (ValueError, AssertionError):
        &#47&#47 divide by chr
        valid_chrs = options.valid_pct_or_chr.split(&quot,&quot)
        test_chrs = options.test_pct_or_chr.split(&quot,&quot)
        fold_contigs = divide_contigs_chr(contigs, test_chrs, valid_chrs)

    &#47&#47 rejoin broken contigs within set
    for fi in range(len(fold_contigs)):
      fold_contigs[fi] = rejoin_large_contigs(fold_contigs[fi])

    &#47&#47 write labeled contigs to BED file
    ctg_bed_file = &quot%s/contigs.bed&quot % options.out_dir
    ctg_bed_out = open(ctg_bed_file, &quotw&quot)
    for fi in range(len(fold_contigs)):
      for ctg in fold_contigs[fi]:
        line = &quot%s\t%d\t%d\t%s&quot % (ctg.chr, ctg.start, ctg.end, fold_labels[fi])
        print(line, file=ctg_bed_out)
    ctg_bed_out.close()

  if options.split_test:
    exit()

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 define model sequences
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  if not options.restart:
    fold_mseqs = []
    for fi in range(num_folds):
      if fold_labels[fi] in [&quotvalid&quot,&quottest&quot]:
        stride_fold = options.stride_test
      else:
        stride_fold = options.stride_train

      &#47&#47 stride sequences across contig
      fold_mseqs_fi = contig_sequences(fold_contigs[fi], seq_tlength,
                                       stride_fold, options.snap, fold_labels[fi])
      fold_mseqs.append(fold_mseqs_fi)

      &#47&#47 shuffle
      random.shuffle(fold_mseqs[fi])

      &#47&#47 down-sample
      if options.sample_pct &lt; 1.0:
        fold_mseqs[fi] = random.sample(fold_mseqs[fi], int(options.sample_pct*len(fold_mseqs[fi])))

    &#47&#47 merge into one list
    mseqs = [ms for fm in fold_mseqs for ms in fm]


  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 mappability
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  if not options.restart:
    if options.umap_bed is not None:
      if shutil.which(&quotbedtools&quot) is None:
        print(&quotInstall Bedtools to annotate unmappable sites&quot, file=sys.stderr)
        exit(1)

      &#47&#47 annotate unmappable positions
      mseqs_unmap = annotate_unmap(mseqs, options.umap_bed,
                                   seq_tlength, options.pool_width)

      &#47&#47 filter unmappable
      mseqs_map_mask = (mseqs_unmap.mean(axis=1, dtype=&quotfloat64&quot) &lt; options.umap_t)
      mseqs = [mseqs[i] for i in range(len(mseqs)) if mseqs_map_mask[i]]
      mseqs_unmap = mseqs_unmap[mseqs_map_mask,:]

      &#47&#47 write to file
      unmap_npy = &quot%s/mseqs_unmap.npy&quot % options.out_dir
      np.save(unmap_npy, mseqs_unmap)

    &#47&#47 write sequences to BED
    seqs_bed_file = &quot%s/sequences.bed&quot % options.out_dir
    write_seqs_bed(seqs_bed_file, mseqs, True)

  else:
    &#47&#47 read from directory
    seqs_bed_file = &quot%s/sequences.bed&quot % options.out_dir
    unmap_npy = &quot%s/mseqs_unmap.npy&quot % options.out_dir
    mseqs = []
    fold_mseqs = []
    for fi in range(num_folds):
      fold_mseqs.append([])
    for line in open(seqs_bed_file):
      a = line.split()
      msg = ModelSeq(a[0], int(a[1]), int(a[2]), a[3])
      mseqs.append(msg)
      if a[3] == &quottrain&quot:
        fi = 0
      elif a[3] == &quotvalid&quot:
        fi = 1
      elif a[3] == &quottest&quot:
        fi = 2
      else:
        fi = int(a[3].replace(&quotfold&quot,&quot&quot))
      fold_mseqs[fi].append(msg)
        
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 read sequence coverage values
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  seqs_cov_dir = &quot%s/seqs_cov&quot % options.out_dir
  if not os.path.isdir(seqs_cov_dir):
    os.mkdir(seqs_cov_dir)

  read_jobs = []

  for ti in range(targets_df.shape[0]):
    genome_cov_file = targets_df[&quotfile&quot].iloc[ti]
    seqs_cov_stem = &quot%s/%d&quot % (seqs_cov_dir, ti)
    seqs_cov_file = &quot%s.h5&quot % seqs_cov_stem

    clip_ti = None
    if &quotclip&quot in targets_df.columns:
      clip_ti = targets_df[&quotclip&quot].iloc[ti]

    clipsoft_ti = None
    if &quotclip_soft&quot in targets_df.columns:
      clipsoft_ti = targets_df[&quotclip_soft&quot].iloc[ti]

    scale_ti = 1
    if &quotscale&quot in targets_df.columns:
      scale_ti = targets_df[&quotscale&quot].iloc[ti]

    if options.restart and os.path.isfile(seqs_cov_file):
      print(&quotSkipping existing %s&quot % seqs_cov_file, file=sys.stderr)
    else:
      cmd = &quotbasenji_data_read.py&quot
      &#47&#47 cmd += &quot --crop %d&quot % options.crop_bp
      cmd += &quot -w %d&quot % options.pool_width
      cmd += &quot -u %s&quot % targets_df[&quotsum_stat&quot].iloc[ti]
      if clip_ti is not None:
        cmd += &quot -c %f&quot % clip_ti
      if clipsoft_ti is not None:
        cmd += &quot --clip_soft %f&quot % clipsoft_ti
      cmd += &quot -s %f&quot % scale_ti
      if options.blacklist_bed:
        cmd += &quot -b %s&quot % options.blacklist_bed
      if options.interp_nan:
        cmd += &quot -i&quot
      cmd += &quot %s&quot % genome_cov_file
      cmd += &quot %s&quot % seqs_bed_file
      cmd += &quot %s&quot % seqs_cov_file

      if options.run_local:
        &#47&#47 breaks on some OS
        &#47&#47 cmd += &quot &&gt; %s.err&quot % seqs_cov_stem
        read_jobs.append(cmd)
      else:
        j = slurm.Job(cmd,
            name=&quotread_t%d&quot % ti,
            out_file=&quot%s.out&quot % seqs_cov_stem,
            err_file=&quot%s.err&quot % seqs_cov_stem,
            queue=&quotstandard&quot, mem=15000, time=&quot12:0:0&quot)
        read_jobs.append(j)

  if options.run_local:
    util.exec_par(read_jobs, options.processes, verbose=True)
  else:
    slurm.multi_run(read_jobs, options.processes, verbose=True,
                    launch_sleep=1, update_sleep=5)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 write TF Records
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 copy targets file
  shutil.copy(targets_file, &quot%s/targets.txt&quot % options.out_dir)

  &#47&#47 initialize TF Records dir
  tfr_dir = &quot%s/tfrecords&quot % options.out_dir
  if not os.path.isdir(tfr_dir):
    os.mkdir(tfr_dir)

  write_jobs = []

  for fold_set in fold_labels:
    fold_set_indexes = [i for i in range(len(mseqs)) if mseqs[i].label == fold_set]
    fold_set_start = fold_set_indexes[0]
    fold_set_end = fold_set_indexes[-1] + 1

    tfr_i = 0
    tfr_start = fold_set_start
    tfr_end = min(tfr_start+options.seqs_per_tfr, fold_set_end)

    while tfr_start &lt;= fold_set_end:
      tfr_stem = &quot%s/%s-%d&quot % (tfr_dir, fold_set, tfr_i)

      cmd = &quotbasenji_data_write.py&quot
      cmd += &quot -s %d&quot % tfr_start
      cmd += &quot -e %d&quot % tfr_end
      cmd += &quot --umap_clip %f&quot % options.umap_clip
      cmd += &quot -x %d&quot % options.crop_bp
      if options.umap_tfr:
        cmd += &quot --umap_tfr&quot
      if options.umap_bed is not None:
        cmd += &quot -u %s&quot % unmap_npy

      cmd += &quot %s&quot % fasta_file
      cmd += &quot %s&quot % seqs_bed_file
      cmd += &quot %s&quot % seqs_cov_dir
      cmd += &quot %s.tfr&quot % tfr_stem

      if options.run_local:
        &#47&#47 breaks on some OS
        &#47&#47 cmd += &quot &&gt; %s.err&quot % tfr_stem
        write_jobs.append(cmd)
      else:
        j = slurm.Job(cmd,
              name=&quotwrite_%s-%d&quot % (fold_set, tfr_i),
              out_file=&quot%s.out&quot % tfr_stem,
              err_file=&quot%s.err&quot % tfr_stem,
              queue=&quotstandard&quot, mem=15000, time=&quot12:0:0&quot)
        write_jobs.append(j)

      &#47&#47 update
      tfr_i += 1
      tfr_start += options.seqs_per_tfr
      tfr_end = min(tfr_start+options.seqs_per_tfr, fold_set_end)

  if options.run_local:
    util.exec_par(write_jobs, options.processes, verbose=True)
  else:
    slurm.multi_run(write_jobs, options.processes, verbose=True,
                    launch_sleep=1, update_sleep=5)


  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 stats
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  stats_dict = {}
  stats_dict[&quotnum_targets&quot] = targets_df.shape[0]
  stats_dict[&quotseq_length&quot] = options.seq_length
  stats_dict[&quotpool_width&quot] = options.pool_width
  stats_dict[&quotcrop_bp&quot] = options.crop_bp

  target_length = options.seq_length - 2*options.crop_bp
  target_length = target_length // options.pool_width
  stats_dict[&quottarget_length&quot] = target_length

  for fi in range(num_folds):
    stats_dict[&quot%s_seqs&quot % fold_labels[fi]] = len(fold_mseqs[fi])

  with open(&quot%s/statistics.json&quot % options.out_dir, &quotw&quot) as stats_json_out:
    json.dump(stats_dict, stats_json_out, indent=4)


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def annotate_unmap(mseqs, unmap_bed, seq_length, pool_width):
   Intersect the sequence segments with unmappable regions
         and annoate the segments as NaN to possible be ignored.

    Args:
      mseqs: list of ModelSeq&quots
      unmap_bed: unmappable regions BED file
      seq_length: sequence length (after cropping)
      pool_width: pooled bin width

    Returns:
      seqs_unmap: NxL binary NA indicators
    

  &#47&#47 print sequence segments to file
  seqs_temp = tempfile.NamedTemporaryFile()
  seqs_bed_file = seqs_temp.name
  write_seqs_bed(seqs_bed_file, mseqs)

  &#47&#47 hash segments to indexes
  chr_start_indexes = {}
  for i in range(len(mseqs)):
    chr_start_indexes[(mseqs[i].chr, mseqs[i].start)] = i

  &#47&#47 initialize unmappable array
  pool_seq_length = seq_length // pool_width
  seqs_unmap = np.zeros((len(mseqs), pool_seq_length), dtype=&quotbool&quot)

  &#47&#47 intersect with unmappable regions
  p = subprocess.Popen(
      &quotbedtools intersect -wo -a %s -b %s&quot % (seqs_bed_file, unmap_bed),
      shell=True, stdout=subprocess.PIPE)
  for line in p.stdout:
    line = line.decode(&quotutf-8&quot)
    a = line.split()

    seq_chrom = a[0]
    seq_start = int(a[1])
    seq_end = int(a[2])
    seq_key = (seq_chrom, seq_start)

    unmap_start = int(a[4])
    unmap_end = int(a[5])

    overlap_start = max(seq_start, unmap_start)
    overlap_end = min(seq_end, unmap_end)

    pool_seq_unmap_start = math.floor((overlap_start - seq_start) / pool_width)
    pool_seq_unmap_end = math.ceil((overlap_end - seq_start) / pool_width)

    &#47&#47 skip minor overlaps to the first
    first_start = seq_start + pool_seq_unmap_start * pool_width
    first_end = first_start + pool_width
    first_overlap = first_end - overlap_start
    if first_overlap &lt; 0.1 * pool_width:
      pool_seq_unmap_start += 1

    &#47&#47 skip minor overlaps to the last
    last_start = seq_start + (pool_seq_unmap_end - 1) * pool_width
    last_overlap = overlap_end - last_start
    if last_overlap &lt; 0.1 * pool_width:
      pool_seq_unmap_end -= 1

    seqs_unmap[chr_start_indexes[seq_key], pool_seq_unmap_start:pool_seq_unmap_end] = True
    assert(seqs_unmap[chr_start_indexes[seq_key], pool_seq_unmap_start:pool_seq_unmap_end].sum() == pool_seq_unmap_end-pool_seq_unmap_start)

  return seqs_unmap


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def break_large_contigs(contigs, break_t, verbose=False):
  Break large contigs in half until all contigs are under
     the size threshold.

  &#47&#47 initialize a heapq of contigs and lengths
  contig_heapq = []
  for ctg in contigs:
    ctg_len = ctg.end - ctg.start
    heapq.heappush(contig_heapq, (-ctg_len, ctg))

  ctg_len = break_t + 1
  while ctg_len &gt; break_t:

    &#47&#47 pop largest contig
    ctg_nlen, ctg = heapq.heappop(contig_heapq)
    ctg_len = -ctg_nlen

    &#47&#47 if too large
    if ctg_len &gt; break_t:
      if verbose:
        print(&quotBreaking %s:%d-%d (%d nt)&quot % (ctg.chr,ctg.start,ctg.end,ctg_len))

      &#47&#47 break in two
      ctg_mid = ctg.start + ctg_len//2

      try:
        ctg_left = Contig(ctg.genome, ctg.chr, ctg.start, ctg_mid)
        ctg_right = Contig(ctg.genome, ctg.chr, ctg_mid, ctg.end)
      except AttributeError:
        ctg_left = Contig(ctg.chr, ctg.start, ctg_mid)
        ctg_right = Contig(ctg.chr, ctg_mid, ctg.end)

      &#47&#47 add left
      ctg_left_len = ctg_left.end - ctg_left.start
      heapq.heappush(contig_heapq, (-ctg_left_len, ctg_left))

      &#47&#47 add right
      ctg_right_len = ctg_right.end - ctg_right.start
      heapq.heappush(contig_heapq, (-ctg_right_len, ctg_right))

  &#47&#47 return to list
  contigs = [len_ctg[1] for len_ctg in contig_heapq]

  return contigs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def contig_sequences(contigs, seq_length, stride, snap=1, label=None):
  &quot&quot&quot Break up a list of Contig&quots into a list of ModelSeq&quots. &quot&quot&quot
  mseqs = []
  for ctg in contigs:
    seq_start = int(np.ceil(ctg.start/snap)*snap)
    seq_end = seq_start + seq_length

    while seq_end &lt;= ctg.end:
      &#47&#47 record sequence
      mseqs.append(ModelSeq(ctg.chr, seq_start, seq_end, label))

      &#47&#47 update
      seq_start += stride
      seq_end += stride
      
  return mseqs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def curate_peaks(<a id="change">targets_df</a>, <a id="change">out_dir</a>, <a id="change">pool_width</a>, <a id="change">crop_bp</a>):
  Merge all peaks, round to nearest pool_width, and add cropped bp.

  &#47&#47 concatenate and extend peaks
  <a id="change">cat_bed_file = &quot%s/peaks_cat.bed&quot % out_dir</a>
  <a id="change">cat_bed_out = open(cat_bed_file, &quotw&quot)</a>
  for bed_file in targets_df.file:
    if bed_file[-3:] == &quot.gz&quot:
      <a id="change">bed_in = gzip.open(bed_file, &quotrt&quot)</a>
    else:
      <a id="change">bed_in = open(bed_file, &quotr&quot)</a>

    for line in bed_in:
      <a id="change">a = line.rstrip().split(&quot\t&quot)</a>
      <a id="change">chrm = a[0]</a>
      <a id="change">start = int(a[1])</a>
      <a id="change">end = int(a[2])</a>
      
      &#47&#47 extend to pool width
      <a id="change">length = end - start</a>
      if length &lt; pool_width:
        <a id="change">mid = (start + end) // 2</a>
        <a id="change">start = mid - pool_width//2</a>
        <a id="change">end = start + pool_width</a>

      &#47&#47 add cropped bp
      <a id="change">start = max(0, start-crop_bp)</a>
      end += crop_bp

      &#47&#47 print
      print(&quot%s\t%d\t%d&quot % (chrm,start,end), file=cat_bed_out)

    bed_in.close()
  <a id="change">cat_bed_out</a><a id="change">.close()</a>

  &#47&#47 merge
  <a id="change">merge_bed_file = &quot%s/peaks_merge.bed&quot % out_dir</a>
  <a id="change">bedtools_cmd = &quotbedtools sort -i %s&quot % cat_bed_file</a>
  bedtools_cmd += &quot | bedtools merge -i - &gt; %s&quot % merge_bed_file
  subprocess.call(bedtools_cmd, shell=True)

  &#47&#47 round and add crop_bp
  <a id="change">full_bed_file = &quot%s/peaks_full.bed&quot % out_dir</a>
  <a id="change">full_bed_out = open(full_bed_file, &quotw&quot)</a>

  for line in open(merge_bed_file):
    <a id="change">a = line.rstrip().split(&quot\t&quot)</a>
    <a id="change">chrm = a[0]</a>
    <a id="change">start = int(a[1])</a>
    <a id="change">end = int(a[2])</a>
    <a id="change">mid = (start + end) // 2</a>
    <a id="change">length = end - start</a>

    &#47&#47 round length to nearest pool_width
    <a id="change">bins = int(np.round(length/pool_width))</a>
    assert(bins &gt; 0)
    <a id="change">start = mid - (bins*pool_width)//2</a>
    <a id="change">start = max(0, start)</a>
    <a id="change">end = start + (bins*pool_width)</a>

    &#47&#47 add cropped bp
    &#47&#47 start = max(0, start-crop_bp)
    &#47&#47 end += crop_bp

    &#47&#47 write
    print(&quot%s\t%d\t%d&quot % (chrm,start,end), file=full_bed_out)

  <a id="change">full_bed_out</a><a id="change">.close()</a>

  return full_bed_file


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def divide_contigs_chr(contigs, test_chrs, valid_chrs):
  Divide list of contigs into train/valid/test lists
     by chromosome.

  &#47&#47 initialize current train/valid/test nucleotides
  train_nt = 0
  valid_nt = 0
  test_nt = 0

  &#47&#47 initialize train/valid/test contig lists
  train_contigs = []
  valid_contigs = []
  test_contigs = []

  &#47&#47 process contigs
  for ctg in contigs:
    ctg_len = ctg.end - ctg.start

    if ctg.chr in test_chrs:
      test_contigs.append(ctg)
      test_nt += ctg_len
    elif ctg.chr in valid_chrs:
      valid_contigs.append(ctg)
      valid_nt += ctg_len
    else:
      train_contigs.append(ctg)
      train_nt += ctg_len

  total_nt = train_nt + valid_nt + test_nt

  print(&quotContigs divided into&quot)
  print(&quot Train: %5d contigs, %10d nt (%.4f)&quot % \
      (len(train_contigs), train_nt, train_nt/total_nt))
  print(&quot Valid: %5d contigs, %10d nt (%.4f)&quot % \
      (len(valid_contigs), valid_nt, valid_nt/total_nt))
  print(&quot Test:  %5d contigs, %10d nt (%.4f)&quot % \
      (len(test_contigs), test_nt, test_nt/total_nt))

  return [train_contigs, valid_contigs, test_contigs]


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def divide_contigs_folds(contigs, folds):
  Divide list of contigs into cross fold lists.

  &#47&#47 sort contigs descending by length
  length_contigs = [(ctg.end-ctg.start,ctg) for ctg in contigs]
  length_contigs.sort(reverse=True)

  &#47&#47 compute total nucleotides
  total_nt = sum([lc[0] for lc in length_contigs])

  &#47&#47 compute aimed fold nucleotides
  fold_nt_aim = int(np.ceil(total_nt / folds))

  &#47&#47 initialize current fold nucleotides
  fold_nt = np.zeros(folds)

  &#47&#47 initialize fold contig lists
  fold_contigs = []
  for fi in range(folds):
    fold_contigs.append([])

  &#47&#47 process contigs
  for ctg_len, ctg in length_contigs:

    &#47&#47 compute gap between current and aim
    fold_nt_gap = fold_nt_aim - fold_nt
    fold_nt_gap = np.clip(fold_nt_gap, 0, np.inf)

    &#47&#47 compute sample probability
    fold_prob = fold_nt_gap / fold_nt_gap.sum()

    &#47&#47 sample train/valid/test
    fi = np.random.choice(folds, p=fold_prob)
    fold_contigs[fi].append(ctg)
    fold_nt[fi] += ctg_len

  print(&quotContigs divided into&quot)
  for fi in range(folds):
    print(&quot Fold%d: %5d contigs, %10d nt (%.4f)&quot % \
      (fi, len(fold_contigs[fi]), fold_nt[fi], fold_nt[fi]/total_nt))

  return fold_contigs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def divide_contigs_pct(contigs, test_pct, valid_pct, pct_abstain=0.2):
  Divide list of contigs into train/valid/test lists,
     aiming for the specified nucleotide percentages.

  &#47&#47 sort contigs descending by length
  length_contigs = [(ctg.end-ctg.start,ctg) for ctg in contigs]
  length_contigs.sort(reverse=True)

  &#47&#47 compute total nucleotides
  total_nt = sum([lc[0] for lc in length_contigs])

  &#47&#47 compute aimed train/valid/test nucleotides
  test_nt_aim = test_pct * total_nt
  valid_nt_aim = valid_pct * total_nt
  train_nt_aim = total_nt - valid_nt_aim - test_nt_aim

  &#47&#47 initialize current train/valid/test nucleotides
  train_nt = 0
  valid_nt = 0
  test_nt = 0

  &#47&#47 initialize train/valid/test contig lists
  train_contigs = []
  valid_contigs = []
  test_contigs = []

  &#47&#47 process contigs
  for ctg_len, ctg in length_contigs:

    &#47&#47 compute gap between current and aim
    test_nt_gap = max(0, test_nt_aim - test_nt)
    valid_nt_gap = max(0, valid_nt_aim - valid_nt)
    train_nt_gap = max(1, train_nt_aim - train_nt)

    &#47&#47 skip if too large
    if ctg_len &gt; pct_abstain*test_nt_gap:
      test_nt_gap = 0
    if ctg_len &gt; pct_abstain*valid_nt_gap:
      valid_nt_gap = 0

    &#47&#47 compute remaining %
    gap_sum = train_nt_gap + valid_nt_gap + test_nt_gap
    test_pct_gap = test_nt_gap / gap_sum
    valid_pct_gap = valid_nt_gap / gap_sum
    train_pct_gap = train_nt_gap / gap_sum

    &#47&#47 sample train/valid/test
    ri = np.random.choice(range(3), 1, p=[train_pct_gap, valid_pct_gap, test_pct_gap])[0]
    if ri == 0:
      train_contigs.append(ctg)
      train_nt += ctg_len
    elif ri == 1:
      valid_contigs.append(ctg)
      valid_nt += ctg_len
    elif ri == 2:
      test_contigs.append(ctg)
      test_nt += ctg_len
    else:
      print(&quotTVT random number beyond 0,1,2&quot, file=sys.stderr)
      exit(1)

  print(&quotContigs divided into&quot)
  print(&quot Train: %5d contigs, %10d nt (%.4f)&quot % \
      (len(train_contigs), train_nt, train_nt/total_nt))
  print(&quot Valid: %5d contigs, %10d nt (%.4f)&quot % \
      (len(valid_contigs), valid_nt, valid_nt/total_nt))
  print(&quot Test:  %5d contigs, %10d nt (%.4f)&quot % \
      (len(test_contigs), test_nt, test_nt/total_nt))

  return [train_contigs, valid_contigs, test_contigs]


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def limit_contigs(contigs, filter_bed):
   Limit to contigs overlapping the given BED.

    Args
     contigs: list of Contigs
     filter_bed: BED file to filter by

    Returns:
     fcontigs: list of Contigs
    

  &#47&#47 print ctgments to BED
  ctg_fd, ctg_bed_file = tempfile.mkstemp()
  ctg_bed_out = open(ctg_bed_file, &quotw&quot)
  for ctg in contigs:
    print(&quot%s\t%d\t%d&quot % (ctg.chr, ctg.start, ctg.end), file=ctg_bed_out)
  ctg_bed_out.close()

  &#47&#47 intersect w/ filter_bed
  fcontigs = []
  p = subprocess.Popen(
      &quotbedtools intersect -a %s -b %s&quot % (ctg_bed_file, filter_bed),
      shell=True,
      stdout=subprocess.PIPE)
  for line in p.stdout:
    a = line.decode(&quotutf-8&quot).split()
    chrom = a[0]
    ctg_start = int(a[1])
    ctg_end = int(a[2])
    fcontigs.append(Contig(chrom, ctg_start, ctg_end))

  p.communicate()

  os.close(ctg_fd)
  os.remove(ctg_bed_file)

  return fcontigs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def rejoin_large_contigs(contigs):
   Rejoin large contigs that were broken up before alignment comparison.

  &#47&#47 split list by chromosome
  chr_contigs = {}
  for ctg in contigs:
    chr_contigs.setdefault(ctg.chr,[]).append(ctg)

  contigs = []
  for chrm in chr_contigs:
    &#47&#47 sort within chromosome
    chr_contigs[chrm].sort(key=lambda x: x.start)

    ctg_ongoing = chr_contigs[chrm][0]
    for i in range(1, len(chr_contigs[chrm])):
      ctg_this = chr_contigs[chrm][i]
      if ctg_ongoing.end == ctg_this.start:
        &#47&#47 join
        &#47&#47 ctg_ongoing.end = ctg_this.end
        ctg_ongoing = ctg_ongoing._replace(end=ctg_this.end)
      else:
        &#47&#47 conclude ongoing
        contigs.append(ctg_ongoing)

        &#47&#47 move to next
        ctg_ongoing = ctg_this

    &#47&#47 conclude final
    contigs.append(ctg_ongoing)

  return contigs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def write_seqs_bed(bed_file, seqs, labels=False):
  &quot&quot&quotWrite sequences to BED file.&quot&quot&quot
  bed_out = open(bed_file, &quotw&quot)
  for i in range(len(seqs)):
    line = &quot%s\t%d\t%d&quot % (seqs[i].chr, seqs[i].start, seqs[i].end)
    if labels:
      line += &quot\t%s&quot % seqs[i].label
    print(line, file=bed_out)
  bed_out.close()

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
Contig = collections.namedtuple(&quotContig&quot, [&quotchr&quot, &quotstart&quot, &quotend&quot])
ModelSeq = collections.namedtuple(&quotModelSeq&quot, [&quotchr&quot, &quotstart&quot, &quotend&quot, &quotlabel&quot])


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
if __name__ == &quot__main__&quot:
  main()
</code></pre>