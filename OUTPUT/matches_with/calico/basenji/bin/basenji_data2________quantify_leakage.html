<link rel="stylesheet" href="../../..//default.css">
<script src="../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/calico/basenji/blob/master/bin/basenji_data2.py#L364">GitHubLink</a>


<a href="https://github.com/maldil/basenji/blob/master/bin/basenji_data2.py#L364">GitMyHubLink</a>

&#47&#47!/usr/bin/env python
&#47&#47 Copyright 2017 Calico LLC

&#47&#47 Licensed under the Apache License, Version 2.0 (the "License");
&#47&#47 you may not use this file except in compliance with the License.
&#47&#47 You may obtain a copy of the License at

&#47&#47     https://www.apache.org/licenses/LICENSE-2.0

&#47&#47 Unless required by applicable law or agreed to in writing, software
&#47&#47 distributed under the License is distributed on an "AS IS" BASIS,
&#47&#47 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
&#47&#47 See the License for the specific language governing permissions and
&#47&#47 limitations under the License.
&#47&#47 =========================================================================
from __future__ import print_function

from optparse import OptionParser
import collections
import gzip
import heapq
import json
import math
import pdb
import os
import random
import subprocess
import sys
import tempfile
import time

import h5py
import networkx as nx
import numpy as np
import pandas as pd
import pybedtools

from basenji import genome
from basenji import util
from basenji_data import annotate_unmap, rejoin_large_contigs

try:
  import slurm
except ModuleNotFoundError:
  pass

&quot&quot&quot
basenji_data2.py

Compute model sequences from the genome, extracting DNA coverage values.

DEPRACATED
-Use basenji_data_align.py and basenji_data.py --restart.
-See farm/analysis/5-22 for example.
&quot&quot&quot

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def main():
  usage = &quotusage: %prog [options] &lt;fasta0_file,fasta1_file&gt; &lt;targets_file&gt;&quot
  parser = OptionParser(usage)
  parser.add_option(&quot-a&quot, dest=&quotalign_net&quot,
      help=&quotAlignment .net file&quot)
  parser.add_option(&quot-b&quot, dest=&quotblacklist_beds&quot,
      help=&quotSet blacklist nucleotides to a baseline value.&quot)
  parser.add_option(&quot--break&quot, dest=&quotbreak_t&quot,
      default=None, type=&quotint&quot,
      help=&quotBreak in half contigs above length [Default: %default]&quot)
  parser.add_option(&quot-c&quot,&quot--crop&quot, dest=&quotcrop_bp&quot,
      default=0, type=&quotint&quot,
      help=&quotCrop bp off each end [Default: %default]&quot)
  parser.add_option(&quot-d&quot, dest=&quotsample_pct&quot,
      default=1.0, type=&quotfloat&quot,
      help=&quotDown-sample the segments&quot)
  parser.add_option(&quot-g&quot, dest=&quotgap_files&quot,
      help=&quotComma-separated list of assembly gaps BED files [Default: %default]&quot)
  parser.add_option(&quot-i&quot, dest=&quotinterp_nan&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotInterpolate NaNs [Default: %default]&quot) 
  parser.add_option(&quot-l&quot, dest=&quotseq_length&quot,
      default=131072, type=&quotint&quot,
      help=&quotSequence length [Default: %default]&quot)
  parser.add_option(&quot--local&quot, dest=&quotrun_local&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotRun jobs locally as opposed to on SLURM [Default: %default]&quot)
  parser.add_option(&quot-n&quot, dest=&quotnet_fill_min&quot,
    default=100000, type=&quotint&quot,
    help=&quotAlignment net fill size minimum [Default: %default]&quot)
  parser.add_option(&quot-o&quot, dest=&quotout_dir&quot,
      default=&quotdata_out&quot,
      help=&quotOutput directory [Default: %default]&quot)
  parser.add_option(&quot-p&quot, dest=&quotprocesses&quot,
      default=None, type=&quotint&quot,
      help=&quotNumber parallel processes [Default: %default]&quot)
  parser.add_option(&quot-r&quot, dest=&quotseqs_per_tfr&quot,
      default=256, type=&quotint&quot,
      help=&quotSequences per TFRecord file [Default: %default]&quot)
  parser.add_option(&quot--restart&quot, dest=&quotrestart&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotSkip already read HDF5 coverage values. [Default: %default]&quot)
  parser.add_option(&quot--seed&quot, dest=&quotseed&quot,
      default=44, type=&quotint&quot,
      help=&quotRandom seed [Default: %default]&quot)
  parser.add_option(&quot--snap&quot, dest=&quotsnap&quot,
      default=None, type=&quotint&quot,
      help=&quotSnap sequences to multiple of the given value [Default: %default]&quot)
  parser.add_option(&quot--stride&quot, &quot--stride_train&quot, dest=&quotstride_train&quot,
      default=1., type=&quotfloat&quot,
      help=&quotStride to advance train sequences [Default: seq_length]&quot)
  parser.add_option(&quot--stride_test&quot, dest=&quotstride_test&quot,
      default=1., type=&quotfloat&quot,
      help=&quotStride to advance valid and test sequences [Default: %default]&quot)
  parser.add_option(&quot--soft&quot, dest=&quotsoft_clip&quot,
      default=False, action=&quotstore_true&quot,
      help=&quotSoft clip values, applying sqrt to the execess above the threshold [Default: %default]&quot)
  parser.add_option(&quot-t&quot, dest=&quottest_pct&quot,
      default=0.1, type=&quotfloat&quot,
      help=&quotProportion of the data for testing [Default: %default]&quot)
  parser.add_option(&quot-u&quot, dest=&quotumap_beds&quot,
      help=&quotComma-separated genome unmappable segments to set to NA&quot)
  parser.add_option(&quot--umap_t&quot, dest=&quotumap_t&quot,
      default=0.5, type=&quotfloat&quot,
      help=&quotRemove sequences with more than this unmappable bin % [Default: %default]&quot)
  parser.add_option(&quot--umap_clip&quot, dest=&quotumap_clip&quot,
      default=None, type=&quotfloat&quot,
      help=&quotClip unmappable regions to this percentile in the sequences\&quot distribution of values&quot)
  parser.add_option(&quot-w&quot, dest=&quotpool_width&quot,
      default=128, type=&quotint&quot,
      help=&quotSum pool width [Default: %default]&quot)
  parser.add_option(&quot-v&quot, dest=&quotvalid_pct&quot,
      default=0.1, type=&quotfloat&quot,
      help=&quotProportion of the data for validation [Default: %default]&quot)
  (options, args) = parser.parse_args()

  if len(args) != 2:
    parser.error(&quotMust provide FASTA and sample coverage label and path files for two genomes.&quot)
  else:
    fasta_files = args[0].split(&quot,&quot)
    targets_file = args[1]

  &#47&#47 there is still some source of stochasticity
  random.seed(options.seed)
  np.random.seed(options.seed)

  &#47&#47 transform proportion strides to base pairs
  if options.stride_train &lt;= 1:
    print(&quotstride_train %.f&quot%options.stride_train, end=&quot&quot)
    options.stride_train = options.stride_train*options.seq_length
    print(&quot converted to %f&quot % options.stride_train)
  options.stride_train = int(np.round(options.stride_train))
  if options.stride_test &lt;= 1:
    print(&quotstride_test %.f&quot%options.stride_test, end=&quot&quot)
    options.stride_test = options.stride_test*options.seq_length
    print(&quot converted to %f&quot % options.stride_test)
  options.stride_test = int(np.round(options.stride_test))

  &#47&#47 check snap
  if options.snap is not None:
    if np.mod(options.seq_length, options.snap) != 0: 
      raise ValueError(&quotseq_length must be a multiple of snap&quot)
    if np.mod(options.stride_train, options.snap) != 0: 
      raise ValueError(&quotstride_train must be a multiple of snap&quot)
    if np.mod(options.stride_test, options.snap) != 0:
      raise ValueError(&quotstride_test must be a multiple of snap&quot)

  if os.path.isdir(options.out_dir) and not options.restart:
    print(&quotRemove output directory %s or use --restart option.&quot % options.out_dir)
    exit(1)
  elif not os.path.isdir(options.out_dir):
    os.mkdir(options.out_dir)

  if options.gap_files is not None:
    options.gap_files = options.gap_files.split(&quot,&quot)

  if options.blacklist_beds is not None:
    options.blacklist_beds = options.blacklist_beds.split(&quot,&quot)

  &#47&#47 read targets
  targets_df = pd.read_table(targets_file, index_col=0)

  &#47&#47 verify genomes
  num_genomes = len(fasta_files)
  assert(len(set(targets_df.genome)) == num_genomes)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 define genomic contigs
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  genome_chr_contigs = []
  for gi in range(num_genomes):
    genome_chr_contigs.append(genome.load_chromosomes(fasta_files[gi]))

    &#47&#47 remove gaps
    if options.gap_files[gi]:
      genome_chr_contigs[gi] = genome.split_contigs(genome_chr_contigs[gi],
                                                    options.gap_files[gi])

  &#47&#47 ditch the chromosomes
  contigs = []
  for gi in range(num_genomes):
    for chrom in genome_chr_contigs[gi]:
      contigs += [Contig(gi, chrom, ctg_start, ctg_end)
                  for ctg_start, ctg_end in genome_chr_contigs[gi][chrom]]

  &#47&#47 filter for large enough
  contigs = [ctg for ctg in contigs if ctg.end - ctg.start &gt;= options.seq_length]

  &#47&#47 break up large contigs
  if options.break_t is not None:
    contigs = break_large_contigs(contigs, options.break_t)

  &#47&#47 print contigs to BED file
  for gi in range(num_genomes):
    contigs_i = [ctg for ctg in contigs if ctg.genome == gi]
    ctg_bed_file = &quot%s/contigs%d.bed&quot % (options.out_dir, gi)
    write_seqs_bed(ctg_bed_file, contigs_i)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 divide between train/valid/test
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47

  &#47&#47 connect contigs across genomes by alignment
  contig_components = connect_contigs(contigs, options.align_net, options.net_fill_min, options.out_dir)

  &#47&#47 divide contig connected components between train/valid/test
  contig_sets = divide_contig_components(contig_components, options.test_pct, options.valid_pct)
  train_contigs, valid_contigs, test_contigs = contig_sets

  &#47&#47 rejoin broken contigs within set
  train_contigs = rejoin_large_contigs(train_contigs)
  valid_contigs = rejoin_large_contigs(valid_contigs)
  test_contigs = rejoin_large_contigs(test_contigs)

  &#47&#47 quantify leakage across sets
  quantify_leakage(options.align_net, train_contigs, valid_contigs, test_contigs, options.out_dir)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 define model sequences
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47

  &#47&#47 stride sequences across contig
  train_mseqs = contig_sequences(train_contigs, options.seq_length,
                                 options.stride_train, options.snap, &quottrain&quot)
  valid_mseqs = contig_sequences(valid_contigs, options.seq_length,
                                 options.stride_test, options.snap, &quotvalid&quot)
  test_mseqs = contig_sequences(test_contigs, options.seq_length,
                                options.stride_test, options.snap, &quottest&quot)

  &#47&#47 shuffle
  random.shuffle(train_mseqs)
  random.shuffle(valid_mseqs)
  random.shuffle(test_mseqs)

  &#47&#47 down-sample
  if options.sample_pct &lt; 1.0:
    train_mseqs = random.sample(train_mseqs, int(options.sample_pct*len(train_mseqs)))
    valid_mseqs = random.sample(valid_mseqs, int(options.sample_pct*len(valid_mseqs)))
    test_mseqs = random.sample(test_mseqs, int(options.sample_pct*len(test_mseqs)))

  &#47&#47 merge
  mseqs = train_mseqs + valid_mseqs + test_mseqs

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 separate sequences by genome
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  mseqs_genome = []
  for gi in range(num_genomes):
    mseqs_gi = [mseqs[si] for si in range(len(mseqs)) if mseqs[si].genome == gi]
    mseqs_genome.append(mseqs_gi)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 mappability
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47

  options.umap_beds = options.umap_beds.split(&quot,&quot)
  unmap_npys = [None, None]

  for gi in range(num_genomes):
    if options.umap_beds[gi] is not None:
      &#47&#47 annotate unmappable positions
      mseqs_unmap = annotate_unmap(mseqs_genome[gi], options.umap_beds[gi],
                                   options.seq_length, options.pool_width)

      &#47&#47 filter unmappable
      mseqs_map_mask = (mseqs_unmap.mean(axis=1, dtype=&quotfloat64&quot) &lt; options.umap_t)
      mseqs_genome[gi] = [mseqs_genome[gi][si] for si in range(len(mseqs_genome[gi])) if mseqs_map_mask[si]]
      mseqs_unmap = mseqs_unmap[mseqs_map_mask,:]

      &#47&#47 write to file
      unmap_npys[gi] = &quot%s/mseqs%d_unmap.npy&quot % (options.out_dir, gi)
      np.save(unmap_npys[gi], mseqs_unmap)

  seqs_bed_files = []
  for gi in range(num_genomes):
    &#47&#47 write sequences to BED
    seqs_bed_files.append(&quot%s/sequences%d.bed&quot % (options.out_dir, gi))
    write_seqs_bed(seqs_bed_files[gi], mseqs_genome[gi], True)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 read sequence coverage values
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  seqs_cov_dir = &quot%s/seqs_cov&quot % options.out_dir
  if not os.path.isdir(seqs_cov_dir):
    os.mkdir(seqs_cov_dir)

  read_jobs = []
  for gi in range(num_genomes):
    read_jobs += make_read_jobs(seqs_bed_files[gi], targets_df,
                                gi, seqs_cov_dir, options)

  if options.run_local:
    util.exec_par(read_jobs, options.processes, verbose=True)
  else:
    slurm.multi_run(read_jobs, options.processes, verbose=True,
                    launch_sleep=1, update_sleep=5)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 write TF Records
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47

  tfr_dir = &quot%s/tfrecords&quot % options.out_dir
  if not os.path.isdir(tfr_dir):
    os.mkdir(tfr_dir)

  &#47&#47 set genome target index starts
  sum_targets = 0
  genome_targets_start = []
  for gi in range(num_genomes):
    genome_targets_start.append(sum_targets)
    targets_df_gi = targets_df[targets_df.genome == gi]
    sum_targets += targets_df_gi.shape[0]

  write_jobs = []
  for gi in range(num_genomes):
    write_jobs += make_write_jobs(mseqs_genome[gi], fasta_files[gi], seqs_bed_files[gi],
                                  seqs_cov_dir, tfr_dir, gi, unmap_npys[gi],
                                  genome_targets_start[gi], sum_targets, options)

  if options.run_local:
    util.exec_par(write_jobs, options.processes, verbose=True)
  else:
    slurm.multi_run(write_jobs, options.processes, verbose=True,
                    launch_sleep=1, update_sleep=5)

  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  &#47&#47 stats
  &#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
  stats_dict = {}
  &#47&#47 stats_dict[&quotnum_targets&quot] = targets_df.shape[0]
  &#47&#47 stats_dict[&quottrain_seqs&quot] = len(train_mseqs)
  &#47&#47 stats_dict[&quotvalid_seqs&quot] = len(valid_mseqs)
  &#47&#47 stats_dict[&quottest_seqs&quot] = len(test_mseqs)
  stats_dict[&quotseq_length&quot] = options.seq_length
  stats_dict[&quotpool_width&quot] = options.pool_width
  stats_dict[&quotcrop_bp&quot] = options.crop_bp

  target_length = options.seq_length - 2*options.crop_bp
  target_length = target_length // options.pool_width
  stats_dict[&quottarget_length&quot] = target_length

  with open(&quot%s/statistics.json&quot % options.out_dir, &quotw&quot) as stats_json_out:
    json.dump(stats_dict, stats_json_out, indent=4)


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def quantify_leakage(<a id="change">align_net_file</a>, <a id="change">train_contigs</a>, <a id="change">valid_contigs</a>, <a id="change">test_contigs</a>, <a id="change">out_dir</a>):
  Quanitfy the leakage across sequence sets.

  def split_genome(contigs):
    genome_contigs = []
    for ctg in contigs:
      while len(genome_contigs) &lt;= ctg.genome:
        genome_contigs.append([])
      genome_contigs[ctg.genome].append((ctg.chr,ctg.start,ctg.end))
    genome_bedtools = [pybedtools.BedTool(ctgs) for ctgs in genome_contigs]
    return genome_bedtools

  def bed_sum(overlaps):
    osum = 0
    for overlap in overlaps:
      osum += int(overlap[2]) - int(overlap[1])
    return osum

  <a id="change">train0_bt, train1_bt = split_genome(train_contigs)</a>
  <a id="change">valid0_bt, valid1_bt = split_genome(valid_contigs)</a>
  <a id="change">test0_bt, test1_bt = split_genome(test_contigs)</a>

  <a id="change">assign0_sums = {}</a>
  <a id="change">assign1_sums = {}</a>

  if os.path.splitext(align_net_file)[-1] == &quot.gz&quot:
    <a id="change">align_net_open = gzip.open(align_net_file, &quotrt&quot)</a>
  else:
    <a id="change">align_net_open = open(align_net_file, &quotr&quot)</a>

  for net_line in align_net_open:
    if net_line.startswith(&quotnet&quot):
      <a id="change">net_a = net_line.split()</a>
      <a id="change">chrom0 = net_a[1]</a>

    elif net_line.startswith(&quot fill&quot):
      <a id="change">net_a = net_line.split()</a>

      &#47&#47 extract genome1 interval
      <a id="change">start0 = int(net_a[1])</a>
      <a id="change">size0 = int(net_a[2])</a>
      <a id="change">end0 = start0+size0</a>
      <a id="change">align0_bt = pybedtools.BedTool([(chrom0,start0,end0)])</a> 

      &#47&#47 extract genome2 interval
      <a id="change">chrom1 = net_a[3]</a>
      <a id="change">start1 = int(net_a[5])</a>
      <a id="change">size1 = int(net_a[6])</a>
      <a id="change">end1 = start1+size1</a>
      <a id="change">align1_bt = pybedtools.BedTool([(chrom1,start1,end1)])</a>

      &#47&#47 count interval overlap
      <a id="change">align0_train_bp = bed_sum(align0_bt.intersect(train0_bt))</a>
      <a id="change">align0_valid_bp = bed_sum(align0_bt.intersect(valid0_bt))</a>
      <a id="change">align0_test_bp = bed_sum(align0_bt.intersect(test0_bt))</a>
      <a id="change">align0_max_bp = max(align0_train_bp, align0_valid_bp, align0_test_bp)</a>

      <a id="change">align1_train_bp = bed_sum(align1_bt.intersect(train1_bt))</a>
      <a id="change">align1_valid_bp = bed_sum(align1_bt.intersect(valid1_bt))</a>
      <a id="change">align1_test_bp = bed_sum(align1_bt.intersect(test1_bt))</a>
      <a id="change">align1_max_bp = max(align1_train_bp, align1_valid_bp, align1_test_bp)</a>

      &#47&#47 assign to class
      if align0_max_bp == 0:
        <a id="change">assign0 = None</a>
      elif align0_train_bp == align0_max_bp:
        <a id="change">assign0 = &quottrain&quot</a>
      elif align0_valid_bp == align0_max_bp:
        <a id="change">assign0 = &quotvalid&quot</a>
      elif align0_test_bp == align0_max_bp:
        <a id="change">assign0 = &quottest&quot</a>
      else:
        print(&quotBad logic&quot)
        exit(1)

      if align1_max_bp == 0:
        <a id="change">assign1 = None</a>
      elif align1_train_bp == align1_max_bp:
        <a id="change">assign1 = &quottrain&quot</a>
      elif align1_valid_bp == align1_max_bp:
        <a id="change">assign1 = &quotvalid&quot</a>
      elif align1_test_bp == align1_max_bp:
        <a id="change">assign1 = &quottest&quot</a>
      else:
        print(&quotBad logic&quot)
        exit(1)

      &#47&#47 increment
      <a id="change">assign0_sums[(assign0,assign1)] = assign0_sums.get((assign0,assign1),0) + align0_max_bp</a>
      <a id="change">assign1_sums[(assign0,assign1)] = assign1_sums.get((assign0,assign1),0) + align1_max_bp</a>

  &#47&#47 sum contigs
  <a id="change">splits0_bp = {}</a>
  <a id="change">splits0_bp[&quottrain&quot] = bed_sum(train0_bt)</a>
  <a id="change">splits0_bp[&quotvalid&quot] = bed_sum(valid0_bt)</a>
  <a id="change">splits0_bp[&quottest&quot] = bed_sum(test0_bt)</a>
  <a id="change">splits1_bp = {}</a>
  <a id="change">splits1_bp[&quottrain&quot] = bed_sum(train1_bt)</a>
  <a id="change">splits1_bp[&quotvalid&quot] = bed_sum(valid1_bt)</a>
  <a id="change">splits1_bp[&quottest&quot] = bed_sum(test1_bt)</a>

  <a id="change">leakage_out = open(&quot%s/leakage.txt&quot % options.out_dir, &quotw&quot)</a>
  print(&quotGenome0&quot, file=leakage_out)
  for split0 in [&quottrain&quot,&quotvalid&quot,&quottest&quot]:
    print(&quot  %5s: %10d nt&quot % (split0, splits0_bp[split0]), file=leakage_out)
    for split1 in [&quottrain&quot,&quotvalid&quot,&quottest&quot,None]:
      <a id="change">ss_bp = assign0_sums.get((split0,split1),0)</a>
      print(&quot    %5s: %10d (%.5f)&quot % (split1, ss_bp, ss_bp/splits0_bp[split0]), file=leakage_out)
  print(&quot\nGenome1&quot, file=leakage_out)
  for split1 in [&quottrain&quot,&quotvalid&quot,&quottest&quot]:
    print(&quot  %5s: %10d nt&quot % (split1, splits1_bp[split1]), file=leakage_out)
    for split0 in [&quottrain&quot,&quotvalid&quot,&quottest&quot,None]:
      <a id="change">ss_bp = assign1_sums.get((split0,split1),0)</a>
      print(&quot    %5s: %10d (%.5f)&quot % (split0, ss_bp, ss_bp/splits1_bp[split1]), file=leakage_out)
  <a id="change">leakage_out</a><a id="change">.close()</a>


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def break_large_contigs(contigs, break_t, verbose=False):
  Break large contigs in half until all contigs are under
     the size threshold.

  &#47&#47 initialize a heapq of contigs and lengths
  contig_heapq = []
  for ctg in contigs:
    ctg_len = ctg.end - ctg.start
    heapq.heappush(contig_heapq, (-ctg_len, ctg))

  ctg_len = break_t + 1
  while ctg_len &gt; break_t:

    &#47&#47 pop largest contig
    ctg_nlen, ctg = heapq.heappop(contig_heapq)
    ctg_len = -ctg_nlen

    &#47&#47 if too large
    if ctg_len &gt; break_t:
      if verbose:
        print(&quotBreaking %s:%d-%d (%d nt)&quot % (ctg.chr,ctg.start,ctg.end,ctg_len))

      &#47&#47 break in two
      ctg_mid = ctg.start + ctg_len//2

      try:
        ctg_left = Contig(ctg.genome, ctg.chr, ctg.start, ctg_mid)
        ctg_right = Contig(ctg.genome, ctg.chr, ctg_mid, ctg.end)
      except AttributeError:
        ctg_left = Contig(ctg.chr, ctg.start, ctg_mid)
        ctg_right = Contig(ctg.chr, ctg_mid, ctg.end)

      &#47&#47 add left
      ctg_left_len = ctg_left.end - ctg_left.start
      heapq.heappush(contig_heapq, (-ctg_left_len, ctg_left))

      &#47&#47 add right
      ctg_right_len = ctg_right.end - ctg_right.start
      heapq.heappush(contig_heapq, (-ctg_right_len, ctg_right))

  &#47&#47 return to list
  contigs = [len_ctg[1] for len_ctg in contig_heapq]

  return contigs

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def contig_sequences(contigs, seq_length, stride, snap=None, label=None):
  &quot&quot&quot Break up a list of Contig&quots into a list of model length
       and stride sequence contigs.&quot&quot&quot
  mseqs = []

  for ctg in contigs:
    if snap is None:
      seq_start = ctg.start
    else:
      seq_start = int(np.ceil(ctg.start/snap)*snap)
    seq_end = seq_start + seq_length

    while seq_end &lt; ctg.end:
      &#47&#47 record sequence
      mseqs.append(ModelSeq(ctg.genome, ctg.chr, seq_start, seq_end, label))

      &#47&#47 update
      seq_start += stride
      seq_end += stride

  return mseqs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def connect_contigs(contigs, align_net_file, net_fill_min, out_dir):
  Connect contigs across genomes by forming a graph that includes
     net format aligning regions and contigs. Compute contig components
     as connected components of that graph.

  &#47&#47 construct align net graph and write net BEDs
  if align_net_file is None:
    graph_contigs_nets = nx.Graph()
  else:
    graph_contigs_nets = make_net_graph(align_net_file, net_fill_min, out_dir)

  &#47&#47 add contig nodes
  for ctg in contigs:
    ctg_node = GraphSeq(ctg.genome, False, ctg.chr, ctg.start, ctg.end)
    graph_contigs_nets.add_node(ctg_node)

  &#47&#47 intersect contigs BED w/ nets BED, adding graph edges.
  intersect_contigs_nets(graph_contigs_nets, 0, out_dir)
  intersect_contigs_nets(graph_contigs_nets, 1, out_dir)

  &#47&#47 find connected components
  contig_components = []
  for contig_net_component in nx.connected_components(graph_contigs_nets):
    &#47&#47 extract only the contigs
    cc_contigs = [contig_or_net for contig_or_net in contig_net_component if contig_or_net.net is False]

    if cc_contigs:
      &#47&#47 add to list
      contig_components.append(cc_contigs)

  &#47&#47 write summary stats
  comp_out = open(&quot%s/contig_components.txt&quot % out_dir, &quotw&quot)
  for ctg_comp in contig_components:
    ctg_comp0 = [ctg for ctg in ctg_comp if ctg.genome == 0]
    ctg_comp1 = [ctg for ctg in ctg_comp if ctg.genome == 1]
    ctg_comp0_nt = sum([ctg.end-ctg.start for ctg in ctg_comp0])
    ctg_comp1_nt = sum([ctg.end-ctg.start for ctg in ctg_comp1])
    ctg_comp_nt = ctg_comp0_nt + ctg_comp1_nt
    cols = [len(ctg_comp), len(ctg_comp0), len(ctg_comp1)]
    cols += [ctg_comp0_nt, ctg_comp1_nt, ctg_comp_nt]
    cols = [str(c) for c in cols]
    print(&quot\t&quot.join(cols), file=comp_out)
  comp_out.close()

  return contig_components


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def contig_stats_genome(contigs):
  Compute contig statistics within each genome.
  contigs_count_genome = []
  contigs_nt_genome = []

  contigs_genome_found = True
  gi = 0
  while contigs_genome_found:
    contigs_genome = [ctg for ctg in contigs if ctg.genome == gi]

    if len(contigs_genome) == 0:
      contigs_genome_found = False

    else:
      contigs_nt = [ctg.end-ctg.start for ctg in contigs_genome]

      contigs_count_genome.append(len(contigs_genome))
      contigs_nt_genome.append(sum(contigs_nt))

      gi += 1

  return contigs_count_genome, contigs_nt_genome


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def divide_contig_components(contig_components, test_pct, valid_pct, pct_abstain=0.5):
  Divide contig connected components into train/valid/test,
     and aiming for the specified nucleotide percentages.

  &#47&#47 sort contig components descending by length
  length_contig_components = []
  for cc_contigs in contig_components:
    cc_len = sum([ctg.end-ctg.start for ctg in cc_contigs])
    length_contig_components.append((cc_len, cc_contigs))
  length_contig_components.sort(reverse=True)

  &#47&#47 compute total nucleotides
  total_nt = sum([lc[0] for lc in length_contig_components])

  &#47&#47 compute aimed train/valid/test nucleotides
  test_nt_aim = test_pct * total_nt
  valid_nt_aim = valid_pct * total_nt
  train_nt_aim = total_nt - valid_nt_aim - test_nt_aim

  &#47&#47 initialize current train/valid/test nucleotides
  train_nt = 0
  valid_nt = 0
  test_nt = 0

  &#47&#47 initialie train/valid/test contig lists
  train_contigs = []
  valid_contigs = []
  test_contigs = []

  &#47&#47 process contigs
  for ctg_comp_len, ctg_comp in length_contig_components:
    &#47&#47 compute gap between current and aim
    test_nt_gap = max(0, test_nt_aim - test_nt)
    valid_nt_gap = max(0, valid_nt_aim - valid_nt)
    train_nt_gap = max(1, train_nt_aim - train_nt)

    &#47&#47 skip if too large
    if ctg_comp_len &gt; pct_abstain*test_nt_gap:
      test_nt_gap = 0
    if ctg_comp_len &gt; pct_abstain*valid_nt_gap:
      valid_nt_gap = 0

    &#47&#47 compute remaining %
    gap_sum = train_nt_gap + valid_nt_gap + test_nt_gap
    test_pct_gap = test_nt_gap / gap_sum
    valid_pct_gap = valid_nt_gap / gap_sum
    train_pct_gap = train_nt_gap / gap_sum

    &#47&#47 sample train/valid/test
    ri = np.random.choice(range(3), 1, p=[train_pct_gap, valid_pct_gap, test_pct_gap])[0]

    &#47&#47 collect contigs (sorted is required for deterministic sequence order)
    if ri == 0:
      for ctg in sorted(ctg_comp):
        train_contigs.append(ctg)
      train_nt += ctg_comp_len
    elif ri == 1:
      for ctg in sorted(ctg_comp):
        valid_contigs.append(ctg)
      valid_nt += ctg_comp_len
    elif ri == 2:
      for ctg in sorted(ctg_comp):
        test_contigs.append(ctg)
      test_nt += ctg_comp_len
    else:
      print(&quotTVT random number beyond 0,1,2&quot, file=sys.stderr)
      exit(1)

  &#47&#47 report genome-specific train/valid/test stats
  report_divide_stats(train_contigs, valid_contigs, test_contigs)

  return train_contigs, valid_contigs, test_contigs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def intersect_contigs_nets(graph_contigs_nets, genome_i, out_dir):
  Intersect the contigs and nets from genome_i, adding the
     overlaps as edges to graph_contigs_nets.

  contigs_file = &quot%s/contigs%d.bed&quot % (out_dir, genome_i)
  nets_file = &quot%s/nets%d.bed&quot % (out_dir, genome_i)

  contigs_bed = pybedtools.BedTool(contigs_file)
  nets_bed = pybedtools.BedTool(nets_file)

  for overlap in contigs_bed.intersect(nets_bed, wo=True):
    ctg_chr = overlap[0]
    ctg_start = int(overlap[1])
    ctg_end = int(overlap[2])
    net_chr = overlap[3]
    net_start = int(overlap[4])
    net_end = int(overlap[5])

    &#47&#47 create node objects
    ctg_node = GraphSeq(genome_i, False, ctg_chr, ctg_start, ctg_end)
    net_node = GraphSeq(genome_i, True, net_chr, net_start, net_end)

    &#47&#47 add edge / verify we found nodes
    gcn_size_pre = graph_contigs_nets.number_of_nodes()
    graph_contigs_nets.add_edge(ctg_node, net_node)
    gcn_size_post = graph_contigs_nets.number_of_nodes()
    assert(gcn_size_pre == gcn_size_post)


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def make_net_graph(align_net_file, net_fill_min, out_dir):
  Construct a Graph with aligned net intervals connected
     by edges.

  graph_nets = nx.Graph()

  nets1_bed_out = open(&quot%s/nets0.bed&quot % out_dir, &quotw&quot)
  nets2_bed_out = open(&quot%s/nets1.bed&quot % out_dir, &quotw&quot)

  if os.path.splitext(align_net_file)[-1] == &quot.gz&quot:
    align_net_open = gzip.open(align_net_file, &quotrt&quot)
  else:
    align_net_open = open(align_net_file, &quotr&quot)

  for net_line in align_net_open:
    if net_line.startswith(&quotnet&quot):
      net_a = net_line.split()
      chrom1 = net_a[1]

    elif net_line.startswith(&quot fill&quot):
      net_a = net_line.split()

      &#47&#47 extract genome1 interval
      start1 = int(net_a[1])
      size1 = int(net_a[2])
      end1 = start1+size1

      &#47&#47 extract genome2 interval
      chrom2 = net_a[3]
      start2 = int(net_a[5])
      size2 = int(net_a[6])
      end2 = start2+size2

      if min(size1, size2) &gt;= net_fill_min:
        &#47&#47 add edge
        net1_node = GraphSeq(0, True, chrom1, start1, end1)
        net2_node = GraphSeq(1, True, chrom2, start2, end2)
        graph_nets.add_edge(net1_node, net2_node)

        &#47&#47 write interval1
        cols = [chrom1, str(start1), str(end1)]
        print(&quot\t&quot.join(cols), file=nets1_bed_out)

        &#47&#47 write interval2
        cols = [chrom2, str(start2), str(end2)]
        print(&quot\t&quot.join(cols), file=nets2_bed_out)

  nets1_bed_out.close()
  nets2_bed_out.close()

  return graph_nets


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def make_read_jobs(seqs_bed_file, targets_df, gi, seqs_cov_dir, options):
  Make basenji_data_read.py jobs for one genome.

  &#47&#47 filter targets
  targets_df_gi = targets_df[targets_df.genome == gi]

  read_jobs = []

  for ti in range(targets_df_gi.shape[0]):
    genome_cov_file = targets_df_gi[&quotfile&quot].iloc[ti]
    seqs_cov_stem = &quot%s/%d-%d&quot % (seqs_cov_dir, gi, ti)
    seqs_cov_file = &quot%s.h5&quot % seqs_cov_stem

    clip_ti = None
    if &quotclip&quot in targets_df_gi.columns:
      clip_ti = targets_df_gi[&quotclip&quot].iloc[ti]

    scale_ti = 1
    if &quotscale&quot in targets_df_gi.columns:
      scale_ti = targets_df_gi[&quotscale&quot].iloc[ti]

    if options.restart and os.path.isfile(seqs_cov_file):
      print(&quotSkipping existing %s&quot % seqs_cov_file, file=sys.stderr)
    else:
      cmd = &quotbasenji_data_read.py&quot
      cmd += &quot --crop %d&quot % options.crop_bp
      cmd += &quot -u %s&quot % targets_df_gi[&quotsum_stat&quot].iloc[ti]
      cmd += &quot -w %d&quot % options.pool_width
      if clip_ti is not None:
        cmd += &quot -c %f&quot % clip_ti
      if options.soft_clip:
        cmd += &quot --soft&quot
      cmd += &quot -s %f&quot % scale_ti
      if options.blacklist_beds[gi]:
        cmd += &quot -b %s&quot % options.blacklist_beds[gi]
      if options.interp_nan:
        cmd += &quot -i&quot
      cmd += &quot %s&quot % genome_cov_file
      cmd += &quot %s&quot % seqs_bed_file
      cmd += &quot %s&quot % seqs_cov_file

      if options.run_local:
        cmd += &quot &&gt; %s.err&quot % seqs_cov_stem
        read_jobs.append(cmd)
      else:
        j = slurm.Job(cmd,
            name=&quotread_t%d&quot % ti,
            out_file=&quot%s.out&quot % seqs_cov_stem,
            err_file=&quot%s.err&quot % seqs_cov_stem,
            queue=&quotstandard&quot, mem=15000, time=&quot12:0:0&quot)
        read_jobs.append(j)

  return read_jobs

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def make_write_jobs(mseqs, fasta_file, seqs_bed_file, seqs_cov_dir, tfr_dir, gi,
                    unmap_npy, targets_start, sum_targets, options):
  Make basenji_data_write.py jobs for one genome.

  write_jobs = []

  for tvt_set in [&quottrain&quot, &quotvalid&quot, &quottest&quot]:
    tvt_set_indexes = [i for i in range(len(mseqs)) if mseqs[i].label == tvt_set]
    tvt_set_start = tvt_set_indexes[0]
    tvt_set_end = tvt_set_indexes[-1] + 1

    tfr_i = 0
    tfr_start = tvt_set_start
    tfr_end = min(tfr_start+options.seqs_per_tfr, tvt_set_end)

    while tfr_start &lt;= tvt_set_end:
      tfr_stem = &quot%s/%s-%d-%d&quot % (tfr_dir, tvt_set, gi, tfr_i)

      cmd = &quotbasenji_data_write.py&quot
      cmd += &quot -s %d&quot % tfr_start
      cmd += &quot -e %d&quot % tfr_end
      cmd += &quot -g %d&quot % gi
      cmd += &quot --ts %d&quot % targets_start
      cmd += &quot --te %d&quot % sum_targets
      cmd += &quot --umap_clip %f&quot % options.umap_clip
      if options.umap_tfr:
        cmd += &quot --umap_tfr&quot
      if unmap_npy is not None:
        cmd += &quot -u %s&quot % unmap_npy

      cmd += &quot %s&quot % fasta_file
      cmd += &quot %s&quot % seqs_bed_file
      cmd += &quot %s&quot % seqs_cov_dir
      cmd += &quot %s.tfr&quot % tfr_stem

      if options.run_local:
        cmd += &quot &&gt; %s.err&quot % tfr_stem
        write_jobs.append(cmd)
      else:
        j = slurm.Job(cmd,
              name=&quotwrite_%s-%d&quot % (tvt_set, tfr_i),
              out_file=&quot%s.out&quot % tfr_stem,
              err_file=&quot%s.err&quot % tfr_stem,
              queue=&quotstandard&quot, mem=15000, time=&quot12:0:0&quot)
        write_jobs.append(j)

      &#47&#47 update
      tfr_i += 1
      tfr_start += options.seqs_per_tfr
      tfr_end = min(tfr_start+options.seqs_per_tfr, tvt_set_end)

  return write_jobs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def rejoin_large_contigs(contigs):
   Rejoin large contigs that were broken up before alignment comparison.

  &#47&#47 split list by genome/chromosome
  gchr_contigs = {}
  for ctg in contigs:
    gchr = (ctg.genome, ctg.chr)
    gchr_contigs.setdefault(gchr,[]).append(ctg)

  contigs = []
  for gchr in gchr_contigs:
    &#47&#47 sort within chromosome
    gchr_contigs[gchr].sort(key=lambda x: x.start)
    &#47&#47 gchr_contigs[gchr] = sorted(gchr_contigs[gchr], key=lambda ctg: ctg.start)

    ctg_ongoing = gchr_contigs[gchr][0]
    for i in range(1, len(gchr_contigs[gchr])):
      ctg_this = gchr_contigs[gchr][i]
      if ctg_ongoing.end == ctg_this.start:
        &#47&#47 join
        &#47&#47 ctg_ongoing.end = ctg_this.end
        ctg_ongoing = ctg_ongoing._replace(end=ctg_this.end)
      else:
        &#47&#47 conclude ongoing
        contigs.append(ctg_ongoing)

        &#47&#47 move to next
        ctg_ongoing = ctg_this

    &#47&#47 conclude final
    contigs.append(ctg_ongoing)

  return contigs


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def report_divide_stats(train_contigs, valid_contigs, test_contigs):
   Report genome-specific statistics about the division of contigs
      between train/valid/test sets.

  &#47&#47 compute genome-specific stats
  train_count_genome, train_nt_genome = contig_stats_genome(train_contigs)
  valid_count_genome, valid_nt_genome = contig_stats_genome(valid_contigs)
  test_count_genome, test_nt_genome = contig_stats_genome(test_contigs)
  num_genomes = len(train_count_genome)

  &#47&#47 sum nt across genomes
  train_nt = sum(train_nt_genome)
  valid_nt = sum(valid_nt_genome)
  test_nt = sum(test_nt_genome)
  total_nt = train_nt + valid_nt + test_nt

  &#47&#47 compute total sum nt per genome
  total_nt_genome = []
  for gi in range(num_genomes):
    total_nt_gi = train_nt_genome[gi] + valid_nt_genome[gi] + test_nt_genome[gi]
    total_nt_genome.append(total_nt_gi)

  print(&quotContigs divided into&quot)
  print(&quot Train: %5d contigs, %10d nt (%.4f)&quot % \
       (len(train_contigs), train_nt, train_nt/total_nt))
  for gi in range(num_genomes):
    print(&quot  Genome%d: %5d contigs, %10d nt (%.4f)&quot % \
         (gi, train_count_genome[gi], train_nt_genome[gi], train_nt_genome[gi]/total_nt_genome[gi]))

  print(&quot Valid: %5d contigs, %10d nt (%.4f)&quot % \
      (len(valid_contigs), valid_nt, valid_nt/total_nt))
  for gi in range(num_genomes):
    print(&quot  Genome%d: %5d contigs, %10d nt (%.4f)&quot % \
         (gi, valid_count_genome[gi], valid_nt_genome[gi], valid_nt_genome[gi]/total_nt_genome[gi]))

  print(&quot Test:  %5d contigs, %10d nt (%.4f)&quot % \
      (len(test_contigs), test_nt, test_nt/total_nt))
  for gi in range(num_genomes):
    print(&quot  Genome%d: %5d contigs, %10d nt (%.4f)&quot % \
         (gi, test_count_genome[gi], test_nt_genome[gi], test_nt_genome[gi]/total_nt_genome[gi]))


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
def write_seqs_bed(bed_file, seqs, labels=False):
  &quot&quot&quotWrite sequences to BED file.&quot&quot&quot
  bed_out = open(bed_file, &quotw&quot)
  for i in range(len(seqs)):
    line = &quot%s\t%d\t%d&quot % (seqs[i].chr, seqs[i].start, seqs[i].end)
    if labels:
      line += &quot\t%s&quot % seqs[i].label
    print(line, file=bed_out)
  bed_out.close()


&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
Contig = collections.namedtuple(&quotContig&quot, [&quotgenome&quot, &quotchr&quot, &quotstart&quot, &quotend&quot])
ModelSeq = collections.namedtuple(&quotModelSeq&quot, [&quotgenome&quot, &quotchr&quot, &quotstart&quot, &quotend&quot, &quotlabel&quot])
GraphSeq = collections.namedtuple(&quotGraphSeq&quot, [&quotgenome&quot, &quotnet&quot, &quotchr&quot, &quotstart&quot, &quotend&quot])

&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47&#47
if __name__ == &quot__main__&quot:
  main()
</code></pre>