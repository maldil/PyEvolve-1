<link rel="stylesheet" href="../../..//default.css">
<script src="../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/google-research/google-research/blob/master/implicit_constrained_optimization/train_celeba_aucroc_joint.py#L466">GitHubLink</a>


<a href="https://github.com/maldil/google-research/blob/master/implicit_constrained_optimization/train_celeba_aucroc_joint.py#L466">GitMyHubLink</a>

&#47&#47 coding=utf-8
&#47&#47 Copyright 2022 The Google Research Authors.
&#47&#47
&#47&#47 Licensed under the Apache License, Version 2.0 (the "License");
&#47&#47 you may not use this file except in compliance with the License.
&#47&#47 You may obtain a copy of the License at
&#47&#47
&#47&#47     http://www.apache.org/licenses/LICENSE-2.0
&#47&#47
&#47&#47 Unless required by applicable law or agreed to in writing, software
&#47&#47 distributed under the License is distributed on an "AS IS" BASIS,
&#47&#47 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
&#47&#47 See the License for the specific language governing permissions and
&#47&#47 limitations under the License.

rTrain and evaluate models: Optimize for partial AUCROC.

It is possible to train a joint model for multiple attributes by specifying them
in the &quotattr&quot flag. It implements the method proposed in
https://arxiv.org/abs/2107.10960
for optimizing partial AUCROC. It also implements the two baselines of
cross-entropy
loss and pairwise proxy loss, which can be selected using the &quotmethod&quot flag.


import ast
import functools
import logging
import os
import sys
import time

from absl import app
from absl import flags
import ml_collections
import numpy as np
from sklearn import metrics
import tensorflow as tf
import tensorflow_datasets as tfds

from implicit_constrained_optimization import co_utils
from implicit_constrained_optimization import models

FLAGS = flags.FLAGS

flags.DEFINE_string(
    &quotmodel_dir&quot, &quot/usr/local/google/home/abhishk/logs/constrained_opt&quot,
    &quotThe directory where the model weights and &quot
    &quottraining/evaluation summaries are stored.&quot)
flags.DEFINE_string(
    &quotproxy_fn_obj&quot, &quotsigmoid&quot,
    &quotproxy function for 0-1 loss (objective): sigmoid/softplus&quot)
flags.DEFINE_string(
    &quotproxy_fn_cons&quot, &quotsigmoid&quot,
    &quotproxy function for 0-1 loss (constraint): sigmoid/softplus&quot)
flags.DEFINE_float(
    &quotsigmoid_temp&quot, 1.,
    &quotTemperature used in the sigmoid proxy for loss and constraint&quot)
flags.DEFINE_string(&quotmethod&quot, &quotico&quot, &quotloss type: ce/pairwise/ico&quot)
flags.DEFINE_string(&quotattr&quot, "[&quotHigh_Cheekbones&quot]",
                    &quotAttribute name(s) for CelebA, separeted by commas&quot)
flags.DEFINE_integer(&quotn_batches_for_threshold&quot, 100,
                     &quotNumber of batches used for computing theshold&quot)
flags.DEFINE_integer(
    &quotth_project_freq&quot, 1000,
    &quotNumber of minibatches after which threshold is set to the operating point&quot)
flags.DEFINE_integer(&quotgrad_update_threshold&quot, 0,
                     &quotUpdate threshold using gradient (0/1)&quot)
flags.DEFINE_float(&quotlr&quot, 0.001, &quotlearning rate&quot)
flags.DEFINE_integer(&quotbatch_size&quot, 512, &quotbatch size&quot)
flags.DEFINE_integer(&quottrain_iters&quot, 50000, &quottraining iterations&quot)
flags.DEFINE_float(&quotclip_grad_min_ratio&quot, 1e-5,
                   &quotlower limit on gradient of constraint wrt. threshold&quot)
flags.DEFINE_float(&quotfpr_low&quot, 0., &quotfpr lower limit for area under ROC curve&quot)
flags.DEFINE_float(&quotfpr_high&quot, 0.01, &quotfpr upper limit for area under ROC curve&quot)
flags.DEFINE_integer(&quotnum_bins&quot, 10, &quotnumber of bins&quot)
flags.DEFINE_integer(&quoteval_freq&quot, 317, &quotevaluation periodicity in iterations&quot)

num_classes = len(ast.literal_eval(FLAGS.attr))
num_eval_samples = 19962
bin_width = (FLAGS.fpr_high - FLAGS.fpr_low) / float(FLAGS.num_bins)
fpr_targets = np.arange(FLAGS.fpr_low, FLAGS.fpr_high,
                        bin_width) + bin_width / 2.


def preprocess(features):
  Preprocess function for images.
  image = features[&quotimage&quot]
  image = tf.image.resize_with_crop_or_pad(image, 160, 160)
  image = tf.image.resize(image, [32, 32])
  image = tf.cast(image, tf.float32) / 255.0
  label = tf.cast(
      tf.stack(
          [features[&quotattributes&quot][key] for key in ast.literal_eval(FLAGS.attr)],
          axis=1), tf.float32)
  return dict(image=image, label=label)


def get_dataset(batch_size, data=&quotceleb_a&quot):
  TFDS Dataset.
  ds, _ = tfds.load(data, split=&quottrain&quot, with_info=True)
  ds = ds.repeat().shuffle(
      batch_size * 4, seed=1).batch(
          batch_size,
          drop_remainder=True).prefetch(tf.data.experimental.AUTOTUNE)
  ds = ds.map(preprocess)

  ds_valid, _ = tfds.load(data, split=&quotvalidation&quot, with_info=True)
  ds_valid = ds_valid.shuffle(
      10000, seed=1).batch(
          batch_size,
          drop_remainder=False).prefetch(tf.data.experimental.AUTOTUNE)
  ds_valid = ds_valid.map(preprocess)

  ds_tst, _ = tfds.load(data, split=&quottest&quot, with_info=True)
  ds_tst = ds_tst.shuffle(
      10000, seed=1).batch(
          batch_size,
          drop_remainder=False).prefetch(tf.data.experimental.AUTOTUNE)
  ds_tst = ds_tst.map(preprocess)
  return ds, ds_valid, ds_tst


def ce_loss(labels, preds):
  Binary CrossEntropy Loss.
  bce = tf.keras.backend.binary_crossentropy(labels, preds)
  bce_mean = tf.reduce_mean(bce, axis=1)  &#47&#47 mean along label dimension
  return tf.reduce_sum(bce_mean)


def pairwise_auc_loss(labels,
                      logits,
                      beta=1.,
                      proxy=&quotsoftplus&quot,
                      temperature=1.):
  rComputes Partial ROC-AUC loss for the FPR range [0, beta].

  The partial AUC can be written as a pairwise loss between the positively
  labeled examples and the top beta-fraction of the negatively labeled examples.

    sum_{i \in Pos} sum_{j \in TopNeg} celoss(logit_i - logit_j).

  where i ranges over the positive datapoints, j ranges over the top beta
  fraction of the negative datapoints (sorted according to logits), logit_k
  denotes the logit (or score) of the k-th datapoint, and celoss is the sigmoid
  cross-entropy loss.

  Args:
    labels: A `Tensor` of shape [batch_size].
    logits: A `Tensor` with the same shape and dtype as `labels`.
    beta: A float value specifying an upper bound on the FPR. Defaults to 1.0
    proxy: Proxy function (softplus/sigmoid)
    temperature: Temperature parameter for the proxy function

  Returns:
    loss: A scalar loss `Tensor`.
  
  if (beta &lt;= 0.0) or (beta &gt; 1.0):
    raise ValueError("&quotbeta&quot needs to be in (0, 1].")

  &#47&#47 Convert inputs to tensors and standardize dtypes.
  labels = tf.reshape(tf.cast(labels, tf.float32), [-1, 1])
  logits = tf.reshape(tf.cast(logits, tf.float32), [-1, 1])

  &#47&#47 Separate out logits positively and negatively labeled examples.
  positive_logits = logits[labels &gt; 0]
  negative_logits = logits[labels &lt;= 0]

  &#47&#47 Pick top "beta" fraction of negatives to cover the FPR range [0, \beta].
  &#47&#47 Equivalently, we pick the top ceil(num_negatives x beta) negatives sorted by
  &#47&#47 logits.
  num_negatives = tf.reduce_sum(tf.cast(labels &lt;= 0, dtype=tf.float32))
  num_beta_negatives = tf.cast(tf.math.ceil(num_negatives * beta), tf.int32)
  sorted_negative_logits = tf.sort(negative_logits, direction=&quotDESCENDING&quot)
  top_negative_logits = sorted_negative_logits[0:num_beta_negatives]

  &#47&#47 Create tensors of pairwise differences between positive logits and top
  &#47&#47 negative logits. These have shapes [num_positives, num_beta_negatives].
  logits_difference = tf.expand_dims(positive_logits, 0) - tf.expand_dims(
      top_negative_logits, 1)

  &#47&#47 Calculate proxy loss.
  if proxy == &quotsoftplus&quot:
    loss = tf.nn.sigmoid_cross_entropy_with_logits(
        labels=tf.ones_like(logits_difference),
        logits=logits_difference * temperature)
  elif proxy == &quotsigmoid&quot:
    loss = tf.nn.sigmoid(-1 * logits_difference * temperature)
  else:
    raise ValueError(&quotUnknown proxy {}&quot.format(proxy))
  return tf.reduce_mean(loss)


def get_threshold_for_metric_ub(metric, target_val, thresholds):
  Compute threshold where metric is less than the target value.
  ind = np.array(metric) &lt;= target_val  &#47&#47 constraint less than target value
  ind_metric = np.where(metric == np.max(metric[ind]))[0][0]
  target_th = thresholds[ind_metric]
  return target_th


def update_threshold(model, train_iterator, num_batches, training):
  Update threshold such that constraints are satisfied on the specified number of minibatches.
  predictions = np.zeros((num_batches * FLAGS.batch_size, num_classes))
  labels = np.zeros((num_batches * FLAGS.batch_size, num_classes))
  th_updated = np.zeros((num_classes, FLAGS.num_bins))
  for i in range(num_batches):
    batch = next(train_iterator)
    images = batch[&quotimage&quot]
    predictions[i * FLAGS.batch_size:(i + 1) * FLAGS.batch_size] = model(
        [images, training])
    labels[i * FLAGS.batch_size:(i + 1) * FLAGS.batch_size] = batch[&quotlabel&quot]
  for j in range(num_classes):
    fprs, _, thresholds = metrics.roc_curve(labels[:, j], predictions[:, j])
    for k, fpr_target in enumerate(fpr_targets):
      th = get_threshold_for_metric_ub(fprs, fpr_target, thresholds)
      th_updated[j, k] = th
  return th_updated


def train_step(model, threshold_var, loss_op, constraint_op, optimizer,
               keras_metrics, images, labels):
  Performs single training step.
  with tf.GradientTape(persistent=True) as tape:
    predictions = model([images, True])  &#47&#47 batchsize x attrs
    if FLAGS.method == &quotaucroc&quot:
      tape.watch(threshold_var)
    if FLAGS.method == &quotce&quot:
      pred_probs = tf.sigmoid(predictions)
      loss = loss_op(labels, pred_probs) / FLAGS.batch_size
      constraint = tf.reduce_mean(
          constraint_op(
              tf.constant(
                  np.zeros((num_classes, FLAGS.num_bins), dtype=np.float32)),
              predictions, labels))
    elif FLAGS.method == &quotpairwise&quot:
      loss = loss_op(labels, predictions)
      constraint = tf.reduce_mean(
          constraint_op(
              tf.constant(
                  np.zeros((num_classes, FLAGS.num_bins), dtype=np.float32)),
              predictions, labels))
    else:
      with tf.GradientTape(persistent=True) as tape2:
        tape2.watch(threshold_var)
        loss_per_label = loss_op(threshold_var, predictions,
                                 labels)  &#47&#47 classes x bins
        loss = tf.reduce_mean(loss_per_label)
        constraint_per_label = constraint_op(threshold_var, predictions,
                                             labels)  &#47&#47 classes x bins
        constraint = tf.reduce_sum(constraint_per_label)
      with tape.stop_recording():
        grad_loss_th = tape2.gradient(loss, threshold_var)
        grad_constraint_th = tape2.gradient(constraint, threshold_var)
        ratio_grads_th = tf.math.divide_no_nan(
            -1 * grad_loss_th,
            tf.sign(grad_constraint_th + 1e-10) * tf.clip_by_value(
                tf.abs(grad_constraint_th), FLAGS.clip_grad_min_ratio,
                np.infty))
      constraint_weighted = tf.reduce_sum(
          tf.multiply(constraint_per_label, ratio_grads_th))
      del tape2

  if FLAGS.method == &quotce&quot or FLAGS.method == &quotpairwise&quot:
    grads = tape.gradient(loss, model.trainable_variables)
    optimizer.apply_gradients(zip(grads, model.trainable_variables))
  else:
    grad_loss_w = tape.gradient(loss, model.trainable_variables)
    grad_weighted_constraint_w = tape.gradient(constraint_weighted,
                                               model.trainable_variables)
    &#47&#47 final gradient wrt. model parameters
    final_grad_w = [
        grad_loss_w[i] + grad_weighted_constraint_w[i]
        for i in range(len(grad_loss_w))
    ]
    optimizer.apply_gradients(zip(final_grad_w, model.trainable_variables))
    if FLAGS.grad_update_threshold:
      &#47&#47 update threshold using forward pass
      predictions = model([images, False])  &#47&#47 batchsize x attrs
      th_updated = np.zeros((num_classes, FLAGS.num_bins), dtype=np.float32)
      for j in range(num_classes):
        fprs, _, thresholds = metrics.roc_curve(labels.numpy()[:, j],
                                                predictions.numpy()[:, j])
        for k, fpr_target in enumerate(fpr_targets):
          th = get_threshold_for_metric_ub(fprs, fpr_target, thresholds)
          th_updated[j, k] = th
      threshold_var.assign(th_updated)

  del tape

  &#47&#47 Update states.
  keras_metrics[&quottrain_loss&quot].update_state(loss)
  return loss, constraint


def eval_step(model, threshold_var, loss_op, constraint_op, proxy_funcs,
              keras_metrics, images, labels):
  Evaluation step on a minibatch.
  predictions = model([images, False])
  num_pos = tf.reduce_sum(labels, axis=0)
  num_neg = tf.reduce_sum(1 - labels, axis=0)
  if FLAGS.method == &quotce&quot:
    pred_probs = tf.sigmoid(predictions)
    loss = loss_op(labels, pred_probs) / FLAGS.batch_size
  elif FLAGS.method == &quotpairwise&quot:
    loss = loss_op(labels, predictions)
  else:
    loss_per_label = loss_op(threshold_var, predictions,
                             labels)  &#47&#47 classes x bins
    loss = tf.reduce_mean(loss_per_label)
  tp_exact = co_utils.tp_func_multi(threshold_var, predictions, labels)
  fp_exact = co_utils.fp_func_multi_th(threshold_var, predictions, labels)
  tp_proxy = proxy_funcs[&quottp&quot](threshold_var, predictions, labels)
  fp_proxy = proxy_funcs[&quotfp&quot](threshold_var, predictions, labels)
  constraint = constraint_op(threshold_var, predictions,
                             labels)  &#47&#47 classes x bins
  &#47&#47 Update states.
  keras_metrics[&quoteval_loss&quot].update_state(loss)
  return (predictions, loss, constraint, tp_exact, tp_proxy, fp_exact, fp_proxy,
          num_pos, num_neg)


def eval_metrics_on_data_source(ds,
                                model,
                                threshold_var,
                                loss_op,
                                constraint_op,
                                proxy_funcs,
                                keras_metrics,
                                target_th=None):
  Evaluate metric on the given TFDS data source.
  iterator = iter(ds)
  tp_total = np.zeros((num_classes, FLAGS.num_bins), dtype=np.float32)
  fp_total = np.zeros((num_classes, FLAGS.num_bins), dtype=np.float32)
  tp_proxy_total = np.zeros((num_classes, FLAGS.num_bins), dtype=np.float32)
  fp_proxy_total = np.zeros((num_classes, FLAGS.num_bins), dtype=np.float32)
  total_pos = np.zeros(num_classes, dtype=np.int32)
  total_neg = np.zeros(num_classes, dtype=np.int32)
  avg_loss = 0
  labels_all = np.zeros((num_eval_samples, num_classes), dtype=np.float32)
  preds_all = np.zeros((num_eval_samples, num_classes), dtype=np.float32)
  batch_ind = 0
  results = ml_collections.ConfigDict()
  for batch in iterator:
    &#47&#47 batch = next(iterator)
    images, labels = batch[&quotimage&quot], batch[&quotlabel&quot]
    labels_all[batch_ind * FLAGS.batch_size:batch_ind * FLAGS.batch_size +
               len(labels)] = labels.numpy()
    preds, loss, _, tp_exact, tp_proxy, fp_exact, fp_proxy, num_pos, num_neg = eval_step(
        model, threshold_var, loss_op, constraint_op, proxy_funcs,
        keras_metrics, images, labels)
    preds_all[batch_ind * FLAGS.batch_size:batch_ind * FLAGS.batch_size +
              len(preds)] = preds.numpy()
    tp_total += tp_exact.numpy()
    fp_total += fp_exact.numpy()
    tp_proxy_total += tp_proxy
    fp_proxy_total += fp_proxy
    total_pos += num_pos
    total_neg += num_neg
    avg_loss += loss
    batch_ind += 1
  results.fpr_exact = fp_total / np.expand_dims(total_neg, axis=1)
  results.tpr_exact = tp_total / np.expand_dims(total_pos, axis=1)
  results.tpr_proxy = tp_proxy_total / np.expand_dims(total_pos, axis=1)
  results.fpr_proxy = fp_proxy_total / np.expand_dims(total_neg, axis=1)
  results.avg_loss = avg_loss / batch_ind

  fprs_all_classes = []
  tprs_all_classes = []
  if target_th is None:
    target_th = np.zeros((num_classes, FLAGS.num_bins), dtype=np.float32)
    for i in range(num_classes):
      fprs, tprs, thresholds = metrics.roc_curve(labels_all[:, i], preds_all[:,
                                                                             i])
      fprs_all_classes.append(fprs)
      tprs_all_classes.append(tprs)
      for j, fpr_target in enumerate(fpr_targets):
        target_th[i, j] = get_threshold_for_metric_ub(fprs, fpr_target,
                                                      thresholds)
  results.tpr_exact_th = co_utils.tpr_func_multi(target_th, preds_all,
                                                 labels_all)
  results.fpr_exact_th = co_utils.fpr_func_multi_th(target_th, preds_all,
                                                    labels_all)
  results.tpr_proxy_th = proxy_funcs[&quottpr&quot](target_th, preds_all, labels_all)
  results.fpr_proxy_th = proxy_funcs[&quotfpr&quot](target_th, preds_all, labels_all)
  results.fprs_all = fprs_all_classes
  results.tprs_all = tprs_all_classes
  aucroc = np.zeros(num_classes, dtype=np.float32)
  for i in range(num_classes):
    aucroc[i] = metrics.roc_auc_score(
        labels_all[:, i], preds_all[:, i], max_fpr=FLAGS.fpr_high)
  results.aucroc = aucroc
  results.target_th = target_th
  return results


def eval_metrics(model, threshold_var, valid_ds, eval_ds, loss_op,
                 constraint_op, proxy_funcs, keras_metrics, it,
                 eval_summary_writer, best_results, logger, fp_log):
  Evaluate the current model on metrics of interest.

  def log_results(results):
    info_str = (&quotIt %s: loss=%s, aucroc=%s, fprs=%s&quot
                &quotthreshold=%s&quot) % (it, results.avg_loss, results.aucroc, [
                    round(t, 4) for t in results.fpr_exact[0]
                ], threshold_var.numpy()[0])
    logger.info(info_str)
    info_str = (&quotIt %s: At corrected threshold: aucroc=%s, fprs=%s, &quot
                &quotthreshold=%s&quot) % (it, results.aucroc, [
                    round(t, 4)
                    for t in np.mean(results.fpr_exact_th.numpy(), axis=0)
                ], [round(t, 4) for t in results.target_th[0]])
    logger.info(info_str)

  logger.info(&quot==================== Eval iter %d =====================&quot, it)
  &#47&#47 validation set
  valid_results = eval_metrics_on_data_source(valid_ds, model, threshold_var,
                                              loss_op, constraint_op,
                                              proxy_funcs, keras_metrics)
  logger.info(&quotValidation set -------&quot)
  log_results(valid_results)
  &#47&#47 test set
  eval_results = eval_metrics_on_data_source(eval_ds, model, threshold_var,
                                             loss_op, constraint_op,
                                             proxy_funcs, keras_metrics)
  logger.info(&quot\nTest set (at tuned threshold) -------&quot)
  log_results(eval_results)
  &#47&#47 best eval result so far
  if np.mean(best_results.aucroc_valid) &lt; np.mean(valid_results.aucroc):
    best_results.aucroc_valid = valid_results.aucroc
    best_results.tprs_valid = valid_results.tpr_exact_th.numpy()
    best_results.aucroc_eval_at_valid = eval_results.aucroc
    best_results.tprs_eval_at_valid = eval_results.tpr_exact_th.numpy()
    best_results.eval_fprs_valid = eval_results.fprs_all
    best_results.eval_tprs_valid = eval_results.tprs_all
  info_str = (
      &quot\nFor best validated model:\n (@ &quot
      &quottuned th) aucroc=%s, tprs=%s&quot) % (best_results.aucroc_eval_at_valid,
                                         best_results.tprs_eval_at_valid)
  fp_log.write(info_str + &quot\n&quot)
  logger.info(info_str)
  logger.info(&quot========================================&quot)

  with eval_summary_writer.as_default():
    tf.summary.scalar(&quotloss/eval&quot, eval_results.avg_loss, step=it)
    for i in range(num_classes):
      tf.summary.scalar(
          &quotaucroc/eval_at_best_valid_&quot + ast.literal_eval(FLAGS.attr)[i],
          best_results.aucroc_eval_at_valid[i],
          step=it)
    tf.summary.scalar(
        &quotaucroc/eval_at_best_valid&quot,
        np.mean(best_results.aucroc_eval_at_valid),
        step=it)
    for i in range(FLAGS.num_bins):
      tf.summary.scalar(
          &quottprs/eval_at_best_valid_bin&quot + str(i),
          np.mean(best_results.tprs_eval_at_valid[:, i]),
          step=it)

  keras_metrics[&quoteval_loss&quot].reset_states()
  keras_metrics[&quoteval_acc&quot].reset_states()
  return best_results


def train_and_eval_model(logger):
  Trains and evaluates the model.

  &#47&#47 discretize the FPR axis and set FPR targets
  num_bins = FLAGS.num_bins

  threshold_new = np.zeros((num_classes, num_bins), dtype=np.float32)

  train_ds, valid_ds, eval_ds = get_dataset(batch_size=FLAGS.batch_size)

  &#47&#47 Building model.
  logger.info(&quotBuilding model...&quot)
  if FLAGS.method == &quotce&quot or FLAGS.method == &quotpairwise&quot:
    model = models.build_model(
        image_size=32, bias_last=True, num_classes=num_classes, squeeze=False)
  else:
    model = models.build_model(
        image_size=32, bias_last=False, num_classes=num_classes, squeeze=False)

  threshold_var = tf.Variable(
      np.zeros((num_classes, num_bins), dtype=np.float32), trainable=False)

  optimizer = tf.keras.optimizers.Adam(learning_rate=FLAGS.lr)

  &#47&#47 Metrics
  train_acc = tf.keras.metrics.CategoricalAccuracy(
      &quottrain_acc&quot, dtype=tf.float32)
  eval_acc = tf.keras.metrics.CategoricalAccuracy(&quoteval_acc&quot, dtype=tf.float32)
  train_loss = tf.keras.metrics.Mean(name=&quottrain_loss&quot, dtype=tf.float32)
  eval_loss = tf.keras.metrics.Mean(name=&quoteval_loss&quot, dtype=tf.float32)
  keras_metrics = dict()
  keras_metrics[&quottrain_loss&quot] = train_loss
  keras_metrics[&quoteval_loss&quot] = eval_loss
  keras_metrics[&quottrain_acc&quot] = train_acc
  keras_metrics[&quoteval_acc&quot] = eval_acc

  if FLAGS.proxy_fn_obj == &quotsoftplus&quot:
    tpr_proxy_func = functools.partial(
        co_utils.tpr_softplus_proxy_func_multi, temperature=FLAGS.sigmoid_temp)
    tp_proxy_func = functools.partial(
        co_utils.tp_softplus_proxy_func_multi, temperature=FLAGS.sigmoid_temp)
  elif FLAGS.proxy_fn_obj == &quotsigmoid&quot:
    tpr_proxy_func = functools.partial(
        co_utils.tpr_sigmoid_proxy_func_multi, temperature=FLAGS.sigmoid_temp)
    tp_proxy_func = functools.partial(
        co_utils.tp_sigmoid_proxy_func_multi, temperature=FLAGS.sigmoid_temp)
  else:
    raise NotImplementedError(&quotProxy function {} not implemented&quot.format(
        FLAGS.proxy_fn_obj))

  if FLAGS.proxy_fn_cons == &quotsoftplus&quot:
    fpr_proxy_func = functools.partial(
        co_utils.fpr_softplus_proxy_func_multi_th,
        temperature=FLAGS.sigmoid_temp)
    fp_proxy_func = functools.partial(
        co_utils.fp_softplus_proxy_func_multi_th,
        temperature=FLAGS.sigmoid_temp)
  elif FLAGS.proxy_fn_cons == &quotsigmoid&quot:
    fpr_proxy_func = functools.partial(
        co_utils.fpr_sigmoid_proxy_func_multi_th,
        temperature=FLAGS.sigmoid_temp)
    fp_proxy_func = functools.partial(
        co_utils.fp_sigmoid_proxy_func_multi_th, temperature=FLAGS.sigmoid_temp)
  else:
    raise NotImplementedError(&quotProxy function {} not implemented&quot.format(
        FLAGS.proxy_fn_cons))
  proxy_funcs = dict()
  proxy_funcs[&quottpr&quot] = tpr_proxy_func
  proxy_funcs[&quotfpr&quot] = fpr_proxy_func
  proxy_funcs[&quottp&quot] = tp_proxy_func
  proxy_funcs[&quotfp&quot] = fp_proxy_func

  &#47&#47 loss and constraint ops
  if FLAGS.method == &quotce&quot:
    loss_op = ce_loss
  elif FLAGS.method == &quotpairwise&quot:
    loss_op = functools.partial(
        pairwise_auc_loss,
        beta=FLAGS.fpr_high,
        proxy=FLAGS.proxy_fn_obj,
        temperature=FLAGS.sigmoid_temp)
  else:
    if FLAGS.proxy_fn_obj == &quotsigmoid&quot:
      loss_op = co_utils.fnr_sigmoid_proxy_func_multi_th
    elif FLAGS.proxy_fn_obj == &quotsoftplus&quot:
      loss_op = co_utils.fnr_softplus_proxy_func_multi_th
  constraint_op = fpr_proxy_func

  &#47&#47 Create summary writers
  train_summary_writer = tf.summary.create_file_writer(
      os.path.join(FLAGS.model_dir, &quotsummaries/train&quot))
  eval_summary_writer = tf.summary.create_file_writer(
      os.path.join(FLAGS.model_dir, &quotsummaries/eval&quot))

  &#47&#47 log file
  <a id="change">fp_log = tf.io.gfile.GFile(
      os.path.join(FLAGS.model_dir, &quottraining_log.txt&quot), &quotw&quot)</a>

  &#47&#47 eval performance at best validated model
  best_results = ml_collections.ConfigDict()
  best_results.aucroc_valid = -1 * np.array([np.inf])
  best_results.tprs_valid = -1 * np.array([np.inf])
  best_results.aucroc_eval_at_valid = -1 * np.array([np.inf])
  best_results.tprs_eval_at_valid = -1 * np.array([np.inf])
  best_results.eval_fprs_valid = []
  best_results.eval_tprs_valid = []

  &#47&#47 Main training loop.
  train_iterator = iter(train_ds)
  initial_ts = time.time()
  for it in range(FLAGS.train_iters):
    batch = next(train_iterator)
    images, labels = batch[&quotimage&quot], batch[&quotlabel&quot]
    loss, constraint = train_step(model, threshold_var, loss_op, constraint_op,
                                  optimizer, keras_metrics, images, labels)
    if FLAGS.method == &quotico&quot and (it + 1) % FLAGS.th_project_freq == 0:
      threshold_new = update_threshold(
          model, train_iterator, FLAGS.n_batches_for_threshold, training=True)
      logger.info(&quotupdating threshold: old {}, new {}&quot.format(
          threshold_var, threshold_new))
      threshold_var.assign(threshold_new)
    if it % 100 == 0:
      with train_summary_writer.as_default():
        tf.summary.scalar(&quotthreshold&quot, threshold_var.numpy()[0, 0], step=it)
        tf.summary.scalar(
            &quotloss/train&quot, keras_metrics[&quottrain_loss&quot].result().numpy(), step=it)
      info_str = (&quotTrain Iter %s: loss=%s, constraint=%s&quot) % (
          it, round(keras_metrics[&quottrain_loss&quot].result().numpy(),
                    8), round(constraint.numpy(), 4))
      logger.info(info_str)
    keras_metrics[&quottrain_loss&quot].reset_states()

    &#47&#47 Evaluation
    if (it + 1) % FLAGS.eval_freq == 0:
      best_results = eval_metrics(model, threshold_var, valid_ds, eval_ds,
                                  loss_op, constraint_op, proxy_funcs,
                                  keras_metrics, it, eval_summary_writer,
                                  best_results, logger, fp_log)

  <a id="change">fp_log</a><a id="change">.close()</a>


def main(_):
  if not tf.io.gfile.exists(FLAGS.model_dir):
    tf.io.gfile.makedirs(FLAGS.model_dir)

  &#47&#47 Train and eval.
  logger = logging.getLogger()
  logger.setLevel(logging.INFO)
  stdout_handler = logging.StreamHandler(sys.stdout)
  logger.addHandler(stdout_handler)
  logger.info(&quotStart training and eval&quot)
  train_and_eval_model(logger)


if __name__ == &quot__main__&quot:
  app.run(main)
</code></pre>