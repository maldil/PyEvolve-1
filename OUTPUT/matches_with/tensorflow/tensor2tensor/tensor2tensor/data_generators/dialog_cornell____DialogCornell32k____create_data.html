<link rel="stylesheet" href="../../../..//default.css">
<script src="../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/dialog_cornell.py#L68">GitHubLink</a>


<a href="https://github.com/maldil/tensor2tensor/blob/master/tensor2tensor/data_generators/dialog_cornell.py#L68">GitMyHubLink</a>

&#47&#47 coding=utf-8
&#47&#47 Copyright 2022 The Tensor2Tensor Authors.
&#47&#47
&#47&#47 Licensed under the Apache License, Version 2.0 (the "License");
&#47&#47 you may not use this file except in compliance with the License.
&#47&#47 You may obtain a copy of the License at
&#47&#47
&#47&#47     http://www.apache.org/licenses/LICENSE-2.0
&#47&#47
&#47&#47 Unless required by applicable law or agreed to in writing, software
&#47&#47 distributed under the License is distributed on an "AS IS" BASIS,
&#47&#47 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
&#47&#47 See the License for the specific language governing permissions and
&#47&#47 limitations under the License.

Cornell Movie Dialog Dataset.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import os
import re

from tensor2tensor.data_generators import dialog_abstract
from tensor2tensor.data_generators import text_encoder
from tensor2tensor.utils import registry


&#47&#47 End-of-sentence marker.
EOS = text_encoder.EOS_ID


@registry.register_problem
class DialogCornell32k(dialog_abstract.DialogAbstract):
  Implements the chatbot problem with Cornell Movie Dialog dataset.

  https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html
  

  @property
  def targeted_vocab_size(self):
    return 2**15

  def preprocess_data(self, train_mode):
    Main function where the preprocessing of the data starts.

    Args:
      train_mode: string, whether we are in train, dev or test mode
    

    &#47&#47 Set the raw data directory and data.
    self.raw_data_dir = os.path.join(&quot/&quot.join(self._data_dir.split(&quot/&quot)[:-1]),
                                     &quotraw_data&quot)
    self.raw_data = os.path.join(self._raw_data_dir,
                                 &quotcornell movie-dialogs corpus&quot)
    self.zipped_data = os.path.join(self._raw_data_dir,
                                    &quotcornell_movie_dialogs_corpus.zip&quot)

    &#47&#47 Create the download url.
    self.url = (&quothttp://www.cs.cornell.edu/~cristian/data/&quot +
                &quotcornell_movie_dialogs_corpus.zip&quot)

    &#47&#47 Check at which part of the pipeline are we at.
    self.data_pipeline_status(train_mode)

  def create_data(self, train_mode):
    Create the source, target and vocab files.

    Args:
      train_mode: string, whether we are in train, dev or test mode
    

    &#47&#47 Open the 6 files.
    trainsource, traintarget, devsource, devtarget, testsource, testtarget = \
        self.open_6_files()

    &#47&#47 Open the raw data.
    <a id="change">movie_lines = open(
        os.path.join(self._raw_data, &quotmovie_lines.txt&quot), errors=&quotignore&quot)</a>
    dialog_list = self.extract_dialog_ids()

    vocabulary = collections.Counter()
    line_dict = {}
    number_of_lines = 0
    &#47&#47 Iterate through file.
    for line in movie_lines:
      if number_of_lines % 10000 == 0:
        print(&quotproblem_log: Parsed &quot + str(number_of_lines) + &quot lines.&quot)

      line = line.split(&quot +++$+++ &quot)
      dialog_id = line[0]
      line = line[4].lower()

      &#47&#47 Do some cleaning.
      line = self.clean_line(line)
      line_dict[dialog_id] = line

      number_of_lines += 1
      &#47&#47 Check if we reached the desired dataset size.
      if (self.targeted_dataset_size != 0 and
          self.targeted_dataset_size &lt; number_of_lines):
        break

    counter = 0
    dataset_split_counter = 0
    &#47&#47 Save the actual dialogs.
    for dialog in dialog_list:
      if counter % 10000 == 0:
        print(&quotproblem_log: Saved &quot +
              str(counter) + &quot/&quot + str(len(dialog_list)) + &quot dialogs.&quot)

      dataset_split_counter += 1
      i = 0
      &#47&#47 Save one utterance.
      for utterance in dialog:
        if (utterance != dialog[-1] and
            dialog[i + 1] != &quotL211194&quot and
            dialog[i + 1] != &quotL1045&quot):
          source_line = line_dict[utterance] + &quot\n&quot
          target_line = line_dict[dialog[i + 1]] + &quot\n&quot

          &#47&#47 Save to the files according to dataset split.
          if dataset_split_counter &lt;= self.dataset_split[&quottrain&quot]:
            &#47&#47 Build vocabulary.
            words = source_line.split()
            for word in words:
              vocabulary[word] = vocabulary.get(word, 0) + 1

            trainsource.write(source_line)
            traintarget.write(target_line)

          elif dataset_split_counter &lt;= (self.dataset_split[&quottrain&quot] +
                                         self.dataset_split[&quotval&quot]):
            devsource.write(source_line)
            devtarget.write(target_line)
          else:
            testsource.write(source_line)
            testtarget.write(target_line)
        i += 1

      &#47&#47 Reset the split counter if we reached 100%.
      if dataset_split_counter == 100:
        dataset_split_counter = 0
      counter += 1

    &#47&#47 Close the files.
    self.close_n_files([trainsource,
                        traintarget,
                        devsource,
                        devtarget,
                        testsource,
                        testtarget])
    <a id="change">movie_lines</a><a id="change">.close()</a>

    &#47&#47 Save the vocabulary.
    self.save_vocab(vocabulary)

  &#47&#47 Extract the dialog ids from the dialog file.
  def extract_dialog_ids(self):
    dialogs = open(os.path.join(self._raw_data, &quotmovie_conversations.txt&quot),
                   errors=&quotignore&quot)

    dialog_list = []
    &#47&#47 Each line contains a dialog.
    for line in dialogs:
      line = line.split(&quot +++$+++ &quot)
      line = line[3].split(&quot,&quot)

      i = 0
      for item in line:
        line[i] = re.sub(&quot[^A-Z0-9]&quot, &quot&quot, item)
        i += 1
      dialog_list.append(line)

    dialogs.close()
    return dialog_list
</code></pre>