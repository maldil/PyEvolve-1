<link rel="stylesheet" href="../../../..//default.css">
<script src="../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/tensorflow/tensor2tensor/blob/master/tensor2tensor/data_generators/dialog_personachat.py#L86">GitHubLink</a>


<a href="https://github.com/maldil/tensor2tensor/blob/master/tensor2tensor/data_generators/dialog_personachat.py#L86">GitMyHubLink</a>

&#47&#47 coding=utf-8
&#47&#47 Copyright 2022 The Tensor2Tensor Authors.
&#47&#47
&#47&#47 Licensed under the Apache License, Version 2.0 (the "License");
&#47&#47 you may not use this file except in compliance with the License.
&#47&#47 You may obtain a copy of the License at
&#47&#47
&#47&#47     http://www.apache.org/licenses/LICENSE-2.0
&#47&#47
&#47&#47 Unless required by applicable law or agreed to in writing, software
&#47&#47 distributed under the License is distributed on an "AS IS" BASIS,
&#47&#47 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
&#47&#47 See the License for the specific language governing permissions and
&#47&#47 limitations under the License.

Persona-chat dataset.

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function

import collections
import os
import tarfile
import zipfile

from tensor2tensor.data_generators import dialog_abstract
from tensor2tensor.data_generators import text_encoder
from tensor2tensor.utils import registry


&#47&#47 End-of-sentence marker.
EOS = text_encoder.EOS_ID


@registry.register_problem
class DialogPersonachat16k(dialog_abstract.DialogAbstract):
  Implements a simple chatbot for the original Persona-chat dataset.

  The personas are not used in this class, only the raw dialogs.
  https://github.com/facebookresearch/ParlAI/tree/master/projects/personachat
  

  def preprocess_data(self, train_mode):
    Main function where the preprocessing of the data starts.

    Args:
      train_mode: string, whether we are in train, dev or test mode
    

    &#47&#47 Set the raw data directory and data.
    self.raw_data_dir = os.path.join(&quot/&quot.join(self._data_dir.split(&quot/&quot)[:-1]),
                                     &quotraw_data&quot)
    self.raw_data = os.path.join(self._raw_data_dir, &quotConvAI2&quot)
    self.zipped_data = os.path.join(self._raw_data_dir, &quotconvai2.tar.gz&quot)

    &#47&#47 Create the download url.
    self.url = &quothttp://parl.ai/downloads/convai2/convai2_fix_723.tgz&quot

    &#47&#47 Check at which part of the pipeline are we at.
    self.data_pipeline_status(train_mode)

  def extract_data(self, train_mode):
    Extract data and go to the next step.

    Args:
      train_mode: string, whether we are in train, dev or test mode
    

    if self._zipped_data[-2:] == &quotgz&quot:
      zip_file = tarfile.open(self._zipped_data, &quotr:gz&quot)
    elif self._zipped_data[-3:] == &quotzip&quot:
      zip_file = zipfile.ZipFile(self._zipped_data, &quotr&quot)
    else:
      print(&quotproblem_log: &quot + self._zipped_data +
            &quot is not a .zip or .gz file, so I can\&quott extract it.&quot)

    zip_file.extractall(self._raw_data)
    zip_file.close()

    &#47&#47 Next step is creating the source, target and vocab files.
    print(&quotproblem_log: Creating &quot +
          train_mode + &quot files in &quot + self._data_dir + &quot.&quot)
    self.create_data(train_mode)

  def create_data(self, train_mode):
    Create the source, target and vocab files.

    Args:
      train_mode: string, whether we are in train, dev or test mode
    

    &#47&#47 Open the 6 files.
    trainsource, traintarget, devsource, devtarget, testsource, testtarget = \
        self.open_6_files()

    &#47&#47 Open the raw data.
    <a id="change">train_dialogs = open(
        os.path.join(self._raw_data, &quottrain_none_original_no_cands.txt&quot),
        errors=&quotignore&quot)</a>
    <a id="change">valid_dialogs = open(
        os.path.join(self._raw_data, &quotvalid_none_original_no_cands.txt&quot),
        errors=&quotignore&quot)</a>
    filenames = [train_dialogs, valid_dialogs]

    &#47&#47 Copy the data to a new file.
    with <a id="change">open(os.path.join(self._raw_data,
                           &quotfull_none_original_no_cands.txt&quot), &quotw&quot)</a> as outfile:
      for fname in filenames:
        with fname as infile:
          outfile.write(infile.read())
    <a id="change">train_dialogs</a><a id="change">.close()</a>
    <a id="change">valid_dialogs</a><a id="change">.close()</a>

    &#47&#47 Open the big file.
    <a id="change">dialogs = open(
        os.path.join(self._raw_data, &quotfull_none_original_no_cands.txt&quot),
        errors=&quotignore&quot)</a>

    number_of_lines = 0
    current_dialog = &quot&quot
    dialog_list = []
    dialog_silenced = False
    &#47&#47 Iterate through the file and build list of dialogs separated by __eou__.
    for line in dialogs:
      if number_of_lines % 10000 == 0:
        print(&quotproblem_log: Parsed &quot + str(number_of_lines) + &quot lines.&quot)

      dialog_id = line.split()[0]
      &#47&#47 Check if this is a refurbished line.
      if (&quot__SILENCE__&quot not in line and
          ((dialog_silenced and dialog_id == &quot1&quot) or not dialog_silenced)):
        dialog_silenced = False
        number_of_lines += 1

        &#47&#47 Get the utterances.
        source = &quot &quot.join(line.split(&quot\t&quot)[0].split()[1:])
        target = line.split(&quot\t&quot)[1].strip(&quot\n&quot)
        source = self.clean_line(source.lower())
        target = self.clean_line(target.lower())

        &#47&#47 Whether this is a new dialog.
        if dialog_id == &quot1&quot and current_dialog:
          dialog_list.append(current_dialog)
          current_dialog = source + &quot__eou__&quot + target + &quot__eou__&quot
        else:
          current_dialog += source + &quot__eou__&quot + target + &quot__eou__&quot
      else:
        dialog_silenced = True

      if (self.targeted_dataset_size != 0 and
          self.targeted_dataset_size &lt; number_of_lines):
        break
    <a id="change">dialogs</a><a id="change">.close()</a>

    vocabulary = collections.Counter()
    number_of_dialogs = 0
    dataset_split_counter = 0
    &#47&#47 Build the dataset.
    for dialog in dialog_list:
      if number_of_dialogs % 1000 == 0:
        print(&quotproblem_log: Parsed &quot + str(number_of_dialogs) + &quot dialogs.&quot)

      &#47&#47 Check which file we should write to.
      if dataset_split_counter &lt;= self.dataset_split[&quottrain&quot]:
        source_file = trainsource
        target_file = traintarget
      elif dataset_split_counter &lt;= (self.dataset_split[&quottrain&quot] +
                                     self.dataset_split[&quotval&quot]):
        source_file = devsource
        target_file = devtarget
      else:
        source_file = testsource
        target_file = testtarget

      utterances = dialog.split(&quot__eou__&quot)[:-1]
      i = 0
      &#47&#47 Loop through the dialog.
      for utterance in utterances:
        i += 1
        &#47&#47 Build vocabulary.
        if dataset_split_counter &lt;= self.dataset_split[&quottrain&quot]:
          words = utterance.split()
          for word in words:
            if word in vocabulary:
              vocabulary[word] += 1
            else:
              vocabulary[word] = 1

        &#47&#47 Write to files.
        if i != len(utterances):
          source_file.write(utterance + &quot\n&quot)
        if i != 1:
          target_file.write(utterance + &quot\n&quot)

      dataset_split_counter += 1
      number_of_dialogs += 1
      &#47&#47 Reset the split counter if we reached 100%.
      if dataset_split_counter == 100:
        dataset_split_counter = 0

    &#47&#47 Close the files.
    self.close_n_files([trainsource,
                        traintarget,
                        devsource,
                        devtarget,
                        testsource,
                        testtarget])
    &#47&#47 Save the vocabulary.
    self.save_vocab(vocabulary)
</code></pre>