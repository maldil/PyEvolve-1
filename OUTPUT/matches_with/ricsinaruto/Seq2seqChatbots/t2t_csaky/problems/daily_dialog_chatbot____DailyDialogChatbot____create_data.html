<link rel="stylesheet" href="../../../..//default.css">
<script src="../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/ricsinaruto/Seq2seqChatbots/blob/master/t2t_csaky/problems/daily_dialog_chatbot.py#L45">GitHubLink</a>


<a href="https://github.com/maldil/Seq2seqChatbots/blob/master/t2t_csaky/problems/daily_dialog_chatbot.py#L45">GitMyHubLink</a>

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import os
from collections import Counter

from tensor2tensor.data_generators import text_encoder
from tensor2tensor.utils import registry

from t2t_csaky.problems import cornell_chatbots


&#47&#47 End-of-sentence marker.
EOS = text_encoder.EOS_ID


@registry.register_problem
class DailyDialogChatbot(cornell_chatbots.CornellChatbotBasic):
  &quot&quot&quot
  A class implementing a simple chatbot problem for the DailyDialog dataset.
  This version doesn&quott use any auxiliary information.
  &quot&quot&quot

  &#47&#47 Main function where the preprocessing of the data starts.
  def preprocess_data(self, train_mode):
    &quot&quot&quot
    Params:
      :train_mode: Whether we are in train or dev mode.
    &quot&quot&quot

    &#47&#47 Set the raw data directory and data.
    self.raw_data_dir = os.path.join(&quot/&quot.join(self._data_dir.split(&quot/&quot)[:-1]),
                                     &quotraw_data&quot)
    self.raw_data = os.path.join(self._raw_data_dir, &quotijcnlp_dailydialog&quot)
    self.zipped_data = os.path.join(self._raw_data_dir,
                                    &quotijcnlp_dailydialog.zip&quot)

    &#47&#47 Create the download url.
    self.url = &quothttp://yanran.li/files/ijcnlp_dailydialog.zip&quot

    &#47&#47 Check at which part of the pipeline are we at.
    self.data_pipeline_status(train_mode)

  &#47&#47 Create the source, target and vocab files.
  def create_data(self, train_mode):
    &quot&quot&quot
    Params:
      :train_mode: Whether we are in train or dev mode.
    &quot&quot&quot

    &#47&#47 Open the 6 files.
    trainSource, trainTarget, devSource, devTarget, testSource, testTarget = \
        self.open_6_files()

    &#47&#47 Open the raw data.
    <a id="change">dialogs = open(
        os.path.join(self._raw_data, &quotdialogues_text.txt&quot), errors=&quotignore&quot)</a>

    vocabulary = Counter()
    number_of_dialogs = 0
    line_counter = 0
    dataset_split_counter = 0
    &#47&#47 Iterate through the file.
    for dialog in dialogs:
      dataset_split_counter += 1
      if number_of_dialogs % 1000 == 0:
        print(&quott2t_csaky_log: Parsed &quot + str(number_of_dialogs) + &quot dialogs.&quot)

      &#47&#47 Utterances are separated by the __eou__ token.
      utterances = dialog.split(&quot__eou__&quot)[:-1]

      &#47&#47 Check which file we should write to.
      if dataset_split_counter &lt;= self.dataset_split[&quottrain&quot]:
        source_file = trainSource
        target_file = trainTarget
      elif dataset_split_counter &lt;= (self.dataset_split[&quottrain&quot] +
                                     self.dataset_split[&quotval&quot]):
        source_file = devSource
        target_file = devTarget
      else:
        source_file = testSource
        target_file = testTarget

      &#47&#47 Clean the utterances.
      i = 0
      for utterance in utterances:
        line_counter += 1
        utterance = self.clean_line(utterance.lower())
        i += 1

        &#47&#47 Build vocabulary.
        if dataset_split_counter &lt;= self.dataset_split[&quottrain&quot]:
          words = utterance.split()
          for word in words:
            if word in vocabulary:
              vocabulary[word] += 1
            else:
              vocabulary[word] = 1

        &#47&#47 Write to files.
        if i != len(utterances):
          source_file.write(utterance + &quot\n&quot)
        if i != 1:
          target_file.write(utterance + &quot\n&quot)

      number_of_dialogs += 1
      &#47&#47 Reset the split counter if we reached 100%.
      if dataset_split_counter == 100:
        dataset_split_counter = 0

      &#47&#47 Check if we reached the desired dataset size.
      if (self.targeted_dataset_size != 0 and
              self.targeted_dataset_size &lt; line_counter):
        break

    &#47&#47 Close the files.
    self.close_n_files([trainSource,
                       trainTarget,
                       devSource,
                       devTarget,
                       testSource,
                       testTarget])
    <a id="change">dialogs</a><a id="change">.close()</a>

    &#47&#47 Save the vocabulary.
    self.save_vocab(vocabulary)
</code></pre>