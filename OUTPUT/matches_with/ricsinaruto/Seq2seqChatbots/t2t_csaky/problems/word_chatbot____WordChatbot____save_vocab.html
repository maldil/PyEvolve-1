<link rel="stylesheet" href="../../../..//default.css">
<script src="../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/ricsinaruto/Seq2seqChatbots/blob/master/t2t_csaky/problems/word_chatbot.py#L224">GitHubLink</a>


<a href="https://github.com/maldil/Seq2seqChatbots/blob/master/t2t_csaky/problems/word_chatbot.py#L224">GitMyHubLink</a>

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function
import tensorflow as tf
import os

from tensor2tensor.data_generators import problem
from tensor2tensor.data_generators import text_problems
from tensor2tensor.data_generators import text_encoder
from tensor2tensor.data_generators import generator_utils
from tensor2tensor.data_generators.text_problems import VocabType
from tensor2tensor.utils import metrics
from tensor2tensor.layers import modalities

from t2t_csaky.config import PROBLEM_HPARAMS

&#47&#47 End-of-sentence marker.
EOS = text_encoder.EOS_ID


class WordChatbot(text_problems.Text2TextProblem):
  &quot&quot&quot
  An abstract base class for word based chatbot problems.
  &quot&quot&quot

  @property
  def vocab_type(self):
    return text_problems.VocabType.TOKEN

  @property
  def is_generate_per_split(self):
    return True

  @property
  def vocab_file(self):
    return self.vocab_filename

  @property
  def vocab_filename(self):
    return &quotvocab.chatbot.&quot + str(self.targeted_vocab_size)

  @property
  def oov_token(self):
    return &quot&lt;unk&gt;&quot

  @property
  def use_subword_tokenizer(self):
    return False

  @property
  def input_space_id(self):
    return problem.SpaceID.EN_TOK

  @property
  def target_space_id(self):
    return problem.SpaceID.EN_TOK

  @property
  def targeted_vocab_size(self):
    return PROBLEM_HPARAMS[&quotvocabulary_size&quot]

  @property
  def targeted_dataset_size(self):
    &#47&#47 Number of utterance pairs in the full dataset.
    &#47&#47 If it&quots 0, then the full size of the dataset is used.
    return PROBLEM_HPARAMS[&quotdataset_size&quot]

  @property
  def dataset_split(self):
    return PROBLEM_HPARAMS[&quotdataset_split&quot]

  @property
  def dataset_splits(self):
    return [{
        &quotsplit&quot: problem.DatasetSplit.TRAIN,
        &quotshards&quot: PROBLEM_HPARAMS[&quotnum_train_shards&quot],
    }, {
        &quotsplit&quot: problem.DatasetSplit.EVAL,
        &quotshards&quot: PROBLEM_HPARAMS[&quotnum_dev_shards&quot],
    }, {
        &quotsplit&quot: problem.DatasetSplit.TEST,
        &quotshards&quot: PROBLEM_HPARAMS[&quotnum_dev_shards&quot],
    }]

  @property
  def data_dir(self):
    return &quot&quot

  @property
  def raw_data_dir(self):
    return &quot&quot

  @property
  def raw_data(self):
    return &quot&quot

  @property
  def zipped_data(self):
    return &quot&quot

  @property
  def url(self):
    return &quot&quot

  &quot&quot&quot Setter methods for the string properties. &quot&quot&quot
  @data_dir.setter
  def data_dir(self, value):
    self._data_dir = value

  @raw_data_dir.setter
  def raw_data_dir(self, value):
    self._raw_data_dir = value

  @raw_data.setter
  def raw_data(self, value):
    self._raw_data = value

  @zipped_data.setter
  def zipped_data(self, value):
    self._zipped_data = value

  @url.setter
  def url(self, value):
    self._url = value

  &#47&#47 Main function where the preprocessing of the data starts.
  def preprocess_data(self, train_mode):
    return NotImplementedError

  &#47&#47 hparams for the problem.
  def hparams(self, defaults, unused_model_hparams):
    p = defaults
    p.stop_at_eos = int(True)

    p.modality = {&quottargets&quot: modalities.ModalityType.SYMBOL}
    if self.has_inputs:
      p.modality[&quotinputs&quot] = modalities.ModalityType.SYMBOL
      p.vocab_size = {&quotinputs&quot: self._encoders[&quotinputs&quot].vocab_size}
    p.vocab_size[&quottargets&quot] = self._encoders[&quotinputs&quot].vocab_size

    if self.vocab_type == VocabType.CHARACTER:
      p.loss_multiplier = 2.0

    if self.packed_length:
      if self.has_inputs:
        p.modality[&quotinputs_segmentation&quot] = modalities.ModalityType.IDENTITY
        p.modality[&quotinputs_position&quot] = modalities.ModalityType.IDENTITY
        p.vocab_size[&quotinputs_segmentation&quot] = None
        p.vocab_size[&quotinputs_position&quot] = None
      p.modality[&quottargets_segmentation&quot] = modalities.ModalityType.IDENTITY
      p.modality[&quottargets_position&quot] = modalities.ModalityType.IDENTITY
      p.vocab_size[&quottargets_segmentation&quot] = None
      p.vocab_size[&quottargets_position&quot] = None

  &#47&#47 What evaluation metrics to use with this problem.
  def eval_metrics(self):
    return [metrics.Metrics.ACC, metrics.Metrics.ACC_TOP5,
            metrics.Metrics.ACC_PER_SEQ,
            metrics.Metrics.NEG_LOG_PERPLEXITY,
            metrics.Metrics.APPROX_BLEU]

  &#47&#47 Override this, to start with preprocessing.
  def generate_data(self, data_dir, tmp_dir, task_id=-1):
    self.data_dir = data_dir
    &#47&#47 Determine whether we are in training or validation mode.
    self.mode = {problem.DatasetSplit.TRAIN: &quottrain&quot,
                 problem.DatasetSplit.EVAL: &quotdev&quot,
                 problem.DatasetSplit.TEST: &quottest&quot}
    filepath_fns = {problem.DatasetSplit.TRAIN: self.training_filepaths,
                    problem.DatasetSplit.EVAL: self.dev_filepaths,
                    problem.DatasetSplit.TEST: self.test_filepaths}

    split_paths = [(split[&quotsplit&quot], filepath_fns[split[&quotsplit&quot]](
      data_dir, split[&quotshards&quot], shuffled=self.already_shuffled))
      for split in self.dataset_splits]
    all_paths = []
    for _, paths in split_paths:
      all_paths.extend(paths)

    if self.is_generate_per_split:
      for split, paths in split_paths:
        &#47&#47 Create the source and target txt files from the raw data.
        self.preprocess_data(self.mode[split])
        generator_utils.generate_files(
            self.generate_encoded_samples(data_dir, tmp_dir, split), paths)
    else:
      self.preprocess_data(self.mode[problem.DatasetSplit.TRAIN])
      generator_utils.generate_files(
          self.generate_encoded_samples(
              data_dir, tmp_dir, problem.DatasetSplit.TRAIN), all_paths)

    generator_utils.shuffle_dataset(all_paths, extra_fn=self._pack_fn())

  &#47&#47 This function generates train and validation pairs in t2t-datagen style.
  def generate_samples(self, data_dir, tmp_dir, data_split):
    &quot&quot&quot
    The function assumes that if you have data at one level of the pipeline,
    you don&quott want to re-generate it, so for example if the 4 txt files exist,
    the function continues by generating the t2t-datagen format files.
    So if you want to re-download or re-generate data,
    you have to delete it first from the appropriate directories.

    Params:
      :data_dir: Directory where the data will be generated
                 The raw data has to be downloaded one directory level higher.
      :data_split: Which data split to generate samples for.
    &quot&quot&quot
    self.data_dir = data_dir
    print(&quott2t_csaky_log: &quot +
          self.mode[data_split] + &quot data generation activated.&quot)

    sPath = os.path.join(data_dir, self.mode[data_split] + &quotSource.txt&quot)
    tPath = os.path.join(data_dir, self.mode[data_split] + &quotTarget.txt&quot)

    &#47&#47 Open the files and yield source-target lines.
    with tf.gfile.GFile(sPath, mode=&quotr&quot) as source_file:
      with tf.gfile.GFile(tPath, mode=&quotr&quot) as target_file:
        source, target = source_file.readline(), target_file.readline()
        while source and target:
          yield {&quotinputs&quot: source.strip(), &quottargets&quot: target.strip()}
          source, target = source_file.readline(), target_file.readline()

  &#47&#47 Save the vocabulary to a file.
  def save_vocab(self, vocab):
    &quot&quot&quot
    Params:
      :vocab: Vocabulary list.
    &quot&quot&quot
    <a id="change">voc_file = open(os.path.join(self._data_dir, self.vocab_file), &quotw&quot)</a>

    &#47&#47 Put the reserved tokens in.
    voc_file.write(&quot&lt;pad&gt;\n&quot)
    voc_file.write(&quot&lt;EOS&gt;\n&quot)
    for word, _ in vocab.most_common(self.targeted_vocab_size - 3):
      voc_file.write(word + &quot\n&quot)
    voc_file.write(&quot&lt;unk&gt;&quot)

    <a id="change">voc_file</a><a id="change">.close()</a>

  &#47&#47 Open the 6 files to write the processed data into.
  def open_6_files(self):
    trainSource = open(os.path.join(self._data_dir, &quottrainSource.txt&quot), &quotw&quot)
    trainTarget = open(os.path.join(self._data_dir, &quottrainTarget.txt&quot), &quotw&quot)
    devSource = open(os.path.join(self._data_dir, &quotdevSource.txt&quot), &quotw&quot)
    devTarget = open(os.path.join(self._data_dir, &quotdevTarget.txt&quot), &quotw&quot)
    testSource = open(os.path.join(self._data_dir, &quottestSource.txt&quot), &quotw&quot)
    testTarget = open(os.path.join(self._data_dir, &quottestTarget.txt&quot), &quotw&quot)

    return trainSource, trainTarget, devSource, \
        devTarget, testSource, testTarget

  &#47&#47 Close the 6 files to write the processed data into.
  def close_n_files(self, files):
    for file in files:
      file.close()
</code></pre>