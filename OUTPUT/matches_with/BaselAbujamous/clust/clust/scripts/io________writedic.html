<link rel="stylesheet" href="../../../..//default.css">
<script src="../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/BaselAbujamous/clust/blob/master/clust/scripts/io.py#L244">GitHubLink</a>


<a href="https://github.com/maldil/clust/blob/master/clust/scripts/io.py#L244">GitMyHubLink</a>

import os
import re
import numpy as np
import clust.scripts.datastructures as ds
import clust.scripts.numeric as nu
import clust.scripts.glob as glob
import clust.scripts.output as op
import sys
import traceback
import math
import portalocker
import pandas as pd


if sys.version_info[0] &lt; 3:
    from StringIO import StringIO
else:
    from io import StringIO


def getFilesInDirectory(path, extension=None):
    for (dirpath, dirnames, filenames) in os.walk(path):
        if extension is None or extension == &quot&quot:
            return [fn for fn in filenames if fn != &quot.DS_Store&quot]
        else:
            if len(extension) &gt; 1 and extension[0] == &quot.&quot and extension[1] != &quot*&quot:
                extension = extension[1:]
            return [fn for fn in filenames if re.match(&quot(.*\.&quot + extension + &quot$)&quot, fn) is not None and fn != &quot.DS_Store&quot]


def readDatasetsFromDirectory(path, delimiter=&quot\t| |, |; |,|;&quot, skiprows=1, skipcolumns=1, returnSkipped=False):

    &#47&#47 Single file name given
    if os.path.isfile(path):
        datafiles = [os.path.basename(path)]
        datafileswithpath = [path]
    &#47&#47 Directory of data files given
    elif os.path.isdir(path):
        datafiles = np.sort(getFilesInDirectory(path)).tolist()
        datafileswithpath = [path + &quot/&quot + df for df in datafiles]
    &#47&#47 Invalid path given
    else:
        raise ValueError(&quotData path {0} does not exist. Either provide a path &quot.format(path) + \
                         &quotof a data file or a path to a directory including data file(s)&quot)

    datafilesread = readDataFromFiles(datafileswithpath, delimiter, float, skiprows, skipcolumns, returnSkipped)

    if returnSkipped:
        return datafilesread + (datafiles, )
    else:
        return datafilesread, datafiles


def readMap(mapfile, delimiter=&quot\t&quot):
    if mapfile is None:
        return None
    return readDataFromFiles([mapfile], delimiter, dtype=str, skiprows=0, skipcolumns=0, returnSkipped=False, data_na_filter=False)[0]


def readReplicates(replicatesfile, datapath, datafiles, replicates, delimiter=&quot\t| |,|;&quot):
    if replicatesfile is None:
        if isinstance(replicates[0], list):
            return None, replicates
        elif isinstance(replicates[0], np.ndarray):
            return None, [r.tolist() for r in replicates]
        else:
            return None, replicates

    L = len(datafiles)
    replicatesIDs = [[-1 for c in replicates[l]] for l in range(L)]
    conditions = [None] * L

    with open(replicatesfile) as f:
        lineNumber = 0
        for line in f:
            lineNumber += 1
            line = line.partition(&quot&#47&#47&quot)[0]
            line = line.rstrip()
            line = list(filter(None, re.split(delimiter, line)))

            &#47&#47 Skip to next line if it is an empty line
            if len(line) &lt; 1:
                continue

            if line[0] in datafiles:
                l = datafiles.index(line[0])  &#47&#47 (l)th dataset
            else:
                raise ValueError(&quotUnrecognised data file name ({0}) in line {1} in {2}.&quot.
                                 format(line[0], lineNumber, replicatesfile))

            &#47&#47 Skip to next line if no condition ID is given
            if len(line) &lt; 2:
                continue

            if conditions[l] is None:
                conditions[l] = [line[1]]
            elif line[1] not in conditions:
                conditions[l] += [line[1]]

            c = conditions[l].index(line[1])  &#47&#47 (c)th condition

            &#47&#47 Skip to next line if no replicates are given
            if len(line) &lt; 3:
                continue

            for r in line[2:]:
                if r in replicates[l]:
                    if isinstance(replicates[l], list):
                        replicatesIDs[l][replicates[l].index(r)] = c
                    elif isinstance(replicates[l], np.ndarray):
                        replicatesIDs[l][replicates[l].tolist().index(r)] = c
                else:
                    raise ValueError(&quotUnrecognised replicate name ({0}) in line {1} in {2}.&quot.
                                     format(r, lineNumber, replicatesfile))

    &#47&#47 Write the conditions of the data files with no entries in the replicates file
    for c in range(len(conditions)):
        if conditions[c] is None:
            with open(&quot{0}/{1}&quot.format(datapath, datafiles[c])) as f:
                line = f.readline()
                while (line[0] == &quot&#47&#47&quot):
                    line = f.readline()
                line = line.rstrip()
                line = filter(None, re.split(delimiter, line))
                conditions[c] = line[1:]
            replicatesIDs[c] = list(range(len(conditions[c])))

    return replicatesIDs, conditions


def readNormalisation(normalisefile, datafiles, delimiter=&quot\t| |,|;&quot, defaultnormalisation=1000):
    

    :param normalisefile: either a list of a single string element which is the normalisation file name,
    or a list of strings representing normalisation codes. In this case, the strings must be convertable to integers.
    :param datafiles:
    :param delimiter:
    :param defaultnormalisation:
    :return:
    
    if normalisefile is None:
        return defaultnormalisation

    &#47&#47 This is in case the normalisation file was given as a single integer, it should not though
    if nu.isint(normalisefile):
        normalisefile = [normalisefile]

    L = len(datafiles)
    normalise = [None] * L

    &#47&#47 This happens when the normalisation codes are given directly rather than in a file
    if len(normalisefile) &gt; 1 or nu.isint(normalisefile[0]):
        for l in range(L):
            normalise[l] = [int(n) for n in normalisefile]
        return normalise

    &#47&#47 This happens when a normalisation file is given
    with open(normalisefile[0]) as f:
        lineNumber = 0
        for line in f:
            lineNumber += 1
            line = line.partition(&quot&#47&#47&quot)[0]
            line = line.rstrip()
            line = list(filter(None, re.split(delimiter, line)))

            &#47&#47 Skip to next line if it is an empty line
            if len(line) &lt; 1:
                continue

            if line[0] in datafiles:
                l = datafiles.index(line[0])  &#47&#47 (l)th dataset
            else:
                raise ValueError(&quotUnrecognised data file name ({0}) in line {1} in {2}.&quot.
                                 format(line[0], lineNumber, normalisefile[0]))

            &#47&#47 If no normalisation is set for the dataset, skip to the next line
            if len(line) &lt; 2:
                continue

            &#47&#47 If the normalisation of this dataset has not been set, set it, otherwise append
            if normalise[l] is None:
                normalise[l] = line[1:]
            else:
                normalise[l] = normalise[l] + line[1:]

    for l in range(L):
        if normalise[l] is None:
            normalise[l] = [defaultnormalisation]
        else:
            normalise[l] = [int(n) for n in normalise[l]]

    return normalise


def readDataFromFiles(datafiles, delimiter=&quot\t| |, |; |,|;&quot, dtype=float, skiprows=1, data_na_filter=True, skipcolumns=1, returnSkipped=True, comm=&quot&#47&#47&quot):
    L = len(datafiles)
    X = [None] * L
    skippedRows = [None] * L
    skippedCols = [None] * L
    for l in range(L):
        with open(datafiles[l]) as f:
            ncols = len(re.split(delimiter, f.readline()))
        &#47&#47 This is now using pandas read_csv, if np.loadtxt is re-used, you HAVE TO set ndmin = 2 here
        X[l] = pdreadcsv_regexdelim(datafiles[l], delimiter=delimiter, dtype=dtype, skiprows=skiprows,
                          usecols=range(skipcolumns, ncols), na_filter=data_na_filter, comments=comm)

        if skiprows &gt; 0:
            skippedRows[l] = pdreadcsv_regexdelim(datafiles[l], delimiter=delimiter, dtype=str, skiprows=0,
                                        usecols=range(skipcolumns, ncols), na_filter=False, comments=comm)[0:skiprows]
            if skiprows == 1:
                skippedRows[l] = skippedRows[l][0]
        else:
            skippedRows[l] = np.array([]).reshape([0, X[l].shape[1]])

        if skipcolumns &gt; 0:
            skippedCols[l] = pdreadcsv_regexdelim(datafiles[l], delimiter=delimiter, dtype=str, skiprows=skiprows,
                                        usecols=range(skipcolumns), na_filter=False, comments=comm)
        else:
            skippedCols[l] = np.array([]).reshape([0, X[l].shape[1]])

    if returnSkipped:
        return (ds.listofarrays2arrayofarrays(X), skippedRows, skippedCols)
    else:
        return ds.listofarrays2arrayofarrays(X)


&#47&#47 Does the same job as numpy.loadtxt but regex delimiter
&#47&#47&#47&#47&#47&#47&#47&#47 OBSOLETE, USE THE PANDAS READ_CSV VERSION BELOW INSTEAD
def nploadtxt_regexdelim(file, delimiter=&quot\t| |, |; |,|;&quot, dtype=float, skiprows=0, usecols=None, ndmin=0, comments=&quot&#47&#47&quot):
    with open(file) as f:
        result = np.loadtxt((re.sub(delimiter, &quot\t&quot, str(x)) for x in f),
                            delimiter=&quot\t&quot, dtype=dtype, skiprows=skiprows, usecols=usecols, ndmin=ndmin, comments=comments)
    return result


&#47&#47 Does the same job as pandas.read_csv but regex delimiter
def pdreadcsv_regexdelim(file, delimiter=&quot\t| |, |; |,|;&quot, dtype=float, skiprows=0, usecols=None, na_filter=True, comments=&quot&#47&#47&quot):
    with open(file) as f:
        result = pd.read_csv(StringIO(&quot\n&quot.join(re.sub(delimiter, &quot\t&quot, str(x)) for x in f)),
                            delimiter=&quot\t&quot, dtype=dtype, header=None, skiprows=skiprows, usecols=usecols, na_filter=na_filter, comment=comments).values
    return result


def writedic(filepath, dic, header=None, delim=&quot\t&quot):
    <a id="change">f = open(filepath, &quotw+&quot)</a>

    &#47&#47 Write header
    if header is not None:
        f.write(&quot{0}\n&quot.format(header))

    &#47&#47 Write the rest
    nokey = re.compile(&quot^nokey[0-9]*$&quot, flags=re.I)  &#47&#47 To match lines with no keys
    for k in dic.keys():

        if nokey.match(k) is None:
            f.write(&quot{0}{1}{2}\n&quot.format(k, delim, dic[k]))
        else:
            f.write(&quot{0}\n&quot.format(dic[k]))

    &#47&#47 Close file
    <a id="change">f</a><a id="change">.close()</a>


def log(msg=None, addextrastick=True):
    if addextrastick:
        msg = op.msgformated(msg, withnewline=False)

    if glob.print_to_log_file:
        with open(glob.logfile, mode=&quota+&quot) as f:
            if msg is not None:
                f.write(msg)
            f.write(&quot\n&quot)

    if glob.print_to_console:
        print(msg)


def logerror(exec_info):
    errstr = traceback.format_exception(exec_info[0], exec_info[1], exec_info[2])
    errstr = &quot&quot.join(errstr)
    log(&quotUnexpected error:\n{0}\nContinuing execution anyway ...&quot.format(errstr))


def resetparallelprogress(parallel_total, log_every_percentage=10.0):
    with open(glob.tmpfile, mode=&quotw+&quot) as f:
        f.write(&quot{0} {1} {2}&quot.format(parallel_total, log_every_percentage, 0.0))
        f.truncate()


def updateparallelprogress(added_value):
    Done = False
    while (not Done):
        try:
            Done = True
            with open(glob.tmpfile, mode=&quotr+&quot) as f:
                portalocker.lock(f, portalocker.LOCK_EX)
                data = f.read().split(" ")
                parallel_total = float(data[0])
                log_every_percentage = float(data[1])
                current_parallel_progress = float(data[2])

                last_log = math.floor(100 * current_parallel_progress
                                      / parallel_total / log_every_percentage) * log_every_percentage
                current_parallel_progress += added_value
                new_log = math.floor(100 * current_parallel_progress
                                     / parallel_total / log_every_percentage) * log_every_percentage

                &#47&#47for i in np.arange(last_log+log_every_percentage, new_log + log_every_percentage, log_every_percentage):
                &#47&#47    log(&quot{0}%&quot.format(int(i)))
                if new_log &gt; last_log:
                    log(&quot{0}%&quot.format(int(new_log)))

                f.seek(0)
                f.write(&quot{0} {1} {2}&quot.format(parallel_total, log_every_percentage, current_parallel_progress))
                f.truncate()
        except:
            Done = False


def getparallelprogress():
    Done = False
    while (not Done):
        try:
            Done = True
            with open(glob.tmpfile, mode=&quotr+&quot) as f:
                data = f.read().split(" ")
                parallel_total = float(data[0])
                log_every_percentage = float(data[1])
                current_parallel_progress = float(data[2])

                return (parallel_total, log_every_percentage, current_parallel_progress)
        except:
            Done = False


def deletetmpfile():
    if os.path.isfile(glob.tmpfile):
        os.remove(glob.tmpfile)

</code></pre>