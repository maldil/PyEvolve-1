<link rel="stylesheet" href="../../../..//default.css">
<script src="../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/dmlc/dgl/blob/master/tests/pytorch/test_dist_optim.py#L51">GitHubLink</a>


<a href="https://github.com/maldil/dgl/blob/master/tests/pytorch/test_dist_optim.py#L51">GitMyHubLink</a>

import os
os.environ[&quotOMP_NUM_THREADS&quot] = &quot1&quot
import dgl
import sys
import numpy as np
import time
import socket
from scipy import sparse as spsp
import torch as th

from dgl.distributed import DistGraphServer, DistGraph
from dgl.distributed import partition_graph, load_partition_book
import multiprocessing as mp
from dgl import function as fn
import backend as F
import unittest
import pickle
import random
from dgl.distributed import DistEmbedding
from dgl.distributed.optim import SparseAdagrad, SparseAdam

def create_random_graph(n):
    arr = (spsp.random(n, n, density=0.001, format=&quotcoo&quot, random_state=100) != 0).astype(np.int64)
    return dgl.from_scipy(arr)

def get_local_usable_addr():
    Get local usable IP and port

    Returns
    -------
    str
        IP address, e.g., &quot192.168.8.12:50051&quot
    
    sock = socket.socket(socket.AF_INET, socket.SOCK_DGRAM)
    try:
        &#47&#47 doesn&quott even have to be reachable
        sock.connect((&quot10.255.255.255&quot, 1))
        ip_addr = sock.getsockname()[0]
    except ValueError:
        ip_addr = &quot127.0.0.1&quot
    finally:
        sock.close()
    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
    sock.bind(("", 0))
    sock.listen(1)
    port = sock.getsockname()[1]
    sock.close()

    return ip_addr + &quot &quot + str(port)

def prepare_dist():
    <a id="change">ip_config</a><a id="change"> = open("optim_ip_config.txt", "w")</a>
    ip_addr = get_local_usable_addr()
    <a id="change">ip_config</a>.write(&quot{}\n&quot.format(ip_addr))
    <a id="change">ip_config</a><a id="change">.close()</a>

def run_server(graph_name, server_id, server_count, num_clients, shared_mem):
    g = DistGraphServer(server_id, "optim_ip_config.txt", num_clients, server_count,
                        &quot/tmp/dist_graph/{}.json&quot.format(graph_name),
                        disable_shared_mem=not shared_mem)
    print(&quotstart server&quot, server_id)
    g.start()

def initializer(shape, dtype):
    arr = th.zeros(shape, dtype=dtype)
    th.manual_seed(0)
    th.nn.init.uniform_(arr, 0, 1.0)
    return arr

def run_client(graph_name, cli_id, part_id, server_count):
    device=F.ctx()
    time.sleep(5)
    os.environ[&quotDGL_NUM_SERVER&quot] = str(server_count)
    dgl.distributed.initialize("optim_ip_config.txt")
    gpb, graph_name, _, _ = load_partition_book(&quot/tmp/dist_graph/{}.json&quot.format(graph_name),
                                                part_id, None)
    g = DistGraph(graph_name, gpb=gpb)
    policy = dgl.distributed.PartitionPolicy(&quotnode&quot, g.get_partition_book())
    num_nodes = g.number_of_nodes()
    emb_dim = 4
    dgl_emb = DistEmbedding(num_nodes, emb_dim, name=&quotoptim&quot, init_func=initializer, part_policy=policy)
    dgl_emb_zero = DistEmbedding(num_nodes, emb_dim, name=&quotoptim-zero&quot, init_func=initializer, part_policy=policy)
    dgl_adam = SparseAdam(params=[dgl_emb, dgl_emb_zero], lr=0.01)
    dgl_adam._world_size = 1
    dgl_adam._rank = 0

    torch_emb = th.nn.Embedding(num_nodes, emb_dim, sparse=True)
    torch_emb_zero = th.nn.Embedding(num_nodes, emb_dim, sparse=True)
    th.manual_seed(0)
    th.nn.init.uniform_(torch_emb.weight, 0, 1.0)
    th.manual_seed(0)
    th.nn.init.uniform_(torch_emb_zero.weight, 0, 1.0)
    torch_adam = th.optim.SparseAdam(
        list(torch_emb.parameters()) + list(torch_emb_zero.parameters()), lr=0.01)

    labels = th.ones((4,)).long()
    idx = th.randint(0, num_nodes, size=(4,))
    dgl_value = dgl_emb(idx, device).to(th.device(&quotcpu&quot))
    torch_value = torch_emb(idx)
    torch_adam.zero_grad()
    torch_loss = th.nn.functional.cross_entropy(torch_value, labels)
    torch_loss.backward()
    torch_adam.step()

    dgl_adam.zero_grad()
    dgl_loss = th.nn.functional.cross_entropy(dgl_value, labels)
    dgl_loss.backward()
    dgl_adam.step()

    assert F.allclose(dgl_emb.weight[0 : num_nodes//2], torch_emb.weight[0 : num_nodes//2])

def check_sparse_adam(num_trainer=1, shared_mem=True):
    prepare_dist()
    g = create_random_graph(2000)
    num_servers = num_trainer
    num_clients = num_trainer
    num_parts = 1

    graph_name = &quotdist_graph_test&quot
    partition_graph(g, graph_name, num_parts, &quot/tmp/dist_graph&quot)

    &#47&#47 let&quots just test on one partition for now.
    &#47&#47 We cannot run multiple servers and clients on the same machine.
    serv_ps = []
    ctx = mp.get_context(&quotspawn&quot)
    for serv_id in range(num_servers):
        p = ctx.Process(target=run_server, args=(graph_name, serv_id, num_servers,
                                                 num_clients, shared_mem))
        serv_ps.append(p)
        p.start()

    cli_ps = []
    for cli_id in range(num_clients):
        print(&quotstart client&quot, cli_id)
        p = ctx.Process(target=run_client, args=(graph_name, cli_id, 0, num_servers))
        p.start()
        cli_ps.append(p)

    for p in cli_ps:
        p.join()

    for p in serv_ps:
        p.join()

@unittest.skipIf(os.name == &quotnt&quot, reason=&quotDo not support windows yet&quot)
def test_sparse_opt():
    os.environ[&quotDGL_DIST_MODE&quot] = &quotdistributed&quot
    check_sparse_adam(1, True)
    check_sparse_adam(1, False)

if __name__ == &quot__main__&quot:
    os.makedirs(&quot/tmp/dist_graph&quot, exist_ok=True)
    test_sparse_opt()</code></pre>