<link rel="stylesheet" href="../../../..//default.css">
<script src="../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/nl8590687/ASRT_SpeechRecognition/blob/master/model_zoo/speech_model/keras_backend.py#L60">GitHubLink</a>


<a href="https://github.com/maldil/ASRT_SpeechRecognition/blob/master/model_zoo/speech_model/keras_backend.py#L60">GitMyHubLink</a>

&#47&#47!/usr/bin/env python3
&#47&#47 -*- coding: utf-8 -*-
&#47&#47
&#47&#47 Copyright 2016-2099 Ailemon.net
&#47&#47
&#47&#47 This file is part of ASRT Speech Recognition Tool.
&#47&#47
&#47&#47 ASRT is free software: you can redistribute it and/or modify
&#47&#47 it under the terms of the GNU General Public License as published by
&#47&#47 the Free Software Foundation, either version 3 of the License, or
&#47&#47 (at your option) any later version.
&#47&#47 ASRT is distributed in the hope that it will be useful,
&#47&#47 but WITHOUT ANY WARRANTY; without even the implied warranty of
&#47&#47 MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
&#47&#47 GNU General Public License for more details.
&#47&#47
&#47&#47 You should have received a copy of the GNU General Public License
&#47&#47 along with ASRT.  If not, see &lt;https://www.gnu.org/licenses/&gt;.
&#47&#47 ============================================================================


@author: nl8590687
若干声学模型模型的定义


import tensorflow as tf
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, Dropout, Input, Reshape, BatchNormalization
from tensorflow.keras.layers import Lambda, Activation,Conv2D, MaxPooling2D
from tensorflow.keras import backend as K
import numpy as np
from utils.ops import ctc_decode_delete_tail_blank

class BaseModel:
    &quot&quot&quot
    定义声学模型类型的接口基类
    &quot&quot&quot
    def __init__(self):
        self.input_shape = None
        self.output_shape = None

    def get_model(self) -&gt; tuple:
        return self.model, self.model_base

    def get_train_model(self) -&gt; Model:
        return self.model

    def get_eval_model(self) -&gt; Model:
        return self.model_base

    def summary(self) -&gt; None:
        self.model.summary()

    def get_model_name(self) -&gt; str:
        return self._model_name

    def load_weights(self, filename :str) -&gt; None:
        self.model.load_weights(filename)

    def save_weights(self, filename :str) -&gt; None:
        self.model.save_weights(filename + &quot.model.h5&quot)
        self.model_base.save_weights(filename + &quot.model.base.h5&quot)

        <a id="change">f</a><a id="change"> = open(&quotepoch_&quot+self._model_name+&quot.txt&quot,&quotw&quot)</a>
        f.write(filename)
        <a id="change">f</a><a id="change">.close()</a>

    def get_loss_function(self):
        raise Exception("method not implemented")
    
    def forward(self, x):
        raise Exception("method not implemented")

def ctc_lambda_func(args):
    y_pred, labels, input_length, label_length = args
    y_pred = y_pred[:, :, :]
    return K.ctc_batch_cost(labels, y_pred, input_length, label_length)

class SpeechModel251BN(BaseModel):
    &quot&quot&quot
    定义CNN+CTC模型，使用函数式模型

    输入层：200维的特征值序列，一条语音数据的最大长度设为1600（大约16s）\\
    隐藏层：卷积池化层，卷积核大小为3x3，池化窗口大小为2 \\
    隐藏层：全连接层 \\
    输出层：全连接层，神经元数量为self.MS_OUTPUT_SIZE，使用softmax作为激活函数， \\
    CTC层：使用CTC的loss作为损失函数，实现连接性时序多输出

    参数： \\
        input_shape: tuple，默认值(1600, 200, 1) \\
        output_shape: tuple，默认值(200, 1428)
    &quot&quot&quot
    def __init__(self, input_shape :tuple=(1600, 200, 1), output_size :int=1428) -&gt; None:
        super().__init__()
        self.input_shape = input_shape
        self._pool_size = 8
        self.output_shape = (input_shape[0] // self._pool_size, output_size)
        self._model_name = &quotSpeechModel251bn&quot
        self.model, self.model_base = self._define_model(self.input_shape, self.output_shape[1])

    def _define_model(self, input_shape, output_size) -&gt; tuple:
        label_max_string_length = 64

        input_data = Input(name=&quotthe_input&quot, shape=input_shape)

        layer_h = Conv2D(32, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv0&quot)(input_data) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN0&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct0&quot)(layer_h)

        layer_h = Conv2D(32, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv1&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN1&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct1&quot)(layer_h)

        layer_h = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h) &#47&#47 池化层

        layer_h = Conv2D(64, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv2&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN2&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct2&quot)(layer_h)

        layer_h = Conv2D(64, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv3&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN3&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct3&quot)(layer_h)

        layer_h = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h) &#47&#47 池化层

        layer_h = Conv2D(128, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv4&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN4&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct4&quot)(layer_h)

        layer_h = Conv2D(128, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv5&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN5&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct5&quot)(layer_h)

        layer_h = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h) &#47&#47 池化层

        layer_h = Conv2D(128, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv6&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN6&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct6&quot)(layer_h)

        layer_h = Conv2D(128, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv7&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN7&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct7&quot)(layer_h)

        layer_h = MaxPooling2D(pool_size=1, strides=None, padding="valid")(layer_h) &#47&#47 池化层

        layer_h = Conv2D(128, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv8&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN8&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct8&quot)(layer_h)

        layer_h = Conv2D(128, (3,3), use_bias=True, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot, name=&quotConv9&quot)(layer_h) &#47&#47 卷积层
        layer_h = BatchNormalization(epsilon=0.0002, name=&quotBN9&quot)(layer_h)
        layer_h = Activation(&quotrelu&quot, name=&quotAct9&quot)(layer_h)

        layer_h = MaxPooling2D(pool_size=1, strides=None, padding="valid")(layer_h) &#47&#47 池化层

        &#47&#47test=Model(inputs = input_data, outputs = layer_h12)
        &#47&#47test.summary()

        layer_h = Reshape((self.output_shape[0], input_shape[1] // self._pool_size * 128), name=&quotReshape0&quot)(layer_h) &#47&#47Reshape层

        layer_h = Dense(128, activation="relu", use_bias=True, kernel_initializer=&quothe_normal&quot, name=&quotDense0&quot)(layer_h) &#47&#47 全连接层

        layer_h = Dense(output_size, use_bias=True, kernel_initializer=&quothe_normal&quot, name=&quotDense1&quot)(layer_h) &#47&#47 全连接层
        y_pred = Activation(&quotsoftmax&quot, name=&quotActivation0&quot)(layer_h)

        model_base = Model(inputs = input_data, outputs = y_pred)
        &#47&#47model_data.summary()

        labels = Input(name=&quotthe_labels&quot, shape=[label_max_string_length], dtype=&quotfloat32&quot)
        input_length = Input(name=&quotinput_length&quot, shape=[1], dtype=&quotint64&quot)
        label_length = Input(name=&quotlabel_length&quot, shape=[1], dtype=&quotint64&quot)
        &#47&#47 Keras doesn&quott currently support loss funcs with extra parameters
        &#47&#47 so CTC loss is implemented in a lambda layer
        loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name=&quotctc&quot)([y_pred, labels, input_length, label_length])

        model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)

        return model, model_base

    def get_loss_function(self) -&gt; dict:
        return {&quotctc&quot: lambda y_true, y_pred: y_pred}

    def forward(self, data_input):
        batch_size = 1 
        in_len = np.zeros((batch_size),dtype = np.int32)

        in_len[0] = self.output_shape[0]

        x_in = np.zeros((batch_size,) + self.input_shape, dtype=np.float)

        for i in range(batch_size):
            x_in[i,0:len(data_input)] = data_input

        base_pred = self.model_base.predict(x = x_in)
        r = K.ctc_decode(base_pred, in_len, greedy = True, beam_width=100, top_paths=1)

        if(tf.__version__[0:2] == &quot1.&quot):
            r1 = r[0][0].eval(session=tf.compat.v1.Session())
        else:
            r1 = r[0][0].numpy()
        
        speech_result = ctc_decode_delete_tail_blank(r1[0])
        return speech_result

class SpeechModel251(BaseModel):
    &quot&quot&quot
    定义CNN+CTC模型，使用函数式模型

    输入层：200维的特征值序列，一条语音数据的最大长度设为1600（大约16s）\\
    隐藏层：卷积池化层，卷积核大小为3x3，池化窗口大小为2 \\
    隐藏层：全连接层 \\
    输出层：全连接层，神经元数量为self.MS_OUTPUT_SIZE，使用softmax作为激活函数， \\
    CTC层：使用CTC的loss作为损失函数，实现连接性时序多输出

    参数： \\
        input_shape: tuple，默认值(1600, 200, 1) \\
        output_shape: tuple，默认值(200, 1428)
    &quot&quot&quot
    def __init__(self, input_shape :tuple=(1600, 200, 1), output_size :int=1428) -&gt; None:
        super().__init__()
        self.input_shape = input_shape
        self._pool_size = 8
        self.output_shape = (input_shape[0] // self._pool_size, output_size)
        self._model_name = &quotSpeechModel251&quot
        self.model, self.model_base = self._define_model(self.input_shape, self.output_shape[1])

    def _define_model(self, input_shape, output_size) -&gt; tuple:
        label_max_string_length = 64

        input_data = Input(name=&quotthe_input&quot, shape=input_shape)

        layer_h1 = Conv2D(32, (3,3), use_bias=False, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(input_data) &#47&#47 卷积层
        layer_h1 = Dropout(0.05)(layer_h1)
        layer_h2 = Conv2D(32, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h1) &#47&#47 卷积层
        layer_h3 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h2) &#47&#47 池化层
        &#47&#47layer_h3 = Dropout(0.2)(layer_h2) &#47&#47 随机中断部分神经网络连接，防止过拟合
        layer_h3 = Dropout(0.05)(layer_h3)
        layer_h4 = Conv2D(64, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h3) &#47&#47 卷积层
        layer_h4 = Dropout(0.1)(layer_h4)
        layer_h5 = Conv2D(64, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h4) &#47&#47 卷积层
        layer_h6 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h5) &#47&#47 池化层

        layer_h6 = Dropout(0.1)(layer_h6)
        layer_h7 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h6) &#47&#47 卷积层
        layer_h7 = Dropout(0.15)(layer_h7)
        layer_h8 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h7) &#47&#47 卷积层
        layer_h9 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h8) &#47&#47 池化层

        layer_h9 = Dropout(0.15)(layer_h9)
        layer_h10 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h9) &#47&#47 卷积层
        layer_h10 = Dropout(0.2)(layer_h10)
        layer_h11 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h10) &#47&#47 卷积层
        layer_h12 = MaxPooling2D(pool_size=1, strides=None, padding="valid")(layer_h11) &#47&#47 池化层

        layer_h12 = Dropout(0.2)(layer_h12)
        layer_h13 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h12) &#47&#47 卷积层
        layer_h13 = Dropout(0.2)(layer_h13)
        layer_h14 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h13) &#47&#47 卷积层
        layer_h15 = MaxPooling2D(pool_size=1, strides=None, padding="valid")(layer_h14) &#47&#47 池化层

        &#47&#47test=Model(inputs = input_data, outputs = layer_h12)
        &#47&#47test.summary()

        layer_h16 = Reshape((self.output_shape[0], input_shape[1] // self._pool_size * 128))(layer_h15) &#47&#47Reshape层
        &#47&#47layer_h6 = Dropout(0.2)(layer_h5) &#47&#47 随机中断部分神经网络连接，防止过拟合
        layer_h16 = Dropout(0.3)(layer_h16)
        layer_h17 = Dense(128, activation="relu", use_bias=True, kernel_initializer=&quothe_normal&quot)(layer_h16) &#47&#47 全连接层
        layer_h17 = Dropout(0.3)(layer_h17)
        layer_h18 = Dense(output_size, use_bias=True, kernel_initializer=&quothe_normal&quot)(layer_h17) &#47&#47 全连接层
        y_pred = Activation(&quotsoftmax&quot, name=&quotActivation0&quot)(layer_h18)

        model_base = Model(inputs = input_data, outputs = y_pred)
        &#47&#47model_data.summary()

        labels = Input(name=&quotthe_labels&quot, shape=[label_max_string_length], dtype=&quotfloat32&quot)
        input_length = Input(name=&quotinput_length&quot, shape=[1], dtype=&quotint64&quot)
        label_length = Input(name=&quotlabel_length&quot, shape=[1], dtype=&quotint64&quot)
        &#47&#47 Keras doesn&quott currently support loss funcs with extra parameters
        &#47&#47 so CTC loss is implemented in a lambda layer
        loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name=&quotctc&quot)([y_pred, labels, input_length, label_length])

        model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)

        return model, model_base

    def get_loss_function(self) -&gt; dict:
        return {&quotctc&quot: lambda y_true, y_pred: y_pred}

    def forward(self, data_input):
        batch_size = 1 
        in_len = np.zeros((batch_size),dtype = np.int32)

        in_len[0] = self.output_shape[0]

        x_in = np.zeros((batch_size,) + self.input_shape, dtype=np.float)

        for i in range(batch_size):
            x_in[i,0:len(data_input)] = data_input

        base_pred = self.model_base.predict(x = x_in)
        r = K.ctc_decode(base_pred, in_len, greedy = True, beam_width=100, top_paths=1)

        if(tf.__version__[0:2] == &quot1.&quot):
            r1 = r[0][0].eval(session=tf.compat.v1.Session())
        else:
            r1 = r[0][0].numpy()
        
        speech_result = ctc_decode_delete_tail_blank(r1[0])
        return speech_result

class SpeechModel25(BaseModel):
    &quot&quot&quot
    定义CNN+CTC模型，使用函数式模型

    输入层：200维的特征值序列，一条语音数据的最大长度设为1600（大约16s）\\
    隐藏层：卷积池化层，卷积核大小为3x3，池化窗口大小为2 \\
    隐藏层：全连接层 \\
    输出层：全连接层，神经元数量为self.MS_OUTPUT_SIZE，使用softmax作为激活函数， \\
    CTC层：使用CTC的loss作为损失函数，实现连接性时序多输出

    参数： \\
        input_shape: tuple，默认值(1600, 200, 1) \\
        output_shape: tuple，默认值(200, 1428)
    &quot&quot&quot
    def __init__(self, input_shape :tuple=(1600, 200, 1), output_size :int=1428) -&gt; None:
        super().__init__()
        self.input_shape = input_shape
        self._pool_size = 8
        self.output_shape = (input_shape[0] // self._pool_size, output_size)
        self._model_name = &quotSpeechModel25&quot
        self.model, self.model_base = self._define_model(self.input_shape, self.output_shape[1])

    def _define_model(self, input_shape, output_size) -&gt; tuple:
        label_max_string_length = 64

        input_data = Input(name=&quotthe_input&quot, shape=input_shape)

        layer_h1 = Conv2D(32, (3,3), use_bias=False, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(input_data) &#47&#47 卷积层
        layer_h1 = Dropout(0.05)(layer_h1)
        layer_h2 = Conv2D(32, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h1) &#47&#47 卷积层
        layer_h3 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h2) &#47&#47 池化层
        &#47&#47layer_h3 = Dropout(0.2)(layer_h2) &#47&#47 随机中断部分神经网络连接，防止过拟合
        layer_h3 = Dropout(0.05)(layer_h3)
        layer_h4 = Conv2D(64, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h3) &#47&#47 卷积层
        layer_h4 = Dropout(0.1)(layer_h4)
        layer_h5 = Conv2D(64, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h4) &#47&#47 卷积层
        layer_h6 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h5) &#47&#47 池化层

        layer_h6 = Dropout(0.1)(layer_h6)
        layer_h7 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h6) &#47&#47 卷积层
        layer_h7 = Dropout(0.15)(layer_h7)
        layer_h8 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h7) &#47&#47 卷积层
        layer_h9 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h8) &#47&#47 池化层

        layer_h9 = Dropout(0.15)(layer_h9)
        layer_h10 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h9) &#47&#47 卷积层
        layer_h10 = Dropout(0.2)(layer_h10)
        layer_h11 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h10) &#47&#47 卷积层
        layer_h12 = MaxPooling2D(pool_size=1, strides=None, padding="valid")(layer_h11) &#47&#47 池化层

        &#47&#47test=Model(inputs = input_data, outputs = layer_h12)
        &#47&#47test.summary()

        layer_h12 = Reshape((self.output_shape[0], input_shape[1] // self._pool_size * 128))(layer_h12) &#47&#47Reshape层
        &#47&#47layer_h6 = Dropout(0.2)(layer_h5) &#47&#47 随机中断部分神经网络连接，防止过拟合
        layer_h12 = Dropout(0.3)(layer_h12)
        layer_h13 = Dense(128, activation="relu", use_bias=True, kernel_initializer=&quothe_normal&quot)(layer_h12) &#47&#47 全连接层
        layer_h13 = Dropout(0.3)(layer_h13)
        layer_h14 = Dense(output_size, use_bias=True, kernel_initializer=&quothe_normal&quot)(layer_h13) &#47&#47 全连接层
        y_pred = Activation(&quotsoftmax&quot, name=&quotActivation0&quot)(layer_h14)

        model_base = Model(inputs = input_data, outputs = y_pred)
        &#47&#47model_data.summary()

        labels = Input(name=&quotthe_labels&quot, shape=[label_max_string_length], dtype=&quotfloat32&quot)
        input_length = Input(name=&quotinput_length&quot, shape=[1], dtype=&quotint64&quot)
        label_length = Input(name=&quotlabel_length&quot, shape=[1], dtype=&quotint64&quot)
        &#47&#47 Keras doesn&quott currently support loss funcs with extra parameters
        &#47&#47 so CTC loss is implemented in a lambda layer
        loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name=&quotctc&quot)([y_pred, labels, input_length, label_length])

        model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)

        return model, model_base

    def get_loss_function(self) -&gt; dict:
        return {&quotctc&quot: lambda y_true, y_pred: y_pred}

    def forward(self, data_input):
        batch_size = 1 
        in_len = np.zeros((batch_size),dtype = np.int32)

        in_len[0] = self.output_shape[0]

        x_in = np.zeros((batch_size,) + self.input_shape, dtype=np.float)

        for i in range(batch_size):
            x_in[i,0:len(data_input)] = data_input

        base_pred = self.model_base.predict(x = x_in)
        r = K.ctc_decode(base_pred, in_len, greedy = True, beam_width=100, top_paths=1)

        if(tf.__version__[0:2] == &quot1.&quot):
            r1 = r[0][0].eval(session=tf.compat.v1.Session())
        else:
            r1 = r[0][0].numpy()
        
        speech_result = ctc_decode_delete_tail_blank(r1[0])
        return speech_result

class SpeechModel24(BaseModel):
    &quot&quot&quot
    定义CNN+CTC模型，使用函数式模型

    输入层：200维的特征值序列，一条语音数据的最大长度设为1600（大约16s）\\
    隐藏层：卷积池化层，卷积核大小为3x3，池化窗口大小为2 \\
    隐藏层：全连接层 \\
    输出层：全连接层，神经元数量为self.MS_OUTPUT_SIZE，使用softmax作为激活函数， \\
    CTC层：使用CTC的loss作为损失函数，实现连接性时序多输出

    参数： \\
        input_shape: tuple，默认值(1600, 200, 1) \\
        output_shape: tuple，默认值(200, 1428)
    &quot&quot&quot
    def __init__(self, input_shape :tuple=(1600, 200, 1), output_size :int=1428) -&gt; None:
        super().__init__()
        self.input_shape = input_shape
        self._pool_size = 8
        self.output_shape = (input_shape[0] // self._pool_size, output_size)
        self._model_name = &quotSpeechModel24&quot
        self.model, self.model_base = self._define_model(self.input_shape, self.output_shape[1])

    def _define_model(self, input_shape, output_size) -&gt; tuple:
        label_max_string_length = 64

        input_data = Input(name=&quotthe_input&quot, shape=input_shape)

        layer_h1 = Conv2D(32, (3,3), use_bias=False, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(input_data) &#47&#47 卷积层
        layer_h1 = Dropout(0.1)(layer_h1)
        layer_h2 = Conv2D(32, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h1) &#47&#47 卷积层
        layer_h3 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h2) &#47&#47 池化层
        &#47&#47layer_h3 = Dropout(0.2)(layer_h2) &#47&#47 随机中断部分神经网络连接，防止过拟合
        layer_h3 = Dropout(0.2)(layer_h3)
        layer_h4 = Conv2D(64, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h3) &#47&#47 卷积层
        layer_h4 = Dropout(0.2)(layer_h4)
        layer_h5 = Conv2D(64, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h4) &#47&#47 卷积层
        layer_h6 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h5) &#47&#47 池化层

        layer_h6 = Dropout(0.3)(layer_h6)
        layer_h7 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h6) &#47&#47 卷积层
        layer_h7 = Dropout(0.3)(layer_h7)
        layer_h8 = Conv2D(128, (3,3), use_bias=True, activation=&quotrelu&quot, padding=&quotsame&quot, kernel_initializer=&quothe_normal&quot)(layer_h7) &#47&#47 卷积层
        layer_h9 = MaxPooling2D(pool_size=2, strides=None, padding="valid")(layer_h8) &#47&#47 池化层

        &#47&#47test=Model(inputs = input_data, outputs = layer_h12)
        &#47&#47test.summary()

        layer_h10 = Reshape((self.output_shape[0], input_shape[1] // self._pool_size * 128))(layer_h9) &#47&#47Reshape层
        &#47&#47layer_h6 = Dropout(0.2)(layer_h5) &#47&#47 随机中断部分神经网络连接，防止过拟合
        layer_h10 = Dropout(0.3)(layer_h10)
        layer_h11 = Dense(128, activation="relu", use_bias=True, kernel_initializer=&quothe_normal&quot)(layer_h10) &#47&#47 全连接层
        layer_h11 = Dropout(0.3)(layer_h11)
        layer_h12 = Dense(output_size, use_bias=True, kernel_initializer=&quothe_normal&quot)(layer_h11) &#47&#47 全连接层
        y_pred = Activation(&quotsoftmax&quot, name=&quotActivation0&quot)(layer_h12)

        model_base = Model(inputs = input_data, outputs = y_pred)
        &#47&#47model_data.summary()

        labels = Input(name=&quotthe_labels&quot, shape=[label_max_string_length], dtype=&quotfloat32&quot)
        input_length = Input(name=&quotinput_length&quot, shape=[1], dtype=&quotint64&quot)
        label_length = Input(name=&quotlabel_length&quot, shape=[1], dtype=&quotint64&quot)
        &#47&#47 Keras doesn&quott currently support loss funcs with extra parameters
        &#47&#47 so CTC loss is implemented in a lambda layer
        loss_out = Lambda(ctc_lambda_func, output_shape=(1,), name=&quotctc&quot)([y_pred, labels, input_length, label_length])

        model = Model(inputs=[input_data, labels, input_length, label_length], outputs=loss_out)

        return model, model_base

    def get_loss_function(self) -&gt; dict:
        return {&quotctc&quot: lambda y_true, y_pred: y_pred}

    def forward(self, data_input):
        batch_size = 1 
        in_len = np.zeros((batch_size),dtype = np.int32)

        in_len[0] = self.output_shape[0]

        x_in = np.zeros((batch_size,) + self.input_shape, dtype=np.float)

        for i in range(batch_size):
            x_in[i,0:len(data_input)] = data_input

        base_pred = self.model_base.predict(x = x_in)
        r = K.ctc_decode(base_pred, in_len, greedy = True, beam_width=100, top_paths=1)

        if(tf.__version__[0:2] == &quot1.&quot):
            r1 = r[0][0].eval(session=tf.compat.v1.Session())
        else:
            r1 = r[0][0].numpy()
        
        speech_result = ctc_decode_delete_tail_blank(r1[0])
        return speech_result
</code></pre>