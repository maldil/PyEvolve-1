<link rel="stylesheet" href="../../../..//default.css">
<script src="../../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/kermitt2/delft/blob/master/delft/utilities/Utilities.py#L160">GitHubLink</a>


<a href="https://github.com/maldil/delft/blob/master/delft/utilities/Utilities.py#L160">GitMyHubLink</a>

&#47&#47 some convenient methods for all models
import pandas as pd
import regex as re
import numpy as np
&#47&#47 seed is fixed for reproducibility
from numpy.random import seed
seed(7)
import os.path
import shutil
import requests
from urllib.parse import urlparse

from tensorflow.keras.preprocessing import text

from tqdm import tqdm 

import argparse
import truecase

def truncate_batch_values(batch_values: list, max_sequence_length: int) -&gt; list:
    return [
        row[:max_sequence_length]
        for row in batch_values
    ]

&#47&#47 read list of words (one per line), e.g. stopwords, badwords
def read_words(words_file):
    return [line.replace(&quot\n&quot,&quot&quot).lower() for line in open(words_file, &quotr&quot)]


&#47&#47 preprocessing used for twitter-trained glove embeddings
def glove_preprocess(text):
    
    adapted from https://nlp.stanford.edu/projects/glove/preprocess-twitter.rb

    
    &#47&#47 Different regex parts for smiley faces
    eyes = "[8:=;]"
    nose = "[&quot`\-]?"
    text = re.sub("https?:* ", "&lt;URL&gt;", text)
    text = re.sub("www.* ", "&lt;URL&gt;", text)
    text = re.sub("\[\[User(.*)\|", &quot&lt;USER&gt;&quot, text)
    text = re.sub("&lt;3", &quot&lt;HEART&gt;&quot, text)
    text = re.sub("[-+]?[.\d]*[\d]+[:,.\d]*", "&lt;NUMBER&gt;", text)
    text = re.sub(eyes + nose + "[Dd)]", &quot&lt;SMILE&gt;&quot, text)
    text = re.sub("[(d]" + nose + eyes, &quot&lt;SMILE&gt;&quot, text)
    text = re.sub(eyes + nose + "p", &quot&lt;LOLFACE&gt;&quot, text)
    text = re.sub(eyes + nose + "\(", &quot&lt;SADFACE&gt;&quot, text)
    text = re.sub("\)" + nose + eyes, &quot&lt;SADFACE&gt;&quot, text)
    text = re.sub(eyes + nose + "[/|l*]", &quot&lt;NEUTRALFACE&gt;&quot, text)
    text = re.sub("/", " / ", text)
    text = re.sub("[-+]?[.\d]*[\d]+[:,.\d]*", "&lt;NUMBER&gt;", text)
    text = re.sub("([!]){2,}", "! &lt;REPEAT&gt;", text)
    text = re.sub("([?]){2,}", "? &lt;REPEAT&gt;", text)
    text = re.sub("([.]){2,}", ". &lt;REPEAT&gt;", text)
    pattern = re.compile(r"(.)\1{2,}")
    text = pattern.sub(r"\1" + " &lt;ELONG&gt;", text)

    return text


&#47&#47 split provided sequence data in two sets given the given ratio between 0 and 1
&#47&#47 for instance ratio at 0.8 will split 80% of the sentence in the first set and 20%
&#47&#47 of the remaining sentence in the second one 
&#47&#47
def split_data_and_labels(x, y, ratio):
    if (len(x) != len(y)):
        print(&quoterror: size of x and y set must be equal, &quot, len(x), &quot=/=&quot, len(y))
        return
    x1 = []
    x2 = []
    y1 = []
    y2 = []
    for i in range(len(x)):
        if np.random.random_sample() &lt; ratio:
            x1.append(x[i])
            y1.append(y[i])
        else:
            x2.append(x[i])
            y2.append(y[i])
    return np.asarray(x1),np.asarray(y1),np.asarray(x2),np.asarray(y2)    


url_regex = re.compile(r"https?:\/\/[a-zA-Z0-9_\-\.]+(?:com|org|fr|de|uk|se|net|edu|gov|int|mil|biz|info|br|ca|cn|in|jp|ru|au|us|ch|it|nl|no|es|pl|ir|cz|kr|co|gr|za|tw|hu|vn|be|mx|at|tr|dk|me|ar|fi|nz)\/?\b")


&#47&#47 produce some statistics
def stats(x_train=None, y_train=None, x_valid=None, y_valid=None, x_eval=None, y_eval=None):
    charset = []
    nb_total_sequences = 0
    nb_total_tokens = 0
    if x_train is not None:
        print(len(x_train), &quottrain sequences&quot)
        nb_total_sequences += len(x_train)
        nb_tokens = 0
        for sentence in x_train:
            nb_tokens += len(sentence)
            for token in sentence:
                for character in token:
                    if not character in charset:
                        charset.append(character)
        print("\t","nb. tokens", nb_tokens)
        nb_total_tokens += nb_tokens
    if y_train is not None:
        nb_entities = 0
        for labels in y_train:
            for label in labels:
                if label != &quotO&quot:
                    nb_entities += 1
        print("\t","with nb. entities", nb_entities)
    if x_valid is not None:
        print(len(x_valid), &quotvalidation sequences&quot)
        nb_total_sequences += len(x_valid)
        nb_tokens = 0
        for sentence in x_valid:
            nb_tokens += len(sentence)
            for token in sentence:
                for character in token:
                    if not character in charset:
                        charset.append(character)
        print("\t","nb. tokens", nb_tokens)
        nb_total_tokens += nb_tokens
    if y_valid is not None:
        nb_entities = 0
        for labels in y_valid:
            for label in labels:
                if label != &quotO&quot:
                    nb_entities += 1
        print("\t","with nb. entities", nb_entities)
    if x_eval is not None:
        print(len(x_eval), &quotevaluation sequences&quot)
        nb_total_sequences += len(x_eval)
        nb_tokens = 0
        for sentence in x_eval:
            nb_tokens += len(sentence)
            for token in sentence:
                for character in token:
                    if not character in charset:
                        charset.append(character)
        print("\t","nb. tokens", nb_tokens)
        nb_total_tokens += nb_tokens
    if y_eval is not None:
        nb_entities = 0
        for labels in y_eval:
            for label in labels:
                if label != &quotO&quot:
                    nb_entities += 1
        print("\t","with nb. entities", nb_entities)

    print("\n")
    print(nb_total_sequences, "total sequences")
    print(nb_total_tokens, "total tokens\n")

    print("total distinct characters:", len(charset), "\n")
    &#47&#47print(charset)


&#47&#47 generate the list of out of vocabulary words present in the Toxic dataset 
&#47&#47 with respect to 3 embeddings: fastText, Gloves and word2vec
def generateOOVEmbeddings():
    &#47&#47 read the (DL cleaned) dataset and build the vocabulary
    print(&quotloading dataframes...&quot)
    train_df = pd.read_csv(&quot../data/training/train2.cleaned.dl.csv&quot)
    test_df = pd.read_csv(&quot../data/eval/test2.cleaned.dl.csv&quot)

    &#47&#47 ps: forget memory and runtime, it&quots python here :D
    list_sentences_train = train_df["comment_text"].values
    list_sentences_test = test_df["comment_text"].values
    list_sentences_all = np.concatenate([list_sentences_train, list_sentences_test])

    tokenizer = text.Tokenizer(num_words=400000)
    tokenizer.fit_on_texts(list(list_sentences_all))
    print(&quotword_index size:&quot, len(tokenizer.word_index), &quotwords&quot)
    word_index = tokenizer.word_index

    &#47&#47 load fastText - only the words
    print(&quotloading fastText embeddings...&quot)
    voc = set()
    <a id="change">f = open(&quot/mnt/data/wikipedia/embeddings/crawl-300d-2M.vec&quot)</a>
    begin = True
    for line in f:
        if begin:
            begin = False
        else: 
            values = line.split()
            word = &quot &quot.join(values[:-300])
            voc.add(word)
    <a id="change">f</a><a id="change">.close()</a>
    print(&quotfastText embeddings:&quot, len(voc), &quotwords&quot)

    oov = []
    for tokenStr in word_index:
        if not tokenStr in voc:
            oov.append(tokenStr)

    print(&quotfastText embeddings:&quot, len(oov), &quotout-of-vocabulary&quot)

    with <a id="change">open("../data/training/oov-fastText.txt", "w")</a> as oovFile:
        for w in oov:
            oovFile.write(w)
            oovFile.write(&quot\n&quot)
    oovFile.close()

    &#47&#47 load gloves - only the words
    print(&quotloading gloves embeddings...&quot)
    voc = set()
    <a id="change">f = open(&quot/mnt/data/wikipedia/embeddings/glove.840B.300d.txt&quot)</a>
    for line in f:
        values = line.split()
        word = &quot &quot.join(values[:-300])
        voc.add(word)
    <a id="change">f</a><a id="change">.close()</a>
    print(&quotgloves embeddings:&quot, len(voc), &quotwords&quot)

    oov = []
    for tokenStr in word_index:
        if not tokenStr in voc:
            oov.append(tokenStr)

    print(&quotgloves embeddings:&quot, len(oov), &quotout-of-vocabulary&quot)

    with <a id="change">open("../data/training/oov-gloves.txt", "w")</a> as oovFile:
        for w in oov:
            oovFile.write(w)
            oovFile.write(&quot\n&quot)
    oovFile.close()

    &#47&#47 load word2vec - only the words
    print(&quotloading word2vec embeddings...&quot)
    voc = set()
    <a id="change">f = open(&quot/mnt/data/wikipedia/embeddings/GoogleNews-vectors-negative300.vec&quot)</a>
    begin = True
    for line in f:
        if begin:
            begin = False
        else: 
            values = line.split()
            word = &quot &quot.join(values[:-300])
            voc.add(word)
    <a id="change">f</a><a id="change">.close()</a>
    print(&quotword2vec embeddings:&quot, len(voc), &quotwords&quot)

    oov = []
    for tokenStr in word_index:
        if not tokenStr in voc:
            oov.append(tokenStr)

    print(&quotword2vec embeddings:&quot, len(oov), &quotout-of-vocabulary&quot)

    with <a id="change">open("../data/training/oov-w2v.txt", "w")</a> as oovFile:
        for w in oov:    
            oovFile.write(w)
            oovFile.write(&quot\n&quot)
    oovFile.close()

     &#47&#47 load numberbatch - only the words
    print(&quotloading numberbatch embeddings...&quot)
    voc = set()
    <a id="change">f = open(&quot/mnt/data/wikipedia/embeddings/numberbatch-en-17.06.txt&quot)</a>
    begin = True
    for line in f:
        if begin:
            begin = False
        else: 
            values = line.split()
            word = &quot &quot.join(values[:-300])
            voc.add(word)
    <a id="change">f</a><a id="change">.close()</a>
    print(&quotnumberbatch embeddings:&quot, len(voc), &quotwords&quot)

    oov = []
    for tokenStr in word_index:
        if not tokenStr in voc:
            oov.append(tokenStr)

    print(&quotnumberbatch embeddings:&quot, len(oov), &quotout-of-vocabulary&quot)

    with <a id="change">open("../data/training/oov-numberbatch.txt", "w")</a> as oovFile:
        for w in oov:    
            oovFile.write(w)
            oovFile.write(&quot\n&quot)
    oovFile.close()


def ontonotes_conll2012_names(pathin, pathout):
    &#47&#47 generate the list of files having a .name extension in the complete ontonotes corpus
    fileout = open(os.path.join(pathout, "names.list"),&quotw+&quot)

    for subdir, dirs, files in os.walk(pathin):
        for file in files:
            if file.endswith(&quot.name&quot):
                ind = subdir.find("data/english/")
                if (ind == -1):
                    print("path to ontonotes files appears invalid")
                subsubdir = subdir[ind:]
                fileout.write(os.path.join(subsubdir, file.replace(".name","")))
                fileout.write("\n")
    fileout.close()


def convert_conll2012_to_iob2(pathin, pathout):
    
    This method will post-process the assembled Ontonotes CoNLL-2012 data for NER. 
    It will take an input like:
      bc/cctv/00/cctv_0001   0    5         Japanese    JJ             *           -    -      -   Speaker&#47&#471    (NORP)           *        *            *        *     -
    and transform it into a simple and readable:
      Japanese  B-NORP
    taking into account the sequence markers and an expected IOB2 scheme.
    
    if pathin == pathout:
        print("input and ouput path must be different:", pathin, pathout)
        return

    names_doc_ids = []
    with open(os.path.join("data", "sequenceLabelling", "CoNLL-2012-NER", "names.list"),&quotr&quot) as f:
        for line in f:
            line = line.rstrip()
            if len(line) == 0:
                continue
            names_doc_ids.append(line)
    print("number of documents with name notation:", len(names_doc_ids))

    nb_files = 0
     &#47&#47 first pass to get number of files - test files for CoNLL-2012 are under conll-2012-test/, not test/
     &#47&#47 we ignore files not having .names extension in the original ontonotes realease 
    for subdir, dirs, files in os.walk(pathin):
        for file in files:
            if &quot/english/&quot in subdir and (file.endswith(&quotgold_conll&quot)) and not &quot/pt/&quot in subdir and not &quot/test/&quot in subdir:
                ind = subdir.find("data/english/")
                if (ind == -1):
                    print("path to ontonotes files appears invalid")
                subsubdir = os.path.join(subdir[ind:], file.replace(".gold_conll", ""))
                if subsubdir in names_doc_ids:
                    nb_files += 1
    nb_total_files = nb_files
    print(nb_total_files, &quottotal files to convert&quot)

    &#47&#47 load restricted set of ids for the CoNLL-2012 dataset
    train_doc_ids = []
    dev_doc_ids = []
    test_doc_ids = []

    with open(os.path.join("data", "sequenceLabelling", "CoNLL-2012-NER", "english-ontonotes-5.0-train-document-ids.txt"),&quotr&quot) as f:
        for line in f:
            line = line.rstrip()
            if len(line) == 0:
                continue
            train_doc_ids.append(line)
    print("number of train documents:", len(train_doc_ids))

    with open(os.path.join("data", "sequenceLabelling", "CoNLL-2012-NER", "english-ontonotes-5.0-development-document-ids.txt"),&quotr&quot) as f:
        for line in f:
            line = line.rstrip()
            if len(line) == 0:
                continue
            dev_doc_ids.append(line)
    print("number of development documents:", len(dev_doc_ids))

    with open(os.path.join("data", "sequenceLabelling", "CoNLL-2012-NER", "english-ontonotes-5.0-conll-2012-test-document-ids.txt"),&quotr&quot) as f:
        for line in f:
            line = line.rstrip()
            if len(line) == 0:
                continue
            test_doc_ids.append(line)
    print("number of test documents:", len(test_doc_ids))

    train_out = open(os.path.join(pathout, "eng.train"),&quotw+&quot, encoding="UTF-8")
    dev_out = open(os.path.join(pathout, "eng.dev"),&quotw+&quot, encoding="UTF-8")
    test_out = open(os.path.join(pathout, "eng.test"),&quotw+&quot, encoding="UTF-8")

    nb_files = 0
    pbar = tqdm(total=nb_total_files)
    for subdir, dirs, files in os.walk(pathin):
        for file in files:
            &#47&#47 pt subdirectory corresponds to the old and new testaments, it does not contain NER annotation, so it is traditionally ignored
            &#47&#47if &quot/english/&quot in subdir and (file.endswith(&quotgold_conll&quot) or (&quot/test/&quot in subdir and file.endswith(&quotgold_parse_conll&quot))) and not &quot/pt/&quot in subdir:
            if &quot/english/&quot in subdir and (file.endswith(&quotgold_conll&quot)) and not &quot/pt/&quot in subdir and not &quot/test/&quot in subdir:

                ind = subdir.find("data/english/")
                if (ind == -1):
                    print("path to ontonotes files appears invalid")
                subsubdir = os.path.join(subdir[ind:], file.replace(".gold_conll", ""))

                if not subsubdir in names_doc_ids:
                    continue

                pbar.update(1)

                f2 = None
                if &quot/train/&quot in subdir and subsubdir in train_doc_ids:
                    f2 = train_out
                elif &quot/development/&quot in subdir and subsubdir in dev_doc_ids:
                    f2 = dev_out
                elif &quot/conll-2012-test/&quot in subdir and subsubdir in test_doc_ids:
                    f2 = test_out

                if f2 is None:
                    continue

                with open(os.path.join(subdir, file),&quotr&quot, encoding="UTF-8") as f1:
                    previous_tag = None
                    for line in f1:
                        line_ = line.rstrip()
                        line_ = &quot &quot.join(line_.split())
                        if len(line_) == 0:
                            f2.write("\n")
                            previous_tag = None
                        elif line_.startswith(&quot&#47&#47begin document&quot):
                            f2.write(line_+"\n\n")
                            previous_tag = None
                        elif line_.startswith(&quot&#47&#47end document&quot):
                            &#47&#47f2.write("\n")
                            previous_tag = None
                        else:
                            pieces = line_.split(&quot &quot)
                            if len(pieces) &lt; 11:
                                print(os.path.join(subdir, file), "-&gt; unexpected number of fiels for line (", len(pieces), "):", line_)
                                previous_tag = None
                            word = pieces[3]
                            &#47&#47 some punctuation are prefixed by / (e.g. /. or /? for dialogue turn apparently)
                            if word.startswith("/") and len(word) &gt; 1:
                                word = word[1:]
                            &#47&#47 in dialogue texts, interjections are maked with a prefix %, e.g. %uh, %eh, we remove this prefix
                            if word.startswith("%") and len(word) &gt; 1:
                                word = word[1:]
                            &#47&#47 there are &quot=&quot&quot prefixes to some words, although I don&quott know what it is used for, we remove it
                            if word.startswith("=") and len(word) &gt; 1:
                                word = word[1:]
                            &#47&#47 we have some markers like -LRB- left bracket, -RRB- right bracket
                            if word == &quot-LRB-&quot:
                                word = &quot(&quot
                            if word == &quot-RRB-&quot:
                                word = &quot)&quot
                            &#47&#47 some tokens are identifier in the form 165.00_177.54_B:, 114.86_118.28_A:, and so on, always _A or _B as suffix
                            &#47&#47 it&quots very unclear why it is in the plain text but clearly noise
                            &#47&#47regex_str = "\d\d\d\.\d\d_\d\d\d\.\d\d_(A|B)"

                            tag = pieces[10]
                            if tag.startswith(&quot(&quot):
                                if tag.endswith(&quot)&quot):
                                    tag = tag[1:-1]
                                    previous_tag = None
                                else:
                                    tag = tag[1:-1]
                                    previous_tag = tag
                                f2.write(word+"\tB-"+tag+"\n")
                            elif tag == &quot*&quot and previous_tag is not None:
                                f2.write(word+"\tI-"+previous_tag+"\n")
                            elif tag == &quot*)&quot:
                                f2.write(word+"\tI-"+previous_tag+"\n")
                                previous_tag = None
                            else:
                                f2.write(word+"\tO\n")
                                previous_tag = None
    pbar.close()

    train_out.close()
    dev_out.close()
    test_out.close()


def convert_conll2003_to_iob2(filein, fileout):
    
    This method will post-process the assembled CoNLL-2003 data for NER. 
    It will take an input like:

    and transform it into a simple and readable:
      Japanese  B-NORP
    taking into account the sequence markers and an expected IOB2 scheme.
    
    with open(filein,&quotr&quot) as f1:
        with open(fileout,&quotw&quot) as f2:
            previous_tag = &quotO&quot
            for line in f1:
                line_ = line.rstrip()
                if len(line_) == 0 or line_.startswith(&quot-DOCSTART-&quot):
                    f2.write(line_+"\n")
                    previous_tag = &quotO&quot
                else:
                    word, pos, phrase, tag = line_.split(&quot &quot)
                    if tag == &quotO&quot or tag.startswith(&quotB-&quot):
                        f2.write(word+"\t"+tag+"\n")
                    else:
                        subtag = tag[2:]
                        if previous_tag.endswith(tag[2:]):
                            f2.write(word+"\t"+tag+"\n")
                        else:
                            f2.write(word+"\tB-"+tag[2:]+"\n")
                    previous_tag = tag


def merge_folders(root_src_dir, root_dst_dir):
    
    Recursively merge two folders including subfolders. 
    This method is motivated by the limitation of shutil.copytree() which supposes that the 
    destination directory must not exist.
    
    for src_dir, dirs, files in os.walk(root_src_dir):
        dst_dir = src_dir.replace(root_src_dir, root_dst_dir, 1)
        if not os.path.exists(dst_dir):
            os.makedirs(dst_dir)
        for file_ in files:
            src_file = os.path.join(src_dir, file_)
            dst_file = os.path.join(dst_dir, file_)
            if os.path.exists(dst_file):
                os.remove(dst_file)
            shutil.copy(src_file, dst_dir)


def truecase_sentence(tokens):
    
    from https://github.com/ghaddarAbs
    for experimenting with CoNLL-2003 casing
    
    word_lst = [(w, idx) for idx, w in enumerate(tokens) if all(c.isalpha() for c in w)]
    lst = [w for w, _ in word_lst if re.match(r&quot\b[A-Z\.\-]+\b&quot, w)]

    if len(lst) and len(lst) == len(word_lst):
        parts = truecase.get_true_case(&quot &quot.join(lst)).split()

        &#47&#47 the trucaser have its own tokenization ...
        &#47&#47 skip if the number of word doesn&quott match
        if len(parts) != len(word_lst): return tokens

        for (w, idx), nw in zip(word_lst, parts):
            tokens[idx] = nw
    return tokens


def download_file(url, path, filename=None):
     
    Download with Python requests which handle well compression
    
    &#47&#47 check path
    if path is None or not os.path.isdir(path):
        print("Invalid destination directory:", path)
    HEADERS = {User-Agent: Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:81.0) Gecko/20100101 Firefox/81.0}
    result = "fail"
    print("downloading", url) 
    
    if filename is None:
        &#47&#47 we use the actual server file name
        a = urlparse(url)
        filename = os.path.basename(a.path)
    destination = os.path.join(path, filename)    
    try:
        resp = requests.get(url, stream=True, allow_redirects=True, headers=HEADERS)
        total_length = resp.headers.get(&quotcontent-length&quot)

        if total_length is None and resp.status_code == 200: 
            &#47&#47 no content length header available, can&quott have a progress bar :(
            with open(destination, &quotwb&quot) as f_out:
                f_out.write(resp.content)
        elif resp.status_code == 200:
            total = int(total_length)
            with open(destination, &quotwb&quot) as f_out, tqdm(
                desc=destination,
                total=total,
                unit=&quotiB&quot,
                unit_scale=True,
                unit_divisor=1024,
            ) as bar:
                for data in resp.iter_content(chunk_size=1024):
                    size = f_out.write(data)
                    bar.update(size)
            result = "success"
    except Exception:
        print("Download failed for {0} with requests".format(url))
    if result == "success":
        return destination
    else:
        return None

def len_until_first_pad(tokens, pad):
    i = len(tokens)-1
    while i&gt;=0:
        if tokens[i] != pad:
            return i+1
        i -= 1
    return 0

def len_until_first_pad_old(tokens, pad):
    for i in range(len(tokens)):
        if tokens[i] == pad:
            return i
    return len(tokens)


if __name__ == "__main__":
    &#47&#47 usage example - for CoNLL-2003, indicate the eng.* file to be converted:
    &#47&#47 &gt; python3 utilities/Utilities.py --dataset-type conll2003 --data-path /home/lopez/resources/CoNLL-2003/eng.train --output-path /home/lopez/resources/CoNLL-2003/iob2/eng.train 
    &#47&#47 for CoNLL-2012, indicate the root directory of the ontonotes data (in CoNLL-2012 format) to be converted:
    &#47&#47 &gt; python3 utilities/Utilities.py --dataset-type conll2012 --data-path /home/lopez/resources/ontonotes/conll-2012/ --output-path /home/lopez/resources/ontonotes/conll-2012/iob2/

    &#47&#47 get the argument
    parser = argparse.ArgumentParser(
        description = "Named Entity Recognizer dataset converter to OIB2 tagging scheme")

    &#47&#47parser.add_argument("action")
    parser.add_argument("--dataset-type",default=&quotconll2003&quot, help="dataset to be used for training the model, one of [&quotconll2003&quot,&quotconll2012&quot]")
    parser.add_argument("--data-path", default=None, help="path to the corpus of documents to process") 
    parser.add_argument("--output-path", default=None, help="path to write the converted dataset") 

    args = parser.parse_args()

    &#47&#47action = args.action 
    dataset_type = args.dataset_type
    data_path = args.data_path
    output_path = args.output_path

    if dataset_type == &quotconll2003&quot:
        convert_conll2003_to_iob2(data_path, output_path)
    elif dataset_type == &quotconll2012&quot:    
        convert_conll2012_to_iob2(data_path, output_path)
    elif dataset_type == &quotontonotes&quot:    
        ontonotes_conll2012_names(data_path, output_path)
</code></pre>