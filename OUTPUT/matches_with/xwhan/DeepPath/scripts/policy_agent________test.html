<link rel="stylesheet" href="../../..//default.css">
<script src="../../..//highlight.pack.js"></script> 
<script>hljs.initHighlightingOnLoad();</script>
<html><pre><code class='java'>
<a href="https://github.com/xwhan/DeepPath/blob/master/scripts/policy_agent.py#L200">GitHubLink</a>


<a href="https://github.com/maldil/DeepPath/blob/master/scripts/policy_agent.py#L200">GitMyHubLink</a>

from __future__ import division
from __future__ import print_function
import tensorflow as tf
import numpy as np
import collections
from itertools import count
from sklearn.metrics.pairwise import cosine_similarity
import time
import sys

from networks import policy_nn, value_nn
from utils import *
from env import Env


relation = sys.argv[1]
task = sys.argv[2]
graphpath = dataPath + &quottasks/&quot + relation + &quot/&quot + &quotgraph.txt&quot
relationPath = dataPath + &quottasks/&quot + relation + &quot/&quot + &quottrain_pos&quot

class PolicyNetwork(object):

	def __init__(self, scope = &quotpolicy_network&quot, learning_rate = 0.001):
		self.initializer = tf.contrib.layers.xavier_initializer()
		with tf.variable_scope(scope):
			self.state = tf.placeholder(tf.float32, [None, state_dim], name = &quotstate&quot)
			self.action = tf.placeholder(tf.int32, [None], name = &quotaction&quot)
			self.target = tf.placeholder(tf.float32, name = &quottarget&quot)
			self.action_prob = policy_nn(self.state, state_dim, action_space, self.initializer)

			action_mask = tf.cast(tf.one_hot(self.action, depth = action_space), tf.bool)
			self.picked_action_prob = tf.boolean_mask(self.action_prob, action_mask)

			self.loss = tf.reduce_sum(-tf.log(self.picked_action_prob)*self.target) + sum(tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES, scope=scope))
			self.optimizer = tf.train.AdamOptimizer(learning_rate = learning_rate)
			self.train_op = self.optimizer.minimize(self.loss)

	def predict(self, state, sess = None):
		sess = sess or tf.get_default_session()
		return sess.run(self.action_prob, {self.state:state})

	def update(self, state, target, action, sess=None):
		sess = sess or tf.get_default_session()
		feed_dict = { self.state: state, self.target: target, self.action: action  }
		_, loss = sess.run([self.train_op, self.loss], feed_dict)
		return loss


def REINFORCE(training_pairs, policy_nn, num_episodes):
	train = training_pairs

	success = 0

	&#47&#47 path_found = set()
	path_found_entity = []
	path_relation_found = []

	for i_episode in range(num_episodes):
		start = time.time()
		print(&quotEpisode %d&quot % i_episode)
		print(&quotTraining sample: &quot, train[i_episode][:-1])

		env = Env(dataPath, train[i_episode])

		sample = train[i_episode].split()
		state_idx = [env.entity2id_[sample[0]], env.entity2id_[sample[1]], 0]

		episode = []
		state_batch_negative = []
		action_batch_negative = []
		for t in count():
			state_vec = env.idx_state(state_idx)
			action_probs = policy_nn.predict(state_vec)
			action_chosen = np.random.choice(np.arange(action_space), p = np.squeeze(action_probs))
			reward, new_state, done = env.interact(state_idx, action_chosen)

			if reward == -1: &#47&#47 the action fails for this step
				state_batch_negative.append(state_vec)
				action_batch_negative.append(action_chosen)

			new_state_vec = env.idx_state(new_state)
			episode.append(Transition(state = state_vec, action = action_chosen, next_state = new_state_vec, reward = reward))

			if done or t == max_steps:
				break

			state_idx = new_state

		&#47&#47 Discourage the agent when it choose an invalid step
		if len(state_batch_negative) != 0:
			print(&quotPenalty to invalid steps:&quot, len(state_batch_negative))
			policy_nn.update(np.reshape(state_batch_negative, (-1, state_dim)), -0.05, action_batch_negative)

		print(&quot----- FINAL PATH -----&quot)
		print(&quot\t&quot.join(env.path))
		print(&quotPATH LENGTH&quot, len(env.path))
		print(&quot----- FINAL PATH -----&quot)

		&#47&#47 If the agent success, do one optimization
		if done == 1:
			print(&quotSuccess&quot)

			path_found_entity.append(path_clean(&quot -&gt; &quot.join(env.path)))

			success += 1
			path_length = len(env.path)
			length_reward = 1/path_length
			global_reward = 1
			
			&#47&#47 if len(path_found) != 0:
			&#47&#47 	path_found_embedding = [env.path_embedding(path.split(&quot -&gt; &quot)) for path in path_found]
			&#47&#47 	curr_path_embedding = env.path_embedding(env.path_relations)
			&#47&#47 	path_found_embedding = np.reshape(path_found_embedding, (-1,embedding_dim))
			&#47&#47 	cos_sim = cosine_similarity(path_found_embedding, curr_path_embedding)
			&#47&#47 	diverse_reward = -np.mean(cos_sim)
			&#47&#47 	print &quotdiverse_reward&quot, diverse_reward
			&#47&#47 	total_reward = 0.1*global_reward + 0.8*length_reward + 0.1*diverse_reward 
			&#47&#47 else:
			&#47&#47 	total_reward = 0.1*global_reward + 0.9*length_reward
			&#47&#47 path_found.add(&quot -&gt; &quot.join(env.path_relations))
			
			total_reward = 0.1*global_reward + 0.9*length_reward
			state_batch = []
			action_batch = []
			for t, transition in enumerate(episode):
				if transition.reward == 0:
					state_batch.append(transition.state)
					action_batch.append(transition.action)
			policy_nn.update(np.reshape(state_batch,(-1,state_dim)), total_reward, action_batch)
		else:
			global_reward = -0.05
			&#47&#47 length_reward = 1/len(env.path)

			state_batch = []
			action_batch = []
			total_reward = global_reward
			for t, transition in enumerate(episode):
				if transition.reward == 0:
					state_batch.append(transition.state)
					action_batch.append(transition.action)
			policy_nn.update(np.reshape(state_batch, (-1,state_dim)), total_reward, action_batch)

			print(&quotFailed, Do one teacher guideline&quot)
			try:
				good_episodes = teacher(sample[0], sample[1], 1, env, graphpath)
				for item in good_episodes:
					teacher_state_batch = []
					teacher_action_batch = []
					total_reward = 0.0*1 + 1*1/len(item)
					for t, transition in enumerate(item):
						teacher_state_batch.append(transition.state)
						teacher_action_batch.append(transition.action)
					policy_nn.update(np.squeeze(teacher_state_batch), 1, teacher_action_batch)

			except Exception as e:
				print(&quotTeacher guideline failed&quot)
		print(&quotEpisode time: &quot, time.time() - start)
		print(&quot\n&quot)
	print(&quotSuccess percentage:&quot, success/num_episodes)

	for path in path_found_entity:
		rel_ent = path.split(&quot -&gt; &quot)
		path_relation = []
		for idx, item in enumerate(rel_ent):
			if idx%2 == 0:
				path_relation.append(item)
		path_relation_found.append(&quot -&gt; &quot.join(path_relation))

	relation_path_stats = collections.Counter(path_relation_found).items()
	relation_path_stats = sorted(relation_path_stats, key = lambda x:x[1], reverse=True)

	f = open(dataPath + &quottasks/&quot + relation + &quot/&quot + &quotpath_stats.txt&quot, &quotw&quot)
	for item in relation_path_stats:
		f.write(item[0]+&quot\t&quot+str(item[1])+&quot\n&quot)
	f.close()
	print(&quotPath stats saved&quot)

	return 

def retrain():
	print(&quotStart retraining&quot)
	tf.reset_default_graph()
	policy_network = PolicyNetwork(scope = &quotsupervised_policy&quot)

	f = open(relationPath)
	training_pairs = f.readlines()
	f.close()

	saver = tf.train.Saver()
	with tf.Session() as sess:
		saver.restore(sess, &quotmodels/policy_supervised_&quot + relation)
		print("sl_policy restored")
		episodes = len(training_pairs)
		if episodes &gt; 300:
			episodes = 300
		REINFORCE(training_pairs, policy_network, episodes)
		saver.save(sess, &quotmodels/policy_retrained&quot + relation)
	print(&quotRetrained model saved&quot)

def test():
	tf.reset_default_graph()
	<a id="change">policy_network</a> = PolicyNetwork(scope = &quotsupervised_policy&quot)

	<a id="change">f</a><a id="change"> = open(relationPath)</a>
	<a id="change">all_data</a> = f.readlines()
	<a id="change">f</a><a id="change">.close()</a>

	<a id="change">test_data</a> = all_data
	<a id="change">test_num</a> = len(test_data)

	<a id="change">success</a> = 0

	<a id="change">saver</a> = tf.train.Saver()
	<a id="change">path_found</a> = []
	<a id="change">path_relation_found</a> = []
	<a id="change">path_set</a> = set()

	with tf.Session() as <a id="change">sess</a>:
		saver.restore(sess, &quotmodels/policy_retrained&quot + relation)
		print(&quotModel reloaded&quot)

		if test_num &gt; 500:
			<a id="change">test_num</a> = 500

		for <a id="change">episode</a> in range(test_num):
			print(&quotTest sample %d: %s&quot % (episode,test_data[episode][:-1]))
			<a id="change">env</a> = Env(dataPath, test_data[episode])
			<a id="change">sample</a> = test_data[episode].split()
			<a id="change">state_idx</a> = [env.entity2id_[sample[0]], env.entity2id_[sample[1]], 0]

			<a id="change">transitions</a> = []

			for <a id="change">t</a> in count():
				<a id="change">state_vec</a> = env.idx_state(state_idx)
				<a id="change">action_probs</a> = policy_network.predict(state_vec)

				action_probs = np.squeeze(action_probs)

				<a id="change">action_chosen</a> = np.random.choice(np.arange(action_space), p = action_probs)
				reward, new_state, done = env.interact(state_idx, action_chosen)
				new_state_vec = env.idx_state(new_state)
				transitions.append(Transition(state = state_vec, action = action_chosen, next_state = new_state_vec, reward = reward))

				if done or t == max_steps_test:
					if done:
						success += 1
						print("Success\n")
						<a id="change">path</a> = path_clean(&quot -&gt; &quot.join(env.path))
						path_found.append(path)
					else:
						print(&quotEpisode ends due to step limit\n&quot)
					break
				state_idx = new_state
			
			if done:
				if len(path_set) != 0:
					<a id="change">path_found_embedding</a> = [env.path_embedding(path.split(&quot -&gt; &quot)) for <a id="change">path</a> in path_set]
					<a id="change">curr_path_embedding</a> = env.path_embedding(env.path_relations)
					<a id="change">path_found_embedding</a> = np.reshape(path_found_embedding, (-1,embedding_dim))
					<a id="change">cos_sim</a> = cosine_similarity(path_found_embedding, curr_path_embedding)
					<a id="change">diverse_reward</a> = -np.mean(cos_sim)
					print(&quotdiverse_reward&quot, diverse_reward)
					&#47&#47total_reward = 0.1*global_reward + 0.8*length_reward + 0.1*diverse_reward 
					<a id="change">state_batch</a> = []
					<a id="change">action_batch</a> = []
					for <a id="change">t</a>, <a id="change">transition</a> in enumerate(transitions):
						if transition.reward == 0:
							state_batch.append(transition.state)
							action_batch.append(transition.action)
					policy_network.update(np.reshape(state_batch,(-1,state_dim)), 0.1*diverse_reward, action_batch)
				path_set.add(&quot -&gt; &quot.join(env.path_relations))


	for <a id="change">path</a> in path_found:
		<a id="change">rel_ent</a> = path.split(&quot -&gt; &quot)
		<a id="change">path_relation</a> = []
		for <a id="change">idx</a>, <a id="change">item</a> in enumerate(rel_ent):
			if idx%2 == 0:
				path_relation.append(item)
		path_relation_found.append(&quot -&gt; &quot.join(path_relation))

	&#47&#47 path_stats = collections.Counter(path_found).items()
	<a id="change">relation_path_stats</a> = collections.Counter(path_relation_found).items()
	<a id="change">relation_path_stats</a> = sorted(relation_path_stats, key = lambda x:x[1], reverse=True)

	<a id="change">ranking_path</a> = []
	for <a id="change">item</a> in relation_path_stats:
		<a id="change">path</a> = item[0]
		<a id="change">length</a> = len(path.split(&quot -&gt; &quot))
		ranking_path.append((path, length))

	<a id="change">ranking_path</a> = sorted(ranking_path, key = lambda x:x[1])
	print(&quotSuccess persentage:&quot, success/test_num)

	<a id="change">f</a><a id="change"> = open(dataPath + &quottasks/&quot + relation + &quot/&quot + &quotpath_to_use.txt&quot, &quotw&quot)</a>
	for <a id="change">item</a> in ranking_path:
		f.write(item[0] + &quot\n&quot)
	<a id="change">f</a><a id="change">.close()</a>
	print(&quotpath to use saved&quot)
	return

if __name__ == "__main__":
	if task == &quottest&quot:
		test()
	elif task == &quotretrain&quot:
		retrain()
	else:
		retrain()
		test()
	&#47&#47 retrain()	



</code></pre>